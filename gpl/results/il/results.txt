Training epoch 1 [0/67107 (0.00%)]	 Loss: 0.263874
Training epoch 1 [640/67107 (0.95%)]	 Loss: 0.145426
Training epoch 1 [1280/67107 (1.91%)]	 Loss: 0.138928
Training epoch 1 [1920/67107 (2.86%)]	 Loss: 0.114156
Training epoch 1 [2560/67107 (3.82%)]	 Loss: 0.085761
Training epoch 1 [3200/67107 (4.77%)]	 Loss: 0.061020
Training epoch 1 [3840/67107 (5.73%)]	 Loss: 0.064757
Training epoch 1 [4480/67107 (6.68%)]	 Loss: 0.054104
Training epoch 1 [5120/67107 (7.63%)]	 Loss: 0.047235
Training epoch 1 [5760/67107 (8.59%)]	 Loss: 0.051844
Training epoch 1 [6400/67107 (9.54%)]	 Loss: 0.023786
Training epoch 1 [7040/67107 (10.50%)]	 Loss: 0.042657
Training epoch 1 [7680/67107 (11.45%)]	 Loss: 0.052486
Training epoch 1 [8320/67107 (12.40%)]	 Loss: 0.008191
Training epoch 1 [8960/67107 (13.36%)]	 Loss: 0.031009
Training epoch 1 [9600/67107 (14.31%)]	 Loss: 0.024942
Training epoch 1 [10240/67107 (15.27%)]	 Loss: 0.013966
Training epoch 1 [10880/67107 (16.22%)]	 Loss: 0.017529
Training epoch 1 [11520/67107 (17.18%)]	 Loss: 0.006471
Training epoch 1 [12160/67107 (18.13%)]	 Loss: 0.016964
Training epoch 1 [12800/67107 (19.08%)]	 Loss: 0.004773
Training epoch 1 [13440/67107 (20.04%)]	 Loss: 0.002077
Training epoch 1 [14080/67107 (20.99%)]	 Loss: 0.005800
Training epoch 1 [14720/67107 (21.95%)]	 Loss: 0.009766
Training epoch 1 [15360/67107 (22.90%)]	 Loss: 0.005783
Training epoch 1 [16000/67107 (23.85%)]	 Loss: 0.014908
Training epoch 1 [16640/67107 (24.81%)]	 Loss: 0.006258
Training epoch 1 [17280/67107 (25.76%)]	 Loss: 0.010371
Training epoch 1 [17920/67107 (26.72%)]	 Loss: 0.019012
Training epoch 1 [18560/67107 (27.67%)]	 Loss: 0.007502
Training epoch 1 [19200/67107 (28.63%)]	 Loss: 0.002558
Training epoch 1 [19840/67107 (29.58%)]	 Loss: 0.001747
Training epoch 1 [20480/67107 (30.53%)]	 Loss: 0.002354
Training epoch 1 [21120/67107 (31.49%)]	 Loss: 0.003144
Training epoch 1 [21760/67107 (32.44%)]	 Loss: 0.001714
Training epoch 1 [22400/67107 (33.40%)]	 Loss: 0.001514
Training epoch 1 [23040/67107 (34.35%)]	 Loss: 0.004339
Training epoch 1 [23680/67107 (35.31%)]	 Loss: 0.001697
Training epoch 1 [24320/67107 (36.26%)]	 Loss: 0.002145
Training epoch 1 [24960/67107 (37.21%)]	 Loss: 0.003494
Training epoch 1 [25600/67107 (38.17%)]	 Loss: 0.001006
Training epoch 1 [26240/67107 (39.12%)]	 Loss: 0.003981
Training epoch 1 [26880/67107 (40.08%)]	 Loss: 0.002778
Training epoch 1 [27520/67107 (41.03%)]	 Loss: 0.001086
Training epoch 1 [28160/67107 (41.98%)]	 Loss: 0.001486
Training epoch 1 [28800/67107 (42.94%)]	 Loss: 0.000494
Training epoch 1 [29440/67107 (43.89%)]	 Loss: 0.001464
Training epoch 1 [30080/67107 (44.85%)]	 Loss: 0.000840
Training epoch 1 [30720/67107 (45.80%)]	 Loss: 0.000518
Training epoch 1 [31360/67107 (46.76%)]	 Loss: 0.001213
Training epoch 1 [32000/67107 (47.71%)]	 Loss: 0.001011
Training epoch 1 [32640/67107 (48.66%)]	 Loss: 0.000842
Training epoch 1 [33280/67107 (49.62%)]	 Loss: 0.000327
Training epoch 1 [33920/67107 (50.57%)]	 Loss: 0.000883
Training epoch 1 [34560/67107 (51.53%)]	 Loss: 0.000509
Training epoch 1 [35200/67107 (52.48%)]	 Loss: 0.001086
Training epoch 1 [35840/67107 (53.44%)]	 Loss: 0.006810
Training epoch 1 [36480/67107 (54.39%)]	 Loss: 0.000181
Training epoch 1 [37120/67107 (55.34%)]	 Loss: 0.000345
Training epoch 1 [37760/67107 (56.30%)]	 Loss: 0.000418
Training epoch 1 [38400/67107 (57.25%)]	 Loss: 0.001497
Training epoch 1 [39040/67107 (58.21%)]	 Loss: 0.000699
Training epoch 1 [39680/67107 (59.16%)]	 Loss: 0.000933
Training epoch 1 [40320/67107 (60.11%)]	 Loss: 0.000391
Training epoch 1 [40960/67107 (61.07%)]	 Loss: 0.006373
Training epoch 1 [41600/67107 (62.02%)]	 Loss: 0.001495
Training epoch 1 [42240/67107 (62.98%)]	 Loss: 0.000350
Training epoch 1 [42880/67107 (63.93%)]	 Loss: 0.001865
Training epoch 1 [43520/67107 (64.89%)]	 Loss: 0.000105
Training epoch 1 [44160/67107 (65.84%)]	 Loss: 0.000621
Training epoch 1 [44800/67107 (66.79%)]	 Loss: 0.000261
Training epoch 1 [45440/67107 (67.75%)]	 Loss: 0.001212
Training epoch 1 [46080/67107 (68.70%)]	 Loss: 0.000253
Training epoch 1 [46720/67107 (69.66%)]	 Loss: 0.001171
Training epoch 1 [47360/67107 (70.61%)]	 Loss: 0.000120
Training epoch 1 [48000/67107 (71.56%)]	 Loss: 0.000309
Training epoch 1 [48640/67107 (72.52%)]	 Loss: 0.000083
Training epoch 1 [49280/67107 (73.47%)]	 Loss: 0.000182
Training epoch 1 [49920/67107 (74.43%)]	 Loss: 0.000229
Training epoch 1 [50560/67107 (75.38%)]	 Loss: 0.000884
Training epoch 1 [51200/67107 (76.34%)]	 Loss: 0.000601
Training epoch 1 [51840/67107 (77.29%)]	 Loss: 0.000805
Training epoch 1 [52480/67107 (78.24%)]	 Loss: 0.000125
Training epoch 1 [53120/67107 (79.20%)]	 Loss: 0.000247
Training epoch 1 [53760/67107 (80.15%)]	 Loss: 0.000198
Training epoch 1 [54400/67107 (81.11%)]	 Loss: 0.000502
Training epoch 1 [55040/67107 (82.06%)]	 Loss: 0.000494
Training epoch 1 [55680/67107 (83.02%)]	 Loss: 0.000303
Training epoch 1 [56320/67107 (83.97%)]	 Loss: 0.000616
Training epoch 1 [56960/67107 (84.92%)]	 Loss: 0.000216
Training epoch 1 [57600/67107 (85.88%)]	 Loss: 0.000105
Training epoch 1 [58240/67107 (86.83%)]	 Loss: 0.000468
Training epoch 1 [58880/67107 (87.79%)]	 Loss: 0.000216
Training epoch 1 [59520/67107 (88.74%)]	 Loss: 0.000207
Training epoch 1 [60160/67107 (89.69%)]	 Loss: 0.000109
Training epoch 1 [60800/67107 (90.65%)]	 Loss: 0.004031
Training epoch 1 [61440/67107 (91.60%)]	 Loss: 0.000151
Training epoch 1 [62080/67107 (92.56%)]	 Loss: 0.000184
Training epoch 1 [62720/67107 (93.51%)]	 Loss: 0.000131
Training epoch 1 [63360/67107 (94.47%)]	 Loss: 0.000408
Training epoch 1 [64000/67107 (95.42%)]	 Loss: 0.000056
Training epoch 1 [64640/67107 (96.37%)]	 Loss: 0.000181
Training epoch 1 [65280/67107 (97.33%)]	 Loss: 0.000040
Training epoch 1 [65920/67107 (98.28%)]	 Loss: 0.002992
Training epoch 1 [66560/67107 (99.24%)]	 Loss: 0.001804
Test set: Average Loss: 0.005366
Training epoch 2 [0/67107 (0.00%)]	 Loss: 0.000621
Training epoch 2 [640/67107 (0.95%)]	 Loss: 0.001432
Training epoch 2 [1280/67107 (1.91%)]	 Loss: 0.000210
Training epoch 2 [1920/67107 (2.86%)]	 Loss: 0.001542
Training epoch 2 [2560/67107 (3.82%)]	 Loss: 0.000339
Training epoch 2 [3200/67107 (4.77%)]	 Loss: 0.000070
Training epoch 2 [3840/67107 (5.73%)]	 Loss: 0.000087
Training epoch 2 [4480/67107 (6.68%)]	 Loss: 0.000044
Training epoch 2 [5120/67107 (7.63%)]	 Loss: 0.000562
Training epoch 2 [5760/67107 (8.59%)]	 Loss: 0.000052
Training epoch 2 [6400/67107 (9.54%)]	 Loss: 0.000422
Training epoch 2 [7040/67107 (10.50%)]	 Loss: 0.000330
Training epoch 2 [7680/67107 (11.45%)]	 Loss: 0.000807
Training epoch 2 [8320/67107 (12.40%)]	 Loss: 0.000348
Training epoch 2 [8960/67107 (13.36%)]	 Loss: 0.001028
Training epoch 2 [9600/67107 (14.31%)]	 Loss: 0.000145
Training epoch 2 [10240/67107 (15.27%)]	 Loss: 0.000193
Training epoch 2 [10880/67107 (16.22%)]	 Loss: 0.000848
Training epoch 2 [11520/67107 (17.18%)]	 Loss: 0.000232
Training epoch 2 [12160/67107 (18.13%)]	 Loss: 0.000926
Training epoch 2 [12800/67107 (19.08%)]	 Loss: 0.001022
Training epoch 2 [13440/67107 (20.04%)]	 Loss: 0.000256
Training epoch 2 [14080/67107 (20.99%)]	 Loss: 0.000120
Training epoch 2 [14720/67107 (21.95%)]	 Loss: 0.000208
Training epoch 2 [15360/67107 (22.90%)]	 Loss: 0.000167
Training epoch 2 [16000/67107 (23.85%)]	 Loss: 0.000398
Training epoch 2 [16640/67107 (24.81%)]	 Loss: 0.000018
Training epoch 2 [17280/67107 (25.76%)]	 Loss: 0.000090
Training epoch 2 [17920/67107 (26.72%)]	 Loss: 0.000078
Training epoch 2 [18560/67107 (27.67%)]	 Loss: 0.000189
Training epoch 2 [19200/67107 (28.63%)]	 Loss: 0.000221
Training epoch 2 [19840/67107 (29.58%)]	 Loss: 0.000027
Training epoch 2 [20480/67107 (30.53%)]	 Loss: 0.000411
Training epoch 2 [21120/67107 (31.49%)]	 Loss: 0.000092
Training epoch 2 [21760/67107 (32.44%)]	 Loss: 0.000192
Training epoch 2 [22400/67107 (33.40%)]	 Loss: 0.000760
Training epoch 2 [23040/67107 (34.35%)]	 Loss: 0.000760
Training epoch 2 [23680/67107 (35.31%)]	 Loss: 0.000060
Training epoch 2 [24320/67107 (36.26%)]	 Loss: 0.001596
Training epoch 2 [24960/67107 (37.21%)]	 Loss: 0.000181
Training epoch 2 [25600/67107 (38.17%)]	 Loss: 0.000344
Training epoch 2 [26240/67107 (39.12%)]	 Loss: 0.000055
Training epoch 2 [26880/67107 (40.08%)]	 Loss: 0.001100
Training epoch 2 [27520/67107 (41.03%)]	 Loss: 0.000155
Training epoch 2 [28160/67107 (41.98%)]	 Loss: 0.000463
Training epoch 2 [28800/67107 (42.94%)]	 Loss: 0.000347
Training epoch 2 [29440/67107 (43.89%)]	 Loss: 0.000803
Training epoch 2 [30080/67107 (44.85%)]	 Loss: 0.001059
Training epoch 2 [30720/67107 (45.80%)]	 Loss: 0.000438
Training epoch 2 [31360/67107 (46.76%)]	 Loss: 0.000194
Training epoch 2 [32000/67107 (47.71%)]	 Loss: 0.000060
Training epoch 2 [32640/67107 (48.66%)]	 Loss: 0.000045
Training epoch 2 [33280/67107 (49.62%)]	 Loss: 0.000188
Training epoch 2 [33920/67107 (50.57%)]	 Loss: 0.000202
Training epoch 2 [34560/67107 (51.53%)]	 Loss: 0.000608
Training epoch 2 [35200/67107 (52.48%)]	 Loss: 0.000333
Training epoch 2 [35840/67107 (53.44%)]	 Loss: 0.002311
Training epoch 2 [36480/67107 (54.39%)]	 Loss: 0.000385
Training epoch 2 [37120/67107 (55.34%)]	 Loss: 0.000116
Training epoch 2 [37760/67107 (56.30%)]	 Loss: 0.000265
Training epoch 2 [38400/67107 (57.25%)]	 Loss: 0.000089
Training epoch 2 [39040/67107 (58.21%)]	 Loss: 0.000497
Training epoch 2 [39680/67107 (59.16%)]	 Loss: 0.000112
Training epoch 2 [40320/67107 (60.11%)]	 Loss: 0.000501
Training epoch 2 [40960/67107 (61.07%)]	 Loss: 0.000527
Training epoch 2 [41600/67107 (62.02%)]	 Loss: 0.000692
Training epoch 2 [42240/67107 (62.98%)]	 Loss: 0.000523
Training epoch 2 [42880/67107 (63.93%)]	 Loss: 0.000054
Training epoch 2 [43520/67107 (64.89%)]	 Loss: 0.000171
Training epoch 2 [44160/67107 (65.84%)]	 Loss: 0.000102
Training epoch 2 [44800/67107 (66.79%)]	 Loss: 0.000196
Training epoch 2 [45440/67107 (67.75%)]	 Loss: 0.000224
Training epoch 2 [46080/67107 (68.70%)]	 Loss: 0.000074
Training epoch 2 [46720/67107 (69.66%)]	 Loss: 0.000891
Training epoch 2 [47360/67107 (70.61%)]	 Loss: 0.000614
Training epoch 2 [48000/67107 (71.56%)]	 Loss: 0.000461
Training epoch 2 [48640/67107 (72.52%)]	 Loss: 0.000815
Training epoch 2 [49280/67107 (73.47%)]	 Loss: 0.000819
Training epoch 2 [49920/67107 (74.43%)]	 Loss: 0.000122
Training epoch 2 [50560/67107 (75.38%)]	 Loss: 0.001015
Training epoch 2 [51200/67107 (76.34%)]	 Loss: 0.000315
Training epoch 2 [51840/67107 (77.29%)]	 Loss: 0.000121
Training epoch 2 [52480/67107 (78.24%)]	 Loss: 0.000032
Training epoch 2 [53120/67107 (79.20%)]	 Loss: 0.000095
Training epoch 2 [53760/67107 (80.15%)]	 Loss: 0.000053
Training epoch 2 [54400/67107 (81.11%)]	 Loss: 0.003728
Training epoch 2 [55040/67107 (82.06%)]	 Loss: 0.000550
Training epoch 2 [55680/67107 (83.02%)]	 Loss: 0.001077
Training epoch 2 [56320/67107 (83.97%)]	 Loss: 0.000435
Training epoch 2 [56960/67107 (84.92%)]	 Loss: 0.000088
Training epoch 2 [57600/67107 (85.88%)]	 Loss: 0.000078
Training epoch 2 [58240/67107 (86.83%)]	 Loss: 0.000141
Training epoch 2 [58880/67107 (87.79%)]	 Loss: 0.000147
Training epoch 2 [59520/67107 (88.74%)]	 Loss: 0.000022
Training epoch 2 [60160/67107 (89.69%)]	 Loss: 0.000508
Training epoch 2 [60800/67107 (90.65%)]	 Loss: 0.000049
Training epoch 2 [61440/67107 (91.60%)]	 Loss: 0.000068
Training epoch 2 [62080/67107 (92.56%)]	 Loss: 0.000036
Training epoch 2 [62720/67107 (93.51%)]	 Loss: 0.000112
Training epoch 2 [63360/67107 (94.47%)]	 Loss: 0.000062
Training epoch 2 [64000/67107 (95.42%)]	 Loss: 0.000198
Training epoch 2 [64640/67107 (96.37%)]	 Loss: 0.000033
Training epoch 2 [65280/67107 (97.33%)]	 Loss: 0.000096
Training epoch 2 [65920/67107 (98.28%)]	 Loss: 0.000041
Training epoch 2 [66560/67107 (99.24%)]	 Loss: 0.000097
Test set: Average Loss: 0.000861
Training epoch 3 [0/67107 (0.00%)]	 Loss: 0.000209
Training epoch 3 [640/67107 (0.95%)]	 Loss: 0.000303
Training epoch 3 [1280/67107 (1.91%)]	 Loss: 0.000291
Training epoch 3 [1920/67107 (2.86%)]	 Loss: 0.000117
Training epoch 3 [2560/67107 (3.82%)]	 Loss: 0.000761
Training epoch 3 [3200/67107 (4.77%)]	 Loss: 0.000088
Training epoch 3 [3840/67107 (5.73%)]	 Loss: 0.000020
Training epoch 3 [4480/67107 (6.68%)]	 Loss: 0.000019
Training epoch 3 [5120/67107 (7.63%)]	 Loss: 0.000034
Training epoch 3 [5760/67107 (8.59%)]	 Loss: 0.000416
Training epoch 3 [6400/67107 (9.54%)]	 Loss: 0.000141
Training epoch 3 [7040/67107 (10.50%)]	 Loss: 0.000878
Training epoch 3 [7680/67107 (11.45%)]	 Loss: 0.000129
Training epoch 3 [8320/67107 (12.40%)]	 Loss: 0.000378
Training epoch 3 [8960/67107 (13.36%)]	 Loss: 0.000277
Training epoch 3 [9600/67107 (14.31%)]	 Loss: 0.001298
Training epoch 3 [10240/67107 (15.27%)]	 Loss: 0.000243
Training epoch 3 [10880/67107 (16.22%)]	 Loss: 0.000120
Training epoch 3 [11520/67107 (17.18%)]	 Loss: 0.000033
Training epoch 3 [12160/67107 (18.13%)]	 Loss: 0.000238
Training epoch 3 [12800/67107 (19.08%)]	 Loss: 0.000484
Training epoch 3 [13440/67107 (20.04%)]	 Loss: 0.000079
Training epoch 3 [14080/67107 (20.99%)]	 Loss: 0.000019
Training epoch 3 [14720/67107 (21.95%)]	 Loss: 0.000466
Training epoch 3 [15360/67107 (22.90%)]	 Loss: 0.000151
Training epoch 3 [16000/67107 (23.85%)]	 Loss: 0.000051
Training epoch 3 [16640/67107 (24.81%)]	 Loss: 0.000032
Training epoch 3 [17280/67107 (25.76%)]	 Loss: 0.000036
Training epoch 3 [17920/67107 (26.72%)]	 Loss: 0.000058
Training epoch 3 [18560/67107 (27.67%)]	 Loss: 0.000948
Training epoch 3 [19200/67107 (28.63%)]	 Loss: 0.000071
Training epoch 3 [19840/67107 (29.58%)]	 Loss: 0.000346
Training epoch 3 [20480/67107 (30.53%)]	 Loss: 0.000092
Training epoch 3 [21120/67107 (31.49%)]	 Loss: 0.000090
Training epoch 3 [21760/67107 (32.44%)]	 Loss: 0.000570
Training epoch 3 [22400/67107 (33.40%)]	 Loss: 0.000598
Training epoch 3 [23040/67107 (34.35%)]	 Loss: 0.000080
Training epoch 3 [23680/67107 (35.31%)]	 Loss: 0.000071
Training epoch 3 [24320/67107 (36.26%)]	 Loss: 0.000346
Training epoch 3 [24960/67107 (37.21%)]	 Loss: 0.000055
Training epoch 3 [25600/67107 (38.17%)]	 Loss: 0.000804
Training epoch 3 [26240/67107 (39.12%)]	 Loss: 0.000094
Training epoch 3 [26880/67107 (40.08%)]	 Loss: 0.000579
Training epoch 3 [27520/67107 (41.03%)]	 Loss: 0.001057
Training epoch 3 [28160/67107 (41.98%)]	 Loss: 0.000088
Training epoch 3 [28800/67107 (42.94%)]	 Loss: 0.000083
Training epoch 3 [29440/67107 (43.89%)]	 Loss: 0.000124
Training epoch 3 [30080/67107 (44.85%)]	 Loss: 0.000073
Training epoch 3 [30720/67107 (45.80%)]	 Loss: 0.000015
Training epoch 3 [31360/67107 (46.76%)]	 Loss: 0.000015
Training epoch 3 [32000/67107 (47.71%)]	 Loss: 0.000064
Training epoch 3 [32640/67107 (48.66%)]	 Loss: 0.000048
Training epoch 3 [33280/67107 (49.62%)]	 Loss: 0.000350
Training epoch 3 [33920/67107 (50.57%)]	 Loss: 0.000493
Training epoch 3 [34560/67107 (51.53%)]	 Loss: 0.000735
Training epoch 3 [35200/67107 (52.48%)]	 Loss: 0.000200
Training epoch 3 [35840/67107 (53.44%)]	 Loss: 0.000073
Training epoch 3 [36480/67107 (54.39%)]	 Loss: 0.000198
Training epoch 3 [37120/67107 (55.34%)]	 Loss: 0.000520
Training epoch 3 [37760/67107 (56.30%)]	 Loss: 0.000118
Training epoch 3 [38400/67107 (57.25%)]	 Loss: 0.000062
Training epoch 3 [39040/67107 (58.21%)]	 Loss: 0.000169
Training epoch 3 [39680/67107 (59.16%)]	 Loss: 0.000239
Training epoch 3 [40320/67107 (60.11%)]	 Loss: 0.000620
Training epoch 3 [40960/67107 (61.07%)]	 Loss: 0.000233
Training epoch 3 [41600/67107 (62.02%)]	 Loss: 0.000094
Training epoch 3 [42240/67107 (62.98%)]	 Loss: 0.000325
Training epoch 3 [42880/67107 (63.93%)]	 Loss: 0.000069
Training epoch 3 [43520/67107 (64.89%)]	 Loss: 0.000109
Training epoch 3 [44160/67107 (65.84%)]	 Loss: 0.000023
Training epoch 3 [44800/67107 (66.79%)]	 Loss: 0.000105
Training epoch 3 [45440/67107 (67.75%)]	 Loss: 0.000080
Training epoch 3 [46080/67107 (68.70%)]	 Loss: 0.000088
Training epoch 3 [46720/67107 (69.66%)]	 Loss: 0.000837
Training epoch 3 [47360/67107 (70.61%)]	 Loss: 0.000427
Training epoch 3 [48000/67107 (71.56%)]	 Loss: 0.000415
Training epoch 3 [48640/67107 (72.52%)]	 Loss: 0.000070
Training epoch 3 [49280/67107 (73.47%)]	 Loss: 0.000086
Training epoch 3 [49920/67107 (74.43%)]	 Loss: 0.000069
Training epoch 3 [50560/67107 (75.38%)]	 Loss: 0.000168
Training epoch 3 [51200/67107 (76.34%)]	 Loss: 0.000097
Training epoch 3 [51840/67107 (77.29%)]	 Loss: 0.000019
Training epoch 3 [52480/67107 (78.24%)]	 Loss: 0.000032
Training epoch 3 [53120/67107 (79.20%)]	 Loss: 0.000029
Training epoch 3 [53760/67107 (80.15%)]	 Loss: 0.000810
Training epoch 3 [54400/67107 (81.11%)]	 Loss: 0.000306
Training epoch 3 [55040/67107 (82.06%)]	 Loss: 0.000168
Training epoch 3 [55680/67107 (83.02%)]	 Loss: 0.000091
Training epoch 3 [56320/67107 (83.97%)]	 Loss: 0.000327
Training epoch 3 [56960/67107 (84.92%)]	 Loss: 0.000086
Training epoch 3 [57600/67107 (85.88%)]	 Loss: 0.000071
Training epoch 3 [58240/67107 (86.83%)]	 Loss: 0.000177
Training epoch 3 [58880/67107 (87.79%)]	 Loss: 0.000015
Training epoch 3 [59520/67107 (88.74%)]	 Loss: 0.000107
Training epoch 3 [60160/67107 (89.69%)]	 Loss: 0.000033
Training epoch 3 [60800/67107 (90.65%)]	 Loss: 0.000119
Training epoch 3 [61440/67107 (91.60%)]	 Loss: 0.000493
Training epoch 3 [62080/67107 (92.56%)]	 Loss: 0.000661
Training epoch 3 [62720/67107 (93.51%)]	 Loss: 0.000171
Training epoch 3 [63360/67107 (94.47%)]	 Loss: 0.000107
Training epoch 3 [64000/67107 (95.42%)]	 Loss: 0.000104
Training epoch 3 [64640/67107 (96.37%)]	 Loss: 0.000099
Training epoch 3 [65280/67107 (97.33%)]	 Loss: 0.000206
Training epoch 3 [65920/67107 (98.28%)]	 Loss: 0.000045
Training epoch 3 [66560/67107 (99.24%)]	 Loss: 0.000149
Test set: Average Loss: 0.009367
Training epoch 4 [0/67107 (0.00%)]	 Loss: 0.000055
Training epoch 4 [640/67107 (0.95%)]	 Loss: 0.000255
Training epoch 4 [1280/67107 (1.91%)]	 Loss: 0.000464
Training epoch 4 [1920/67107 (2.86%)]	 Loss: 0.000143
Training epoch 4 [2560/67107 (3.82%)]	 Loss: 0.000104
Training epoch 4 [3200/67107 (4.77%)]	 Loss: 0.000379
Training epoch 4 [3840/67107 (5.73%)]	 Loss: 0.000100
Training epoch 4 [4480/67107 (6.68%)]	 Loss: 0.000273
Training epoch 4 [5120/67107 (7.63%)]	 Loss: 0.000345
Training epoch 4 [5760/67107 (8.59%)]	 Loss: 0.000032
Training epoch 4 [6400/67107 (9.54%)]	 Loss: 0.000892
Training epoch 4 [7040/67107 (10.50%)]	 Loss: 0.000097
Training epoch 4 [7680/67107 (11.45%)]	 Loss: 0.000173
Training epoch 4 [8320/67107 (12.40%)]	 Loss: 0.001536
Training epoch 4 [8960/67107 (13.36%)]	 Loss: 0.000688
Training epoch 4 [9600/67107 (14.31%)]	 Loss: 0.000160
Training epoch 4 [10240/67107 (15.27%)]	 Loss: 0.000157
Training epoch 4 [10880/67107 (16.22%)]	 Loss: 0.000116
Training epoch 4 [11520/67107 (17.18%)]	 Loss: 0.000029
Training epoch 4 [12160/67107 (18.13%)]	 Loss: 0.000030
Training epoch 4 [12800/67107 (19.08%)]	 Loss: 0.000045
Training epoch 4 [13440/67107 (20.04%)]	 Loss: 0.000037
Training epoch 4 [14080/67107 (20.99%)]	 Loss: 0.000017
Training epoch 4 [14720/67107 (21.95%)]	 Loss: 0.000276
Training epoch 4 [15360/67107 (22.90%)]	 Loss: 0.000685
Training epoch 4 [16000/67107 (23.85%)]	 Loss: 0.000668
Training epoch 4 [16640/67107 (24.81%)]	 Loss: 0.000592
Training epoch 4 [17280/67107 (25.76%)]	 Loss: 0.000176
Training epoch 4 [17920/67107 (26.72%)]	 Loss: 0.000118
Training epoch 4 [18560/67107 (27.67%)]	 Loss: 0.000234
Training epoch 4 [19200/67107 (28.63%)]	 Loss: 0.000052
Training epoch 4 [19840/67107 (29.58%)]	 Loss: 0.000018
Training epoch 4 [20480/67107 (30.53%)]	 Loss: 0.000180
Training epoch 4 [21120/67107 (31.49%)]	 Loss: 0.000459
Training epoch 4 [21760/67107 (32.44%)]	 Loss: 0.000314
Training epoch 4 [22400/67107 (33.40%)]	 Loss: 0.001027
Training epoch 4 [23040/67107 (34.35%)]	 Loss: 0.000583
Training epoch 4 [23680/67107 (35.31%)]	 Loss: 0.000091
Training epoch 4 [24320/67107 (36.26%)]	 Loss: 0.000176
Training epoch 4 [24960/67107 (37.21%)]	 Loss: 0.000115
Training epoch 4 [25600/67107 (38.17%)]	 Loss: 0.000022
Training epoch 4 [26240/67107 (39.12%)]	 Loss: 0.000041
Training epoch 4 [26880/67107 (40.08%)]	 Loss: 0.000028
Training epoch 4 [27520/67107 (41.03%)]	 Loss: 0.000098
Training epoch 4 [28160/67107 (41.98%)]	 Loss: 0.000038
Training epoch 4 [28800/67107 (42.94%)]	 Loss: 0.001028
Training epoch 4 [29440/67107 (43.89%)]	 Loss: 0.000100
Training epoch 4 [30080/67107 (44.85%)]	 Loss: 0.000145
Training epoch 4 [30720/67107 (45.80%)]	 Loss: 0.000190
Training epoch 4 [31360/67107 (46.76%)]	 Loss: 0.000311
Training epoch 4 [32000/67107 (47.71%)]	 Loss: 0.000066
Training epoch 4 [32640/67107 (48.66%)]	 Loss: 0.000209
Training epoch 4 [33280/67107 (49.62%)]	 Loss: 0.000317
Training epoch 4 [33920/67107 (50.57%)]	 Loss: 0.000065
Training epoch 4 [34560/67107 (51.53%)]	 Loss: 0.000033
Training epoch 4 [35200/67107 (52.48%)]	 Loss: 0.000057
Training epoch 4 [35840/67107 (53.44%)]	 Loss: 0.000622
Training epoch 4 [36480/67107 (54.39%)]	 Loss: 0.000182
Training epoch 4 [37120/67107 (55.34%)]	 Loss: 0.000629
Training epoch 4 [37760/67107 (56.30%)]	 Loss: 0.000966
Training epoch 4 [38400/67107 (57.25%)]	 Loss: 0.000222
Training epoch 4 [39040/67107 (58.21%)]	 Loss: 0.000243
Training epoch 4 [39680/67107 (59.16%)]	 Loss: 0.000280
Training epoch 4 [40320/67107 (60.11%)]	 Loss: 0.000966
Training epoch 4 [40960/67107 (61.07%)]	 Loss: 0.000421
Training epoch 4 [41600/67107 (62.02%)]	 Loss: 0.000087
Training epoch 4 [42240/67107 (62.98%)]	 Loss: 0.000036
Training epoch 4 [42880/67107 (63.93%)]	 Loss: 0.000036
Training epoch 4 [43520/67107 (64.89%)]	 Loss: 0.000037
Training epoch 4 [44160/67107 (65.84%)]	 Loss: 0.000065
Training epoch 4 [44800/67107 (66.79%)]	 Loss: 0.000340
Training epoch 4 [45440/67107 (67.75%)]	 Loss: 0.000700
Training epoch 4 [46080/67107 (68.70%)]	 Loss: 0.000367
Training epoch 4 [46720/67107 (69.66%)]	 Loss: 0.000050
Training epoch 4 [47360/67107 (70.61%)]	 Loss: 0.000289
Training epoch 4 [48000/67107 (71.56%)]	 Loss: 0.000176
Training epoch 4 [48640/67107 (72.52%)]	 Loss: 0.000259
Training epoch 4 [49280/67107 (73.47%)]	 Loss: 0.000073
Training epoch 4 [49920/67107 (74.43%)]	 Loss: 0.000372
Training epoch 4 [50560/67107 (75.38%)]	 Loss: 0.000010
Training epoch 4 [51200/67107 (76.34%)]	 Loss: 0.000185
Training epoch 4 [51840/67107 (77.29%)]	 Loss: 0.001524
Training epoch 4 [52480/67107 (78.24%)]	 Loss: 0.000479
Training epoch 4 [53120/67107 (79.20%)]	 Loss: 0.000068
Training epoch 4 [53760/67107 (80.15%)]	 Loss: 0.000157
Training epoch 4 [54400/67107 (81.11%)]	 Loss: 0.000058
Training epoch 4 [55040/67107 (82.06%)]	 Loss: 0.000063
Training epoch 4 [55680/67107 (83.02%)]	 Loss: 0.000030
Training epoch 4 [56320/67107 (83.97%)]	 Loss: 0.000010
Training epoch 4 [56960/67107 (84.92%)]	 Loss: 0.000011
Training epoch 4 [57600/67107 (85.88%)]	 Loss: 0.000023
Training epoch 4 [58240/67107 (86.83%)]	 Loss: 0.000065
Training epoch 4 [58880/67107 (87.79%)]	 Loss: 0.000068
Training epoch 4 [59520/67107 (88.74%)]	 Loss: 0.000050
Training epoch 4 [60160/67107 (89.69%)]	 Loss: 0.000202
Training epoch 4 [60800/67107 (90.65%)]	 Loss: 0.000054
Training epoch 4 [61440/67107 (91.60%)]	 Loss: 0.000025
Training epoch 4 [62080/67107 (92.56%)]	 Loss: 0.000170
Training epoch 4 [62720/67107 (93.51%)]	 Loss: 0.000060
Training epoch 4 [63360/67107 (94.47%)]	 Loss: 0.000200
Training epoch 4 [64000/67107 (95.42%)]	 Loss: 0.000189
Training epoch 4 [64640/67107 (96.37%)]	 Loss: 0.000052
Training epoch 4 [65280/67107 (97.33%)]	 Loss: 0.001064
Training epoch 4 [65920/67107 (98.28%)]	 Loss: 0.000567
Training epoch 4 [66560/67107 (99.24%)]	 Loss: 0.000063
Test set: Average Loss: 0.001595
Training epoch 5 [0/67107 (0.00%)]	 Loss: 0.000118
Training epoch 5 [640/67107 (0.95%)]	 Loss: 0.000048
Training epoch 5 [1280/67107 (1.91%)]	 Loss: 0.000047
Training epoch 5 [1920/67107 (2.86%)]	 Loss: 0.000022
Training epoch 5 [2560/67107 (3.82%)]	 Loss: 0.000022
Training epoch 5 [3200/67107 (4.77%)]	 Loss: 0.000849
Training epoch 5 [3840/67107 (5.73%)]	 Loss: 0.000185
Training epoch 5 [4480/67107 (6.68%)]	 Loss: 0.000274
Training epoch 5 [5120/67107 (7.63%)]	 Loss: 0.000321
Training epoch 5 [5760/67107 (8.59%)]	 Loss: 0.000248
Training epoch 5 [6400/67107 (9.54%)]	 Loss: 0.000433
Training epoch 5 [7040/67107 (10.50%)]	 Loss: 0.000223
Training epoch 5 [7680/67107 (11.45%)]	 Loss: 0.000031
Training epoch 5 [8320/67107 (12.40%)]	 Loss: 0.000034
Training epoch 5 [8960/67107 (13.36%)]	 Loss: 0.000026
Training epoch 5 [9600/67107 (14.31%)]	 Loss: 0.000047
Training epoch 5 [10240/67107 (15.27%)]	 Loss: 0.000026
Training epoch 5 [10880/67107 (16.22%)]	 Loss: 0.000025
Training epoch 5 [11520/67107 (17.18%)]	 Loss: 0.000085
Training epoch 5 [12160/67107 (18.13%)]	 Loss: 0.000607
Training epoch 5 [12800/67107 (19.08%)]	 Loss: 0.000665
Training epoch 5 [13440/67107 (20.04%)]	 Loss: 0.000198
Training epoch 5 [14080/67107 (20.99%)]	 Loss: 0.000180
Training epoch 5 [14720/67107 (21.95%)]	 Loss: 0.000133
Training epoch 5 [15360/67107 (22.90%)]	 Loss: 0.000716
Training epoch 5 [16000/67107 (23.85%)]	 Loss: 0.000404
Training epoch 5 [16640/67107 (24.81%)]	 Loss: 0.000300
Training epoch 5 [17280/67107 (25.76%)]	 Loss: 0.000061
Training epoch 5 [17920/67107 (26.72%)]	 Loss: 0.000054
Training epoch 5 [18560/67107 (27.67%)]	 Loss: 0.000021
Training epoch 5 [19200/67107 (28.63%)]	 Loss: 0.000347
Training epoch 5 [19840/67107 (29.58%)]	 Loss: 0.000134
Training epoch 5 [20480/67107 (30.53%)]	 Loss: 0.000100
Training epoch 5 [21120/67107 (31.49%)]	 Loss: 0.000322
Training epoch 5 [21760/67107 (32.44%)]	 Loss: 0.000138
Training epoch 5 [22400/67107 (33.40%)]	 Loss: 0.000935
Training epoch 5 [23040/67107 (34.35%)]	 Loss: 0.000298
Training epoch 5 [23680/67107 (35.31%)]	 Loss: 0.000077
Training epoch 5 [24320/67107 (36.26%)]	 Loss: 0.000027
Training epoch 5 [24960/67107 (37.21%)]	 Loss: 0.000031
Training epoch 5 [25600/67107 (38.17%)]	 Loss: 0.000022
Training epoch 5 [26240/67107 (39.12%)]	 Loss: 0.000315
Training epoch 5 [26880/67107 (40.08%)]	 Loss: 0.000143
Training epoch 5 [27520/67107 (41.03%)]	 Loss: 0.000095
Training epoch 5 [28160/67107 (41.98%)]	 Loss: 0.000263
Training epoch 5 [28800/67107 (42.94%)]	 Loss: 0.000104
Training epoch 5 [29440/67107 (43.89%)]	 Loss: 0.001180
Training epoch 5 [30080/67107 (44.85%)]	 Loss: 0.000288
Training epoch 5 [30720/67107 (45.80%)]	 Loss: 0.000043
Training epoch 5 [31360/67107 (46.76%)]	 Loss: 0.000014
Training epoch 5 [32000/67107 (47.71%)]	 Loss: 0.000075
Training epoch 5 [32640/67107 (48.66%)]	 Loss: 0.000045
Training epoch 5 [33280/67107 (49.62%)]	 Loss: 0.000368
Training epoch 5 [33920/67107 (50.57%)]	 Loss: 0.000084
Training epoch 5 [34560/67107 (51.53%)]	 Loss: 0.000104
Training epoch 5 [35200/67107 (52.48%)]	 Loss: 0.000319
Training epoch 5 [35840/67107 (53.44%)]	 Loss: 0.000136
Training epoch 5 [36480/67107 (54.39%)]	 Loss: 0.001421
Training epoch 5 [37120/67107 (55.34%)]	 Loss: 0.000372
Training epoch 5 [37760/67107 (56.30%)]	 Loss: 0.000041
Training epoch 5 [38400/67107 (57.25%)]	 Loss: 0.000588
Training epoch 5 [39040/67107 (58.21%)]	 Loss: 0.000090
Training epoch 5 [39680/67107 (59.16%)]	 Loss: 0.000147
Training epoch 5 [40320/67107 (60.11%)]	 Loss: 0.000217
Training epoch 5 [40960/67107 (61.07%)]	 Loss: 0.000149
Training epoch 5 [41600/67107 (62.02%)]	 Loss: 0.000660
Training epoch 5 [42240/67107 (62.98%)]	 Loss: 0.000448
Training epoch 5 [42880/67107 (63.93%)]	 Loss: 0.000067
Training epoch 5 [43520/67107 (64.89%)]	 Loss: 0.000029
Training epoch 5 [44160/67107 (65.84%)]	 Loss: 0.000045
Training epoch 5 [44800/67107 (66.79%)]	 Loss: 0.000054
Training epoch 5 [45440/67107 (67.75%)]	 Loss: 0.000052
Training epoch 5 [46080/67107 (68.70%)]	 Loss: 0.000205
Training epoch 5 [46720/67107 (69.66%)]	 Loss: 0.000364
Training epoch 5 [47360/67107 (70.61%)]	 Loss: 0.000166
Training epoch 5 [48000/67107 (71.56%)]	 Loss: 0.000100
Training epoch 5 [48640/67107 (72.52%)]	 Loss: 0.000149
Training epoch 5 [49280/67107 (73.47%)]	 Loss: 0.000478
Training epoch 5 [49920/67107 (74.43%)]	 Loss: 0.000044
Training epoch 5 [50560/67107 (75.38%)]	 Loss: 0.000010
Training epoch 5 [51200/67107 (76.34%)]	 Loss: 0.000038
Training epoch 5 [51840/67107 (77.29%)]	 Loss: 0.000017
Training epoch 5 [52480/67107 (78.24%)]	 Loss: 0.000206
Training epoch 5 [53120/67107 (79.20%)]	 Loss: 0.000116
Training epoch 5 [53760/67107 (80.15%)]	 Loss: 0.000049
Training epoch 5 [54400/67107 (81.11%)]	 Loss: 0.000050
Training epoch 5 [55040/67107 (82.06%)]	 Loss: 0.000012
Training epoch 5 [55680/67107 (83.02%)]	 Loss: 0.000017
Training epoch 5 [56320/67107 (83.97%)]	 Loss: 0.000339
Training epoch 5 [56960/67107 (84.92%)]	 Loss: 0.000117
Training epoch 5 [57600/67107 (85.88%)]	 Loss: 0.000349
Training epoch 5 [58240/67107 (86.83%)]	 Loss: 0.000310
Training epoch 5 [58880/67107 (87.79%)]	 Loss: 0.000162
Training epoch 5 [59520/67107 (88.74%)]	 Loss: 0.000850
Training epoch 5 [60160/67107 (89.69%)]	 Loss: 0.000526
Training epoch 5 [60800/67107 (90.65%)]	 Loss: 0.000026
Training epoch 5 [61440/67107 (91.60%)]	 Loss: 0.000786
Training epoch 5 [62080/67107 (92.56%)]	 Loss: 0.000139
Training epoch 5 [62720/67107 (93.51%)]	 Loss: 0.000187
Training epoch 5 [63360/67107 (94.47%)]	 Loss: 0.000773
Training epoch 5 [64000/67107 (95.42%)]	 Loss: 0.000181
Training epoch 5 [64640/67107 (96.37%)]	 Loss: 0.000075
Training epoch 5 [65280/67107 (97.33%)]	 Loss: 0.000148
Training epoch 5 [65920/67107 (98.28%)]	 Loss: 0.000073
Training epoch 5 [66560/67107 (99.24%)]	 Loss: 0.000023
Test set: Average Loss: 0.000426
Training epoch 6 [0/67107 (0.00%)]	 Loss: 0.000018
Training epoch 6 [640/67107 (0.95%)]	 Loss: 0.000020
Training epoch 6 [1280/67107 (1.91%)]	 Loss: 0.000061
Training epoch 6 [1920/67107 (2.86%)]	 Loss: 0.000424
Training epoch 6 [2560/67107 (3.82%)]	 Loss: 0.000075
Training epoch 6 [3200/67107 (4.77%)]	 Loss: 0.000049
Training epoch 6 [3840/67107 (5.73%)]	 Loss: 0.000062
Training epoch 6 [4480/67107 (6.68%)]	 Loss: 0.000156
Training epoch 6 [5120/67107 (7.63%)]	 Loss: 0.000043
Training epoch 6 [5760/67107 (8.59%)]	 Loss: 0.000054
Training epoch 6 [6400/67107 (9.54%)]	 Loss: 0.000037
Training epoch 6 [7040/67107 (10.50%)]	 Loss: 0.000045
Training epoch 6 [7680/67107 (11.45%)]	 Loss: 0.000577
Training epoch 6 [8320/67107 (12.40%)]	 Loss: 0.000106
Training epoch 6 [8960/67107 (13.36%)]	 Loss: 0.000090
Training epoch 6 [9600/67107 (14.31%)]	 Loss: 0.000110
Training epoch 6 [10240/67107 (15.27%)]	 Loss: 0.000148
Training epoch 6 [10880/67107 (16.22%)]	 Loss: 0.000024
Training epoch 6 [11520/67107 (17.18%)]	 Loss: 0.000114
Training epoch 6 [12160/67107 (18.13%)]	 Loss: 0.000028
Training epoch 6 [12800/67107 (19.08%)]	 Loss: 0.000092
Training epoch 6 [13440/67107 (20.04%)]	 Loss: 0.000767
Training epoch 6 [14080/67107 (20.99%)]	 Loss: 0.000102
Training epoch 6 [14720/67107 (21.95%)]	 Loss: 0.000193
Training epoch 6 [15360/67107 (22.90%)]	 Loss: 0.000108
Training epoch 6 [16000/67107 (23.85%)]	 Loss: 0.000122
Training epoch 6 [16640/67107 (24.81%)]	 Loss: 0.000024
Training epoch 6 [17280/67107 (25.76%)]	 Loss: 0.000085
Training epoch 6 [17920/67107 (26.72%)]	 Loss: 0.000007
Training epoch 6 [18560/67107 (27.67%)]	 Loss: 0.000033
Training epoch 6 [19200/67107 (28.63%)]	 Loss: 0.000010
Training epoch 6 [19840/67107 (29.58%)]	 Loss: 0.000042
Training epoch 6 [20480/67107 (30.53%)]	 Loss: 0.000346
Training epoch 6 [21120/67107 (31.49%)]	 Loss: 0.000419
Training epoch 6 [21760/67107 (32.44%)]	 Loss: 0.000314
Training epoch 6 [22400/67107 (33.40%)]	 Loss: 0.000067
Training epoch 6 [23040/67107 (34.35%)]	 Loss: 0.000653
Training epoch 6 [23680/67107 (35.31%)]	 Loss: 0.000205
Training epoch 6 [24320/67107 (36.26%)]	 Loss: 0.000027
Training epoch 6 [24960/67107 (37.21%)]	 Loss: 0.000329
Training epoch 6 [25600/67107 (38.17%)]	 Loss: 0.000068
Training epoch 6 [26240/67107 (39.12%)]	 Loss: 0.000134
Training epoch 6 [26880/67107 (40.08%)]	 Loss: 0.001589
Training epoch 6 [27520/67107 (41.03%)]	 Loss: 0.000710
Training epoch 6 [28160/67107 (41.98%)]	 Loss: 0.000070
Training epoch 6 [28800/67107 (42.94%)]	 Loss: 0.000141
Training epoch 6 [29440/67107 (43.89%)]	 Loss: 0.000075
Training epoch 6 [30080/67107 (44.85%)]	 Loss: 0.000029
Training epoch 6 [30720/67107 (45.80%)]	 Loss: 0.000033
Training epoch 6 [31360/67107 (46.76%)]	 Loss: 0.000007
Training epoch 6 [32000/67107 (47.71%)]	 Loss: 0.000010
Training epoch 6 [32640/67107 (48.66%)]	 Loss: 0.000027
Training epoch 6 [33280/67107 (49.62%)]	 Loss: 0.000083
Training epoch 6 [33920/67107 (50.57%)]	 Loss: 0.000380
Training epoch 6 [34560/67107 (51.53%)]	 Loss: 0.000850
Training epoch 6 [35200/67107 (52.48%)]	 Loss: 0.000329
Training epoch 6 [35840/67107 (53.44%)]	 Loss: 0.000036
Training epoch 6 [36480/67107 (54.39%)]	 Loss: 0.000162
Training epoch 6 [37120/67107 (55.34%)]	 Loss: 0.000180
Training epoch 6 [37760/67107 (56.30%)]	 Loss: 0.000191
Training epoch 6 [38400/67107 (57.25%)]	 Loss: 0.000112
Training epoch 6 [39040/67107 (58.21%)]	 Loss: 0.000039
Training epoch 6 [39680/67107 (59.16%)]	 Loss: 0.000013
Training epoch 6 [40320/67107 (60.11%)]	 Loss: 0.000047
Training epoch 6 [40960/67107 (61.07%)]	 Loss: 0.000320
Training epoch 6 [41600/67107 (62.02%)]	 Loss: 0.000120
Training epoch 6 [42240/67107 (62.98%)]	 Loss: 0.000066
Training epoch 6 [42880/67107 (63.93%)]	 Loss: 0.000432
Training epoch 6 [43520/67107 (64.89%)]	 Loss: 0.000092
Training epoch 6 [44160/67107 (65.84%)]	 Loss: 0.000598
Training epoch 6 [44800/67107 (66.79%)]	 Loss: 0.000226
Training epoch 6 [45440/67107 (67.75%)]	 Loss: 0.000100
Training epoch 6 [46080/67107 (68.70%)]	 Loss: 0.000020
Training epoch 6 [46720/67107 (69.66%)]	 Loss: 0.000020
Training epoch 6 [47360/67107 (70.61%)]	 Loss: 0.000112
Training epoch 6 [48000/67107 (71.56%)]	 Loss: 0.000543
Training epoch 6 [48640/67107 (72.52%)]	 Loss: 0.000589
Training epoch 6 [49280/67107 (73.47%)]	 Loss: 0.000148
Training epoch 6 [49920/67107 (74.43%)]	 Loss: 0.000142
Training epoch 6 [50560/67107 (75.38%)]	 Loss: 0.000120
Training epoch 6 [51200/67107 (76.34%)]	 Loss: 0.001058
Training epoch 6 [51840/67107 (77.29%)]	 Loss: 0.000547
Training epoch 6 [52480/67107 (78.24%)]	 Loss: 0.000078
Training epoch 6 [53120/67107 (79.20%)]	 Loss: 0.000552
Training epoch 6 [53760/67107 (80.15%)]	 Loss: 0.000063
Training epoch 6 [54400/67107 (81.11%)]	 Loss: 0.000101
Training epoch 6 [55040/67107 (82.06%)]	 Loss: 0.000112
Training epoch 6 [55680/67107 (83.02%)]	 Loss: 0.000425
Training epoch 6 [56320/67107 (83.97%)]	 Loss: 0.000529
Training epoch 6 [56960/67107 (84.92%)]	 Loss: 0.000104
Training epoch 6 [57600/67107 (85.88%)]	 Loss: 0.000053
Training epoch 6 [58240/67107 (86.83%)]	 Loss: 0.000037
Training epoch 6 [58880/67107 (87.79%)]	 Loss: 0.000009
Training epoch 6 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 6 [60160/67107 (89.69%)]	 Loss: 0.000007
Training epoch 6 [60800/67107 (90.65%)]	 Loss: 0.000084
Training epoch 6 [61440/67107 (91.60%)]	 Loss: 0.000422
Training epoch 6 [62080/67107 (92.56%)]	 Loss: 0.000904
Training epoch 6 [62720/67107 (93.51%)]	 Loss: 0.000716
Training epoch 6 [63360/67107 (94.47%)]	 Loss: 0.000368
Training epoch 6 [64000/67107 (95.42%)]	 Loss: 0.001250
Training epoch 6 [64640/67107 (96.37%)]	 Loss: 0.000279
Training epoch 6 [65280/67107 (97.33%)]	 Loss: 0.000030
Training epoch 6 [65920/67107 (98.28%)]	 Loss: 0.000047
Training epoch 6 [66560/67107 (99.24%)]	 Loss: 0.000026
Test set: Average Loss: 0.004606
Training epoch 7 [0/67107 (0.00%)]	 Loss: 0.000034
Training epoch 7 [640/67107 (0.95%)]	 Loss: 0.000151
Training epoch 7 [1280/67107 (1.91%)]	 Loss: 0.000511
Training epoch 7 [1920/67107 (2.86%)]	 Loss: 0.000841
Training epoch 7 [2560/67107 (3.82%)]	 Loss: 0.000236
Training epoch 7 [3200/67107 (4.77%)]	 Loss: 0.000069
Training epoch 7 [3840/67107 (5.73%)]	 Loss: 0.000137
Training epoch 7 [4480/67107 (6.68%)]	 Loss: 0.000232
Training epoch 7 [5120/67107 (7.63%)]	 Loss: 0.000164
Training epoch 7 [5760/67107 (8.59%)]	 Loss: 0.000012
Training epoch 7 [6400/67107 (9.54%)]	 Loss: 0.000049
Training epoch 7 [7040/67107 (10.50%)]	 Loss: 0.000270
Training epoch 7 [7680/67107 (11.45%)]	 Loss: 0.000220
Training epoch 7 [8320/67107 (12.40%)]	 Loss: 0.000256
Training epoch 7 [8960/67107 (13.36%)]	 Loss: 0.000755
Training epoch 7 [9600/67107 (14.31%)]	 Loss: 0.000659
Training epoch 7 [10240/67107 (15.27%)]	 Loss: 0.000192
Training epoch 7 [10880/67107 (16.22%)]	 Loss: 0.000077
Training epoch 7 [11520/67107 (17.18%)]	 Loss: 0.000213
Training epoch 7 [12160/67107 (18.13%)]	 Loss: 0.000142
Training epoch 7 [12800/67107 (19.08%)]	 Loss: 0.000046
Training epoch 7 [13440/67107 (20.04%)]	 Loss: 0.000187
Training epoch 7 [14080/67107 (20.99%)]	 Loss: 0.000075
Training epoch 7 [14720/67107 (21.95%)]	 Loss: 0.000099
Training epoch 7 [15360/67107 (22.90%)]	 Loss: 0.001408
Training epoch 7 [16000/67107 (23.85%)]	 Loss: 0.000535
Training epoch 7 [16640/67107 (24.81%)]	 Loss: 0.000037
Training epoch 7 [17280/67107 (25.76%)]	 Loss: 0.000126
Training epoch 7 [17920/67107 (26.72%)]	 Loss: 0.000016
Training epoch 7 [18560/67107 (27.67%)]	 Loss: 0.000027
Training epoch 7 [19200/67107 (28.63%)]	 Loss: 0.000040
Training epoch 7 [19840/67107 (29.58%)]	 Loss: 0.000051
Training epoch 7 [20480/67107 (30.53%)]	 Loss: 0.001097
Training epoch 7 [21120/67107 (31.49%)]	 Loss: 0.000165
Training epoch 7 [21760/67107 (32.44%)]	 Loss: 0.000237
Training epoch 7 [22400/67107 (33.40%)]	 Loss: 0.001188
Training epoch 7 [23040/67107 (34.35%)]	 Loss: 0.000544
Training epoch 7 [23680/67107 (35.31%)]	 Loss: 0.000048
Training epoch 7 [24320/67107 (36.26%)]	 Loss: 0.000101
Training epoch 7 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 7 [25600/67107 (38.17%)]	 Loss: 0.000018
Training epoch 7 [26240/67107 (39.12%)]	 Loss: 0.000025
Training epoch 7 [26880/67107 (40.08%)]	 Loss: 0.000030
Training epoch 7 [27520/67107 (41.03%)]	 Loss: 0.000015
Training epoch 7 [28160/67107 (41.98%)]	 Loss: 0.000018
Training epoch 7 [28800/67107 (42.94%)]	 Loss: 0.000827
Training epoch 7 [29440/67107 (43.89%)]	 Loss: 0.000288
Training epoch 7 [30080/67107 (44.85%)]	 Loss: 0.000288
Training epoch 7 [30720/67107 (45.80%)]	 Loss: 0.000208
Training epoch 7 [31360/67107 (46.76%)]	 Loss: 0.000273
Training epoch 7 [32000/67107 (47.71%)]	 Loss: 0.000116
Training epoch 7 [32640/67107 (48.66%)]	 Loss: 0.000147
Training epoch 7 [33280/67107 (49.62%)]	 Loss: 0.000319
Training epoch 7 [33920/67107 (50.57%)]	 Loss: 0.000080
Training epoch 7 [34560/67107 (51.53%)]	 Loss: 0.000017
Training epoch 7 [35200/67107 (52.48%)]	 Loss: 0.000024
Training epoch 7 [35840/67107 (53.44%)]	 Loss: 0.000874
Training epoch 7 [36480/67107 (54.39%)]	 Loss: 0.000163
Training epoch 7 [37120/67107 (55.34%)]	 Loss: 0.000356
Training epoch 7 [37760/67107 (56.30%)]	 Loss: 0.001097
Training epoch 7 [38400/67107 (57.25%)]	 Loss: 0.000271
Training epoch 7 [39040/67107 (58.21%)]	 Loss: 0.000160
Training epoch 7 [39680/67107 (59.16%)]	 Loss: 0.000164
Training epoch 7 [40320/67107 (60.11%)]	 Loss: 0.000602
Training epoch 7 [40960/67107 (61.07%)]	 Loss: 0.000602
Training epoch 7 [41600/67107 (62.02%)]	 Loss: 0.000024
Training epoch 7 [42240/67107 (62.98%)]	 Loss: 0.000180
Training epoch 7 [42880/67107 (63.93%)]	 Loss: 0.000238
Training epoch 7 [43520/67107 (64.89%)]	 Loss: 0.000343
Training epoch 7 [44160/67107 (65.84%)]	 Loss: 0.001249
Training epoch 7 [44800/67107 (66.79%)]	 Loss: 0.000248
Training epoch 7 [45440/67107 (67.75%)]	 Loss: 0.000138
Training epoch 7 [46080/67107 (68.70%)]	 Loss: 0.000053
Training epoch 7 [46720/67107 (69.66%)]	 Loss: 0.000020
Training epoch 7 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 7 [48000/67107 (71.56%)]	 Loss: 0.000082
Training epoch 7 [48640/67107 (72.52%)]	 Loss: 0.000033
Training epoch 7 [49280/67107 (73.47%)]	 Loss: 0.000021
Training epoch 7 [49920/67107 (74.43%)]	 Loss: 0.000199
Training epoch 7 [50560/67107 (75.38%)]	 Loss: 0.000097
Training epoch 7 [51200/67107 (76.34%)]	 Loss: 0.000057
Training epoch 7 [51840/67107 (77.29%)]	 Loss: 0.000931
Training epoch 7 [52480/67107 (78.24%)]	 Loss: 0.000611
Training epoch 7 [53120/67107 (79.20%)]	 Loss: 0.000044
Training epoch 7 [53760/67107 (80.15%)]	 Loss: 0.000178
Training epoch 7 [54400/67107 (81.11%)]	 Loss: 0.000275
Training epoch 7 [55040/67107 (82.06%)]	 Loss: 0.000419
Training epoch 7 [55680/67107 (83.02%)]	 Loss: 0.000096
Training epoch 7 [56320/67107 (83.97%)]	 Loss: 0.000598
Training epoch 7 [56960/67107 (84.92%)]	 Loss: 0.000179
Training epoch 7 [57600/67107 (85.88%)]	 Loss: 0.000245
Training epoch 7 [58240/67107 (86.83%)]	 Loss: 0.000350
Training epoch 7 [58880/67107 (87.79%)]	 Loss: 0.000335
Training epoch 7 [59520/67107 (88.74%)]	 Loss: 0.000073
Training epoch 7 [60160/67107 (89.69%)]	 Loss: 0.000168
Training epoch 7 [60800/67107 (90.65%)]	 Loss: 0.000061
Training epoch 7 [61440/67107 (91.60%)]	 Loss: 0.000015
Training epoch 7 [62080/67107 (92.56%)]	 Loss: 0.000011
Training epoch 7 [62720/67107 (93.51%)]	 Loss: 0.000018
Training epoch 7 [63360/67107 (94.47%)]	 Loss: 0.000340
Training epoch 7 [64000/67107 (95.42%)]	 Loss: 0.000012
Training epoch 7 [64640/67107 (96.37%)]	 Loss: 0.000148
Training epoch 7 [65280/67107 (97.33%)]	 Loss: 0.001480
Training epoch 7 [65920/67107 (98.28%)]	 Loss: 0.000650
Training epoch 7 [66560/67107 (99.24%)]	 Loss: 0.000061
Test set: Average Loss: 0.000606
Training epoch 8 [0/67107 (0.00%)]	 Loss: 0.000113
Training epoch 8 [640/67107 (0.95%)]	 Loss: 0.000038
Training epoch 8 [1280/67107 (1.91%)]	 Loss: 0.000016
Training epoch 8 [1920/67107 (2.86%)]	 Loss: 0.000011
Training epoch 8 [2560/67107 (3.82%)]	 Loss: 0.000034
Training epoch 8 [3200/67107 (4.77%)]	 Loss: 0.000148
Training epoch 8 [3840/67107 (5.73%)]	 Loss: 0.000044
Training epoch 8 [4480/67107 (6.68%)]	 Loss: 0.000097
Training epoch 8 [5120/67107 (7.63%)]	 Loss: 0.001165
Training epoch 8 [5760/67107 (8.59%)]	 Loss: 0.000722
Training epoch 8 [6400/67107 (9.54%)]	 Loss: 0.000048
Training epoch 8 [7040/67107 (10.50%)]	 Loss: 0.000046
Training epoch 8 [7680/67107 (11.45%)]	 Loss: 0.000065
Training epoch 8 [8320/67107 (12.40%)]	 Loss: 0.000019
Training epoch 8 [8960/67107 (13.36%)]	 Loss: 0.000070
Training epoch 8 [9600/67107 (14.31%)]	 Loss: 0.000008
Training epoch 8 [10240/67107 (15.27%)]	 Loss: 0.000034
Training epoch 8 [10880/67107 (16.22%)]	 Loss: 0.000031
Training epoch 8 [11520/67107 (17.18%)]	 Loss: 0.000600
Training epoch 8 [12160/67107 (18.13%)]	 Loss: 0.000427
Training epoch 8 [12800/67107 (19.08%)]	 Loss: 0.000282
Training epoch 8 [13440/67107 (20.04%)]	 Loss: 0.000122
Training epoch 8 [14080/67107 (20.99%)]	 Loss: 0.000168
Training epoch 8 [14720/67107 (21.95%)]	 Loss: 0.000042
Training epoch 8 [15360/67107 (22.90%)]	 Loss: 0.000188
Training epoch 8 [16000/67107 (23.85%)]	 Loss: 0.000552
Training epoch 8 [16640/67107 (24.81%)]	 Loss: 0.000034
Training epoch 8 [17280/67107 (25.76%)]	 Loss: 0.000103
Training epoch 8 [17920/67107 (26.72%)]	 Loss: 0.000342
Training epoch 8 [18560/67107 (27.67%)]	 Loss: 0.000107
Training epoch 8 [19200/67107 (28.63%)]	 Loss: 0.000189
Training epoch 8 [19840/67107 (29.58%)]	 Loss: 0.000177
Training epoch 8 [20480/67107 (30.53%)]	 Loss: 0.000622
Training epoch 8 [21120/67107 (31.49%)]	 Loss: 0.000091
Training epoch 8 [21760/67107 (32.44%)]	 Loss: 0.000029
Training epoch 8 [22400/67107 (33.40%)]	 Loss: 0.000030
Training epoch 8 [23040/67107 (34.35%)]	 Loss: 0.000045
Training epoch 8 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 8 [24320/67107 (36.26%)]	 Loss: 0.000046
Training epoch 8 [24960/67107 (37.21%)]	 Loss: 0.000006
Training epoch 8 [25600/67107 (38.17%)]	 Loss: 0.000065
Training epoch 8 [26240/67107 (39.12%)]	 Loss: 0.000622
Training epoch 8 [26880/67107 (40.08%)]	 Loss: 0.001046
Training epoch 8 [27520/67107 (41.03%)]	 Loss: 0.000536
Training epoch 8 [28160/67107 (41.98%)]	 Loss: 0.000171
Training epoch 8 [28800/67107 (42.94%)]	 Loss: 0.000451
Training epoch 8 [29440/67107 (43.89%)]	 Loss: 0.000286
Training epoch 8 [30080/67107 (44.85%)]	 Loss: 0.000011
Training epoch 8 [30720/67107 (45.80%)]	 Loss: 0.000941
Training epoch 8 [31360/67107 (46.76%)]	 Loss: 0.000028
Training epoch 8 [32000/67107 (47.71%)]	 Loss: 0.000136
Training epoch 8 [32640/67107 (48.66%)]	 Loss: 0.000598
Training epoch 8 [33280/67107 (49.62%)]	 Loss: 0.000291
Training epoch 8 [33920/67107 (50.57%)]	 Loss: 0.000098
Training epoch 8 [34560/67107 (51.53%)]	 Loss: 0.000131
Training epoch 8 [35200/67107 (52.48%)]	 Loss: 0.000067
Training epoch 8 [35840/67107 (53.44%)]	 Loss: 0.000010
Training epoch 8 [36480/67107 (54.39%)]	 Loss: 0.000014
Training epoch 8 [37120/67107 (55.34%)]	 Loss: 0.000015
Training epoch 8 [37760/67107 (56.30%)]	 Loss: 0.000886
Training epoch 8 [38400/67107 (57.25%)]	 Loss: 0.000098
Training epoch 8 [39040/67107 (58.21%)]	 Loss: 0.000119
Training epoch 8 [39680/67107 (59.16%)]	 Loss: 0.000063
Training epoch 8 [40320/67107 (60.11%)]	 Loss: 0.000504
Training epoch 8 [40960/67107 (61.07%)]	 Loss: 0.000248
Training epoch 8 [41600/67107 (62.02%)]	 Loss: 0.000143
Training epoch 8 [42240/67107 (62.98%)]	 Loss: 0.000370
Training epoch 8 [42880/67107 (63.93%)]	 Loss: 0.000130
Training epoch 8 [43520/67107 (64.89%)]	 Loss: 0.000027
Training epoch 8 [44160/67107 (65.84%)]	 Loss: 0.000045
Training epoch 8 [44800/67107 (66.79%)]	 Loss: 0.000058
Training epoch 8 [45440/67107 (67.75%)]	 Loss: 0.000133
Training epoch 8 [46080/67107 (68.70%)]	 Loss: 0.000361
Training epoch 8 [46720/67107 (69.66%)]	 Loss: 0.001046
Training epoch 8 [47360/67107 (70.61%)]	 Loss: 0.000306
Training epoch 8 [48000/67107 (71.56%)]	 Loss: 0.000049
Training epoch 8 [48640/67107 (72.52%)]	 Loss: 0.000187
Training epoch 8 [49280/67107 (73.47%)]	 Loss: 0.000161
Training epoch 8 [49920/67107 (74.43%)]	 Loss: 0.000157
Training epoch 8 [50560/67107 (75.38%)]	 Loss: 0.000295
Training epoch 8 [51200/67107 (76.34%)]	 Loss: 0.000026
Training epoch 8 [51840/67107 (77.29%)]	 Loss: 0.000025
Training epoch 8 [52480/67107 (78.24%)]	 Loss: 0.000684
Training epoch 8 [53120/67107 (79.20%)]	 Loss: 0.000123
Training epoch 8 [53760/67107 (80.15%)]	 Loss: 0.000446
Training epoch 8 [54400/67107 (81.11%)]	 Loss: 0.001064
Training epoch 8 [55040/67107 (82.06%)]	 Loss: 0.000275
Training epoch 8 [55680/67107 (83.02%)]	 Loss: 0.000193
Training epoch 8 [56320/67107 (83.97%)]	 Loss: 0.000230
Training epoch 8 [56960/67107 (84.92%)]	 Loss: 0.000905
Training epoch 8 [57600/67107 (85.88%)]	 Loss: 0.000464
Training epoch 8 [58240/67107 (86.83%)]	 Loss: 0.000081
Training epoch 8 [58880/67107 (87.79%)]	 Loss: 0.000041
Training epoch 8 [59520/67107 (88.74%)]	 Loss: 0.000224
Training epoch 8 [60160/67107 (89.69%)]	 Loss: 0.000403
Training epoch 8 [60800/67107 (90.65%)]	 Loss: 0.001483
Training epoch 8 [61440/67107 (91.60%)]	 Loss: 0.000316
Training epoch 8 [62080/67107 (92.56%)]	 Loss: 0.000678
Training epoch 8 [62720/67107 (93.51%)]	 Loss: 0.000053
Training epoch 8 [63360/67107 (94.47%)]	 Loss: 0.000021
Training epoch 8 [64000/67107 (95.42%)]	 Loss: 0.000035
Training epoch 8 [64640/67107 (96.37%)]	 Loss: 0.000009
Training epoch 8 [65280/67107 (97.33%)]	 Loss: 0.000023
Training epoch 8 [65920/67107 (98.28%)]	 Loss: 0.000021
Training epoch 8 [66560/67107 (99.24%)]	 Loss: 0.000592
Test set: Average Loss: 0.000615
Training epoch 9 [0/67107 (0.00%)]	 Loss: 0.000148
Training epoch 9 [640/67107 (0.95%)]	 Loss: 0.000033
Training epoch 9 [1280/67107 (1.91%)]	 Loss: 0.000074
Training epoch 9 [1920/67107 (2.86%)]	 Loss: 0.000274
Training epoch 9 [2560/67107 (3.82%)]	 Loss: 0.000080
Training epoch 9 [3200/67107 (4.77%)]	 Loss: 0.000080
Training epoch 9 [3840/67107 (5.73%)]	 Loss: 0.000183
Training epoch 9 [4480/67107 (6.68%)]	 Loss: 0.000036
Training epoch 9 [5120/67107 (7.63%)]	 Loss: 0.000087
Training epoch 9 [5760/67107 (8.59%)]	 Loss: 0.000057
Training epoch 9 [6400/67107 (9.54%)]	 Loss: 0.000158
Training epoch 9 [7040/67107 (10.50%)]	 Loss: 0.001521
Training epoch 9 [7680/67107 (11.45%)]	 Loss: 0.000704
Training epoch 9 [8320/67107 (12.40%)]	 Loss: 0.000071
Training epoch 9 [8960/67107 (13.36%)]	 Loss: 0.000137
Training epoch 9 [9600/67107 (14.31%)]	 Loss: 0.000021
Training epoch 9 [10240/67107 (15.27%)]	 Loss: 0.000046
Training epoch 9 [10880/67107 (16.22%)]	 Loss: 0.000053
Training epoch 9 [11520/67107 (17.18%)]	 Loss: 0.000061
Training epoch 9 [12160/67107 (18.13%)]	 Loss: 0.000032
Training epoch 9 [12800/67107 (19.08%)]	 Loss: 0.000087
Training epoch 9 [13440/67107 (20.04%)]	 Loss: 0.000198
Training epoch 9 [14080/67107 (20.99%)]	 Loss: 0.000038
Training epoch 9 [14720/67107 (21.95%)]	 Loss: 0.000043
Training epoch 9 [15360/67107 (22.90%)]	 Loss: 0.000041
Training epoch 9 [16000/67107 (23.85%)]	 Loss: 0.000014
Training epoch 9 [16640/67107 (24.81%)]	 Loss: 0.000008
Training epoch 9 [17280/67107 (25.76%)]	 Loss: 0.000219
Training epoch 9 [17920/67107 (26.72%)]	 Loss: 0.000117
Training epoch 9 [18560/67107 (27.67%)]	 Loss: 0.000012
Training epoch 9 [19200/67107 (28.63%)]	 Loss: 0.000328
Training epoch 9 [19840/67107 (29.58%)]	 Loss: 0.000227
Training epoch 9 [20480/67107 (30.53%)]	 Loss: 0.000782
Training epoch 9 [21120/67107 (31.49%)]	 Loss: 0.000428
Training epoch 9 [21760/67107 (32.44%)]	 Loss: 0.000074
Training epoch 9 [22400/67107 (33.40%)]	 Loss: 0.000571
Training epoch 9 [23040/67107 (34.35%)]	 Loss: 0.000145
Training epoch 9 [23680/67107 (35.31%)]	 Loss: 0.000118
Training epoch 9 [24320/67107 (36.26%)]	 Loss: 0.000177
Training epoch 9 [24960/67107 (37.21%)]	 Loss: 0.000166
Training epoch 9 [25600/67107 (38.17%)]	 Loss: 0.000038
Training epoch 9 [26240/67107 (39.12%)]	 Loss: 0.000444
Training epoch 9 [26880/67107 (40.08%)]	 Loss: 0.000069
Training epoch 9 [27520/67107 (41.03%)]	 Loss: 0.000114
Training epoch 9 [28160/67107 (41.98%)]	 Loss: 0.000026
Training epoch 9 [28800/67107 (42.94%)]	 Loss: 0.000044
Training epoch 9 [29440/67107 (43.89%)]	 Loss: 0.000012
Training epoch 9 [30080/67107 (44.85%)]	 Loss: 0.000161
Training epoch 9 [30720/67107 (45.80%)]	 Loss: 0.000470
Training epoch 9 [31360/67107 (46.76%)]	 Loss: 0.000161
Training epoch 9 [32000/67107 (47.71%)]	 Loss: 0.000059
Training epoch 9 [32640/67107 (48.66%)]	 Loss: 0.000176
Training epoch 9 [33280/67107 (49.62%)]	 Loss: 0.000364
Training epoch 9 [33920/67107 (50.57%)]	 Loss: 0.000062
Training epoch 9 [34560/67107 (51.53%)]	 Loss: 0.000018
Training epoch 9 [35200/67107 (52.48%)]	 Loss: 0.000013
Training epoch 9 [35840/67107 (53.44%)]	 Loss: 0.000020
Training epoch 9 [36480/67107 (54.39%)]	 Loss: 0.000009
Training epoch 9 [37120/67107 (55.34%)]	 Loss: 0.000006
Training epoch 9 [37760/67107 (56.30%)]	 Loss: 0.000594
Training epoch 9 [38400/67107 (57.25%)]	 Loss: 0.000176
Training epoch 9 [39040/67107 (58.21%)]	 Loss: 0.000393
Training epoch 9 [39680/67107 (59.16%)]	 Loss: 0.000099
Training epoch 9 [40320/67107 (60.11%)]	 Loss: 0.000023
Training epoch 9 [40960/67107 (61.07%)]	 Loss: 0.000157
Training epoch 9 [41600/67107 (62.02%)]	 Loss: 0.000221
Training epoch 9 [42240/67107 (62.98%)]	 Loss: 0.000006
Training epoch 9 [42880/67107 (63.93%)]	 Loss: 0.000015
Training epoch 9 [43520/67107 (64.89%)]	 Loss: 0.000030
Training epoch 9 [44160/67107 (65.84%)]	 Loss: 0.000859
Training epoch 9 [44800/67107 (66.79%)]	 Loss: 0.000071
Training epoch 9 [45440/67107 (67.75%)]	 Loss: 0.000146
Training epoch 9 [46080/67107 (68.70%)]	 Loss: 0.000015
Training epoch 9 [46720/67107 (69.66%)]	 Loss: 0.000541
Training epoch 9 [47360/67107 (70.61%)]	 Loss: 0.000428
Training epoch 9 [48000/67107 (71.56%)]	 Loss: 0.000259
Training epoch 9 [48640/67107 (72.52%)]	 Loss: 0.000395
Training epoch 9 [49280/67107 (73.47%)]	 Loss: 0.000052
Training epoch 9 [49920/67107 (74.43%)]	 Loss: 0.000053
Training epoch 9 [50560/67107 (75.38%)]	 Loss: 0.000062
Training epoch 9 [51200/67107 (76.34%)]	 Loss: 0.000097
Training epoch 9 [51840/67107 (77.29%)]	 Loss: 0.000211
Training epoch 9 [52480/67107 (78.24%)]	 Loss: 0.000270
Training epoch 9 [53120/67107 (79.20%)]	 Loss: 0.000859
Training epoch 9 [53760/67107 (80.15%)]	 Loss: 0.000358
Training epoch 9 [54400/67107 (81.11%)]	 Loss: 0.000052
Training epoch 9 [55040/67107 (82.06%)]	 Loss: 0.000059
Training epoch 9 [55680/67107 (83.02%)]	 Loss: 0.000062
Training epoch 9 [56320/67107 (83.97%)]	 Loss: 0.000178
Training epoch 9 [56960/67107 (84.92%)]	 Loss: 0.000065
Training epoch 9 [57600/67107 (85.88%)]	 Loss: 0.000011
Training epoch 9 [58240/67107 (86.83%)]	 Loss: 0.000006
Training epoch 9 [58880/67107 (87.79%)]	 Loss: 0.000265
Training epoch 9 [59520/67107 (88.74%)]	 Loss: 0.000342
Training epoch 9 [60160/67107 (89.69%)]	 Loss: 0.000044
Training epoch 9 [60800/67107 (90.65%)]	 Loss: 0.000043
Training epoch 9 [61440/67107 (91.60%)]	 Loss: 0.000240
Training epoch 9 [62080/67107 (92.56%)]	 Loss: 0.000035
Training epoch 9 [62720/67107 (93.51%)]	 Loss: 0.000322
Training epoch 9 [63360/67107 (94.47%)]	 Loss: 0.000158
Training epoch 9 [64000/67107 (95.42%)]	 Loss: 0.000144
Training epoch 9 [64640/67107 (96.37%)]	 Loss: 0.000150
Training epoch 9 [65280/67107 (97.33%)]	 Loss: 0.000018
Training epoch 9 [65920/67107 (98.28%)]	 Loss: 0.000063
Training epoch 9 [66560/67107 (99.24%)]	 Loss: 0.000019
Test set: Average Loss: 0.002479
Training epoch 10 [0/67107 (0.00%)]	 Loss: 0.000206
Training epoch 10 [640/67107 (0.95%)]	 Loss: 0.000281
Training epoch 10 [1280/67107 (1.91%)]	 Loss: 0.000041
Training epoch 10 [1920/67107 (2.86%)]	 Loss: 0.000124
Training epoch 10 [2560/67107 (3.82%)]	 Loss: 0.000227
Training epoch 10 [3200/67107 (4.77%)]	 Loss: 0.000117
Training epoch 10 [3840/67107 (5.73%)]	 Loss: 0.000664
Training epoch 10 [4480/67107 (6.68%)]	 Loss: 0.000281
Training epoch 10 [5120/67107 (7.63%)]	 Loss: 0.000092
Training epoch 10 [5760/67107 (8.59%)]	 Loss: 0.000045
Training epoch 10 [6400/67107 (9.54%)]	 Loss: 0.000013
Training epoch 10 [7040/67107 (10.50%)]	 Loss: 0.000041
Training epoch 10 [7680/67107 (11.45%)]	 Loss: 0.000106
Training epoch 10 [8320/67107 (12.40%)]	 Loss: 0.000231
Training epoch 10 [8960/67107 (13.36%)]	 Loss: 0.000023
Training epoch 10 [9600/67107 (14.31%)]	 Loss: 0.000621
Training epoch 10 [10240/67107 (15.27%)]	 Loss: 0.000660
Training epoch 10 [10880/67107 (16.22%)]	 Loss: 0.000141
Training epoch 10 [11520/67107 (17.18%)]	 Loss: 0.000079
Training epoch 10 [12160/67107 (18.13%)]	 Loss: 0.000043
Training epoch 10 [12800/67107 (19.08%)]	 Loss: 0.000301
Training epoch 10 [13440/67107 (20.04%)]	 Loss: 0.000068
Training epoch 10 [14080/67107 (20.99%)]	 Loss: 0.000191
Training epoch 10 [14720/67107 (21.95%)]	 Loss: 0.000117
Training epoch 10 [15360/67107 (22.90%)]	 Loss: 0.000097
Training epoch 10 [16000/67107 (23.85%)]	 Loss: 0.000132
Training epoch 10 [16640/67107 (24.81%)]	 Loss: 0.000060
Training epoch 10 [17280/67107 (25.76%)]	 Loss: 0.000177
Training epoch 10 [17920/67107 (26.72%)]	 Loss: 0.000060
Training epoch 10 [18560/67107 (27.67%)]	 Loss: 0.000038
Training epoch 10 [19200/67107 (28.63%)]	 Loss: 0.000050
Training epoch 10 [19840/67107 (29.58%)]	 Loss: 0.000048
Training epoch 10 [20480/67107 (30.53%)]	 Loss: 0.000013
Training epoch 10 [21120/67107 (31.49%)]	 Loss: 0.000719
Training epoch 10 [21760/67107 (32.44%)]	 Loss: 0.000156
Training epoch 10 [22400/67107 (33.40%)]	 Loss: 0.000266
Training epoch 10 [23040/67107 (34.35%)]	 Loss: 0.001407
Training epoch 10 [23680/67107 (35.31%)]	 Loss: 0.000513
Training epoch 10 [24320/67107 (36.26%)]	 Loss: 0.000026
Training epoch 10 [24960/67107 (37.21%)]	 Loss: 0.000125
Training epoch 10 [25600/67107 (38.17%)]	 Loss: 0.000024
Training epoch 10 [26240/67107 (39.12%)]	 Loss: 0.000025
Training epoch 10 [26880/67107 (40.08%)]	 Loss: 0.000035
Training epoch 10 [27520/67107 (41.03%)]	 Loss: 0.000008
Training epoch 10 [28160/67107 (41.98%)]	 Loss: 0.000048
Training epoch 10 [28800/67107 (42.94%)]	 Loss: 0.000093
Training epoch 10 [29440/67107 (43.89%)]	 Loss: 0.000073
Training epoch 10 [30080/67107 (44.85%)]	 Loss: 0.000738
Training epoch 10 [30720/67107 (45.80%)]	 Loss: 0.000508
Training epoch 10 [31360/67107 (46.76%)]	 Loss: 0.000051
Training epoch 10 [32000/67107 (47.71%)]	 Loss: 0.000037
Training epoch 10 [32640/67107 (48.66%)]	 Loss: 0.000044
Training epoch 10 [33280/67107 (49.62%)]	 Loss: 0.000027
Training epoch 10 [33920/67107 (50.57%)]	 Loss: 0.000012
Training epoch 10 [34560/67107 (51.53%)]	 Loss: 0.000014
Training epoch 10 [35200/67107 (52.48%)]	 Loss: 0.000014
Training epoch 10 [35840/67107 (53.44%)]	 Loss: 0.000053
Training epoch 10 [36480/67107 (54.39%)]	 Loss: 0.000334
Training epoch 10 [37120/67107 (55.34%)]	 Loss: 0.000037
Training epoch 10 [37760/67107 (56.30%)]	 Loss: 0.000086
Training epoch 10 [38400/67107 (57.25%)]	 Loss: 0.000083
Training epoch 10 [39040/67107 (58.21%)]	 Loss: 0.000043
Training epoch 10 [39680/67107 (59.16%)]	 Loss: 0.000079
Training epoch 10 [40320/67107 (60.11%)]	 Loss: 0.000039
Training epoch 10 [40960/67107 (61.07%)]	 Loss: 0.000026
Training epoch 10 [41600/67107 (62.02%)]	 Loss: 0.000131
Training epoch 10 [42240/67107 (62.98%)]	 Loss: 0.000345
Training epoch 10 [42880/67107 (63.93%)]	 Loss: 0.000107
Training epoch 10 [43520/67107 (64.89%)]	 Loss: 0.000360
Training epoch 10 [44160/67107 (65.84%)]	 Loss: 0.000187
Training epoch 10 [44800/67107 (66.79%)]	 Loss: 0.000496
Training epoch 10 [45440/67107 (67.75%)]	 Loss: 0.000038
Training epoch 10 [46080/67107 (68.70%)]	 Loss: 0.000045
Training epoch 10 [46720/67107 (69.66%)]	 Loss: 0.000024
Training epoch 10 [47360/67107 (70.61%)]	 Loss: 0.000021
Training epoch 10 [48000/67107 (71.56%)]	 Loss: 0.000146
Training epoch 10 [48640/67107 (72.52%)]	 Loss: 0.000111
Training epoch 10 [49280/67107 (73.47%)]	 Loss: 0.000311
Training epoch 10 [49920/67107 (74.43%)]	 Loss: 0.000294
Training epoch 10 [50560/67107 (75.38%)]	 Loss: 0.000203
Training epoch 10 [51200/67107 (76.34%)]	 Loss: 0.000170
Training epoch 10 [51840/67107 (77.29%)]	 Loss: 0.000540
Training epoch 10 [52480/67107 (78.24%)]	 Loss: 0.000088
Training epoch 10 [53120/67107 (79.20%)]	 Loss: 0.000050
Training epoch 10 [53760/67107 (80.15%)]	 Loss: 0.000029
Training epoch 10 [54400/67107 (81.11%)]	 Loss: 0.000040
Training epoch 10 [55040/67107 (82.06%)]	 Loss: 0.000046
Training epoch 10 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 10 [56320/67107 (83.97%)]	 Loss: 0.000008
Training epoch 10 [56960/67107 (84.92%)]	 Loss: 0.000091
Training epoch 10 [57600/67107 (85.88%)]	 Loss: 0.000054
Training epoch 10 [58240/67107 (86.83%)]	 Loss: 0.000046
Training epoch 10 [58880/67107 (87.79%)]	 Loss: 0.000193
Training epoch 10 [59520/67107 (88.74%)]	 Loss: 0.000119
Training epoch 10 [60160/67107 (89.69%)]	 Loss: 0.000059
Training epoch 10 [60800/67107 (90.65%)]	 Loss: 0.000080
Training epoch 10 [61440/67107 (91.60%)]	 Loss: 0.000036
Training epoch 10 [62080/67107 (92.56%)]	 Loss: 0.000027
Training epoch 10 [62720/67107 (93.51%)]	 Loss: 0.000013
Training epoch 10 [63360/67107 (94.47%)]	 Loss: 0.000242
Training epoch 10 [64000/67107 (95.42%)]	 Loss: 0.000379
Training epoch 10 [64640/67107 (96.37%)]	 Loss: 0.000504
Training epoch 10 [65280/67107 (97.33%)]	 Loss: 0.000298
Training epoch 10 [65920/67107 (98.28%)]	 Loss: 0.000090
Training epoch 10 [66560/67107 (99.24%)]	 Loss: 0.000111
Test set: Average Loss: 0.010324
Training epoch 11 [0/67107 (0.00%)]	 Loss: 0.000577
Training epoch 11 [640/67107 (0.95%)]	 Loss: 0.000300
Training epoch 11 [1280/67107 (1.91%)]	 Loss: 0.000077
Training epoch 11 [1920/67107 (2.86%)]	 Loss: 0.000027
Training epoch 11 [2560/67107 (3.82%)]	 Loss: 0.000015
Training epoch 11 [3200/67107 (4.77%)]	 Loss: 0.000179
Training epoch 11 [3840/67107 (5.73%)]	 Loss: 0.000234
Training epoch 11 [4480/67107 (6.68%)]	 Loss: 0.000274
Training epoch 11 [5120/67107 (7.63%)]	 Loss: 0.000239
Training epoch 11 [5760/67107 (8.59%)]	 Loss: 0.000109
Training epoch 11 [6400/67107 (9.54%)]	 Loss: 0.000305
Training epoch 11 [7040/67107 (10.50%)]	 Loss: 0.000283
Training epoch 11 [7680/67107 (11.45%)]	 Loss: 0.000010
Training epoch 11 [8320/67107 (12.40%)]	 Loss: 0.000020
Training epoch 11 [8960/67107 (13.36%)]	 Loss: 0.000013
Training epoch 11 [9600/67107 (14.31%)]	 Loss: 0.000022
Training epoch 11 [10240/67107 (15.27%)]	 Loss: 0.000114
Training epoch 11 [10880/67107 (16.22%)]	 Loss: 0.000438
Training epoch 11 [11520/67107 (17.18%)]	 Loss: 0.000018
Training epoch 11 [12160/67107 (18.13%)]	 Loss: 0.000095
Training epoch 11 [12800/67107 (19.08%)]	 Loss: 0.000453
Training epoch 11 [13440/67107 (20.04%)]	 Loss: 0.000204
Training epoch 11 [14080/67107 (20.99%)]	 Loss: 0.000051
Training epoch 11 [14720/67107 (21.95%)]	 Loss: 0.000188
Training epoch 11 [15360/67107 (22.90%)]	 Loss: 0.000067
Training epoch 11 [16000/67107 (23.85%)]	 Loss: 0.000109
Training epoch 11 [16640/67107 (24.81%)]	 Loss: 0.000038
Training epoch 11 [17280/67107 (25.76%)]	 Loss: 0.000023
Training epoch 11 [17920/67107 (26.72%)]	 Loss: 0.000332
Training epoch 11 [18560/67107 (27.67%)]	 Loss: 0.000066
Training epoch 11 [19200/67107 (28.63%)]	 Loss: 0.000202
Training epoch 11 [19840/67107 (29.58%)]	 Loss: 0.000687
Training epoch 11 [20480/67107 (30.53%)]	 Loss: 0.000227
Training epoch 11 [21120/67107 (31.49%)]	 Loss: 0.000082
Training epoch 11 [21760/67107 (32.44%)]	 Loss: 0.000073
Training epoch 11 [22400/67107 (33.40%)]	 Loss: 0.000815
Training epoch 11 [23040/67107 (34.35%)]	 Loss: 0.000318
Training epoch 11 [23680/67107 (35.31%)]	 Loss: 0.000005
Training epoch 11 [24320/67107 (36.26%)]	 Loss: 0.000102
Training epoch 11 [24960/67107 (37.21%)]	 Loss: 0.000055
Training epoch 11 [25600/67107 (38.17%)]	 Loss: 0.000039
Training epoch 11 [26240/67107 (39.12%)]	 Loss: 0.000580
Training epoch 11 [26880/67107 (40.08%)]	 Loss: 0.000161
Training epoch 11 [27520/67107 (41.03%)]	 Loss: 0.000162
Training epoch 11 [28160/67107 (41.98%)]	 Loss: 0.000817
Training epoch 11 [28800/67107 (42.94%)]	 Loss: 0.000304
Training epoch 11 [29440/67107 (43.89%)]	 Loss: 0.000186
Training epoch 11 [30080/67107 (44.85%)]	 Loss: 0.000052
Training epoch 11 [30720/67107 (45.80%)]	 Loss: 0.000378
Training epoch 11 [31360/67107 (46.76%)]	 Loss: 0.000271
Training epoch 11 [32000/67107 (47.71%)]	 Loss: 0.000092
Training epoch 11 [32640/67107 (48.66%)]	 Loss: 0.000020
Training epoch 11 [33280/67107 (49.62%)]	 Loss: 0.000016
Training epoch 11 [33920/67107 (50.57%)]	 Loss: 0.000440
Training epoch 11 [34560/67107 (51.53%)]	 Loss: 0.000033
Training epoch 11 [35200/67107 (52.48%)]	 Loss: 0.000050
Training epoch 11 [35840/67107 (53.44%)]	 Loss: 0.000049
Training epoch 11 [36480/67107 (54.39%)]	 Loss: 0.000064
Training epoch 11 [37120/67107 (55.34%)]	 Loss: 0.000030
Training epoch 11 [37760/67107 (56.30%)]	 Loss: 0.000422
Training epoch 11 [38400/67107 (57.25%)]	 Loss: 0.000026
Training epoch 11 [39040/67107 (58.21%)]	 Loss: 0.000029
Training epoch 11 [39680/67107 (59.16%)]	 Loss: 0.000051
Training epoch 11 [40320/67107 (60.11%)]	 Loss: 0.000041
Training epoch 11 [40960/67107 (61.07%)]	 Loss: 0.000069
Training epoch 11 [41600/67107 (62.02%)]	 Loss: 0.000586
Training epoch 11 [42240/67107 (62.98%)]	 Loss: 0.001013
Training epoch 11 [42880/67107 (63.93%)]	 Loss: 0.000319
Training epoch 11 [43520/67107 (64.89%)]	 Loss: 0.000114
Training epoch 11 [44160/67107 (65.84%)]	 Loss: 0.000066
Training epoch 11 [44800/67107 (66.79%)]	 Loss: 0.000156
Training epoch 11 [45440/67107 (67.75%)]	 Loss: 0.000101
Training epoch 11 [46080/67107 (68.70%)]	 Loss: 0.000034
Training epoch 11 [46720/67107 (69.66%)]	 Loss: 0.000019
Training epoch 11 [47360/67107 (70.61%)]	 Loss: 0.000022
Training epoch 11 [48000/67107 (71.56%)]	 Loss: 0.000174
Training epoch 11 [48640/67107 (72.52%)]	 Loss: 0.000092
Training epoch 11 [49280/67107 (73.47%)]	 Loss: 0.000030
Training epoch 11 [49920/67107 (74.43%)]	 Loss: 0.000600
Training epoch 11 [50560/67107 (75.38%)]	 Loss: 0.000377
Training epoch 11 [51200/67107 (76.34%)]	 Loss: 0.000086
Training epoch 11 [51840/67107 (77.29%)]	 Loss: 0.000060
Training epoch 11 [52480/67107 (78.24%)]	 Loss: 0.000147
Training epoch 11 [53120/67107 (79.20%)]	 Loss: 0.000360
Training epoch 11 [53760/67107 (80.15%)]	 Loss: 0.000050
Training epoch 11 [54400/67107 (81.11%)]	 Loss: 0.000438
Training epoch 11 [55040/67107 (82.06%)]	 Loss: 0.000367
Training epoch 11 [55680/67107 (83.02%)]	 Loss: 0.000067
Training epoch 11 [56320/67107 (83.97%)]	 Loss: 0.000257
Training epoch 11 [56960/67107 (84.92%)]	 Loss: 0.000299
Training epoch 11 [57600/67107 (85.88%)]	 Loss: 0.000161
Training epoch 11 [58240/67107 (86.83%)]	 Loss: 0.000119
Training epoch 11 [58880/67107 (87.79%)]	 Loss: 0.000039
Training epoch 11 [59520/67107 (88.74%)]	 Loss: 0.000007
Training epoch 11 [60160/67107 (89.69%)]	 Loss: 0.000020
Training epoch 11 [60800/67107 (90.65%)]	 Loss: 0.000027
Training epoch 11 [61440/67107 (91.60%)]	 Loss: 0.000056
Training epoch 11 [62080/67107 (92.56%)]	 Loss: 0.000010
Training epoch 11 [62720/67107 (93.51%)]	 Loss: 0.000068
Training epoch 11 [63360/67107 (94.47%)]	 Loss: 0.000419
Training epoch 11 [64000/67107 (95.42%)]	 Loss: 0.000378
Training epoch 11 [64640/67107 (96.37%)]	 Loss: 0.000181
Training epoch 11 [65280/67107 (97.33%)]	 Loss: 0.000161
Training epoch 11 [65920/67107 (98.28%)]	 Loss: 0.000110
Training epoch 11 [66560/67107 (99.24%)]	 Loss: 0.000450
Test set: Average Loss: 0.000736
Training epoch 12 [0/67107 (0.00%)]	 Loss: 0.000189
Training epoch 12 [640/67107 (0.95%)]	 Loss: 0.000059
Training epoch 12 [1280/67107 (1.91%)]	 Loss: 0.000710
Training epoch 12 [1920/67107 (2.86%)]	 Loss: 0.000093
Training epoch 12 [2560/67107 (3.82%)]	 Loss: 0.000178
Training epoch 12 [3200/67107 (4.77%)]	 Loss: 0.000488
Training epoch 12 [3840/67107 (5.73%)]	 Loss: 0.000065
Training epoch 12 [4480/67107 (6.68%)]	 Loss: 0.000056
Training epoch 12 [5120/67107 (7.63%)]	 Loss: 0.000083
Training epoch 12 [5760/67107 (8.59%)]	 Loss: 0.000020
Training epoch 12 [6400/67107 (9.54%)]	 Loss: 0.000025
Training epoch 12 [7040/67107 (10.50%)]	 Loss: 0.000051
Training epoch 12 [7680/67107 (11.45%)]	 Loss: 0.000031
Training epoch 12 [8320/67107 (12.40%)]	 Loss: 0.000012
Training epoch 12 [8960/67107 (13.36%)]	 Loss: 0.000125
Training epoch 12 [9600/67107 (14.31%)]	 Loss: 0.000208
Training epoch 12 [10240/67107 (15.27%)]	 Loss: 0.000333
Training epoch 12 [10880/67107 (16.22%)]	 Loss: 0.000512
Training epoch 12 [11520/67107 (17.18%)]	 Loss: 0.000195
Training epoch 12 [12160/67107 (18.13%)]	 Loss: 0.000364
Training epoch 12 [12800/67107 (19.08%)]	 Loss: 0.000131
Training epoch 12 [13440/67107 (20.04%)]	 Loss: 0.000030
Training epoch 12 [14080/67107 (20.99%)]	 Loss: 0.000152
Training epoch 12 [14720/67107 (21.95%)]	 Loss: 0.000294
Training epoch 12 [15360/67107 (22.90%)]	 Loss: 0.000064
Training epoch 12 [16000/67107 (23.85%)]	 Loss: 0.000133
Training epoch 12 [16640/67107 (24.81%)]	 Loss: 0.000226
Training epoch 12 [17280/67107 (25.76%)]	 Loss: 0.000228
Training epoch 12 [17920/67107 (26.72%)]	 Loss: 0.000077
Training epoch 12 [18560/67107 (27.67%)]	 Loss: 0.000047
Training epoch 12 [19200/67107 (28.63%)]	 Loss: 0.000007
Training epoch 12 [19840/67107 (29.58%)]	 Loss: 0.000013
Training epoch 12 [20480/67107 (30.53%)]	 Loss: 0.000020
Training epoch 12 [21120/67107 (31.49%)]	 Loss: 0.000101
Training epoch 12 [21760/67107 (32.44%)]	 Loss: 0.000276
Training epoch 12 [22400/67107 (33.40%)]	 Loss: 0.000208
Training epoch 12 [23040/67107 (34.35%)]	 Loss: 0.000193
Training epoch 12 [23680/67107 (35.31%)]	 Loss: 0.000087
Training epoch 12 [24320/67107 (36.26%)]	 Loss: 0.000152
Training epoch 12 [24960/67107 (37.21%)]	 Loss: 0.000031
Training epoch 12 [25600/67107 (38.17%)]	 Loss: 0.000015
Training epoch 12 [26240/67107 (39.12%)]	 Loss: 0.000024
Training epoch 12 [26880/67107 (40.08%)]	 Loss: 0.000019
Training epoch 12 [27520/67107 (41.03%)]	 Loss: 0.000006
Training epoch 12 [28160/67107 (41.98%)]	 Loss: 0.000177
Training epoch 12 [28800/67107 (42.94%)]	 Loss: 0.000051
Training epoch 12 [29440/67107 (43.89%)]	 Loss: 0.000136
Training epoch 12 [30080/67107 (44.85%)]	 Loss: 0.000735
Training epoch 12 [30720/67107 (45.80%)]	 Loss: 0.000186
Training epoch 12 [31360/67107 (46.76%)]	 Loss: 0.000074
Training epoch 12 [32000/67107 (47.71%)]	 Loss: 0.000105
Training epoch 12 [32640/67107 (48.66%)]	 Loss: 0.000696
Training epoch 12 [33280/67107 (49.62%)]	 Loss: 0.000382
Training epoch 12 [33920/67107 (50.57%)]	 Loss: 0.000709
Training epoch 12 [34560/67107 (51.53%)]	 Loss: 0.000033
Training epoch 12 [35200/67107 (52.48%)]	 Loss: 0.000100
Training epoch 12 [35840/67107 (53.44%)]	 Loss: 0.000076
Training epoch 12 [36480/67107 (54.39%)]	 Loss: 0.000039
Training epoch 12 [37120/67107 (55.34%)]	 Loss: 0.000045
Training epoch 12 [37760/67107 (56.30%)]	 Loss: 0.000108
Training epoch 12 [38400/67107 (57.25%)]	 Loss: 0.000303
Training epoch 12 [39040/67107 (58.21%)]	 Loss: 0.000365
Training epoch 12 [39680/67107 (59.16%)]	 Loss: 0.000132
Training epoch 12 [40320/67107 (60.11%)]	 Loss: 0.000076
Training epoch 12 [40960/67107 (61.07%)]	 Loss: 0.000099
Training epoch 12 [41600/67107 (62.02%)]	 Loss: 0.000156
Training epoch 12 [42240/67107 (62.98%)]	 Loss: 0.000487
Training epoch 12 [42880/67107 (63.93%)]	 Loss: 0.000168
Training epoch 12 [43520/67107 (64.89%)]	 Loss: 0.000520
Training epoch 12 [44160/67107 (65.84%)]	 Loss: 0.000162
Training epoch 12 [44800/67107 (66.79%)]	 Loss: 0.000057
Training epoch 12 [45440/67107 (67.75%)]	 Loss: 0.000012
Training epoch 12 [46080/67107 (68.70%)]	 Loss: 0.000011
Training epoch 12 [46720/67107 (69.66%)]	 Loss: 0.000042
Training epoch 12 [47360/67107 (70.61%)]	 Loss: 0.000047
Training epoch 12 [48000/67107 (71.56%)]	 Loss: 0.000015
Training epoch 12 [48640/67107 (72.52%)]	 Loss: 0.000188
Training epoch 12 [49280/67107 (73.47%)]	 Loss: 0.000046
Training epoch 12 [49920/67107 (74.43%)]	 Loss: 0.000060
Training epoch 12 [50560/67107 (75.38%)]	 Loss: 0.000107
Training epoch 12 [51200/67107 (76.34%)]	 Loss: 0.000044
Training epoch 12 [51840/67107 (77.29%)]	 Loss: 0.000107
Training epoch 12 [52480/67107 (78.24%)]	 Loss: 0.000116
Training epoch 12 [53120/67107 (79.20%)]	 Loss: 0.000157
Training epoch 12 [53760/67107 (80.15%)]	 Loss: 0.000876
Training epoch 12 [54400/67107 (81.11%)]	 Loss: 0.000489
Training epoch 12 [55040/67107 (82.06%)]	 Loss: 0.000040
Training epoch 12 [55680/67107 (83.02%)]	 Loss: 0.000039
Training epoch 12 [56320/67107 (83.97%)]	 Loss: 0.000029
Training epoch 12 [56960/67107 (84.92%)]	 Loss: 0.000025
Training epoch 12 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 12 [58240/67107 (86.83%)]	 Loss: 0.000038
Training epoch 12 [58880/67107 (87.79%)]	 Loss: 0.000007
Training epoch 12 [59520/67107 (88.74%)]	 Loss: 0.000010
Training epoch 12 [60160/67107 (89.69%)]	 Loss: 0.000011
Training epoch 12 [60800/67107 (90.65%)]	 Loss: 0.000298
Training epoch 12 [61440/67107 (91.60%)]	 Loss: 0.000296
Training epoch 12 [62080/67107 (92.56%)]	 Loss: 0.000080
Training epoch 12 [62720/67107 (93.51%)]	 Loss: 0.000066
Training epoch 12 [63360/67107 (94.47%)]	 Loss: 0.000091
Training epoch 12 [64000/67107 (95.42%)]	 Loss: 0.000021
Training epoch 12 [64640/67107 (96.37%)]	 Loss: 0.000182
Training epoch 12 [65280/67107 (97.33%)]	 Loss: 0.000161
Training epoch 12 [65920/67107 (98.28%)]	 Loss: 0.000110
Training epoch 12 [66560/67107 (99.24%)]	 Loss: 0.000068
Test set: Average Loss: 0.012640
Training epoch 13 [0/67107 (0.00%)]	 Loss: 0.000032
Training epoch 13 [640/67107 (0.95%)]	 Loss: 0.000023
Training epoch 13 [1280/67107 (1.91%)]	 Loss: 0.000020
Training epoch 13 [1920/67107 (2.86%)]	 Loss: 0.000324
Training epoch 13 [2560/67107 (3.82%)]	 Loss: 0.000051
Training epoch 13 [3200/67107 (4.77%)]	 Loss: 0.000244
Training epoch 13 [3840/67107 (5.73%)]	 Loss: 0.000022
Training epoch 13 [4480/67107 (6.68%)]	 Loss: 0.000071
Training epoch 13 [5120/67107 (7.63%)]	 Loss: 0.000015
Training epoch 13 [5760/67107 (8.59%)]	 Loss: 0.000182
Training epoch 13 [6400/67107 (9.54%)]	 Loss: 0.000061
Training epoch 13 [7040/67107 (10.50%)]	 Loss: 0.000074
Training epoch 13 [7680/67107 (11.45%)]	 Loss: 0.000053
Training epoch 13 [8320/67107 (12.40%)]	 Loss: 0.000043
Training epoch 13 [8960/67107 (13.36%)]	 Loss: 0.000008
Training epoch 13 [9600/67107 (14.31%)]	 Loss: 0.000313
Training epoch 13 [10240/67107 (15.27%)]	 Loss: 0.000179
Training epoch 13 [10880/67107 (16.22%)]	 Loss: 0.000027
Training epoch 13 [11520/67107 (17.18%)]	 Loss: 0.000063
Training epoch 13 [12160/67107 (18.13%)]	 Loss: 0.000088
Training epoch 13 [12800/67107 (19.08%)]	 Loss: 0.000057
Training epoch 13 [13440/67107 (20.04%)]	 Loss: 0.000067
Training epoch 13 [14080/67107 (20.99%)]	 Loss: 0.000228
Training epoch 13 [14720/67107 (21.95%)]	 Loss: 0.000106
Training epoch 13 [15360/67107 (22.90%)]	 Loss: 0.000022
Training epoch 13 [16000/67107 (23.85%)]	 Loss: 0.000024
Training epoch 13 [16640/67107 (24.81%)]	 Loss: 0.000574
Training epoch 13 [17280/67107 (25.76%)]	 Loss: 0.000092
Training epoch 13 [17920/67107 (26.72%)]	 Loss: 0.000271
Training epoch 13 [18560/67107 (27.67%)]	 Loss: 0.000455
Training epoch 13 [19200/67107 (28.63%)]	 Loss: 0.000058
Training epoch 13 [19840/67107 (29.58%)]	 Loss: 0.000212
Training epoch 13 [20480/67107 (30.53%)]	 Loss: 0.000072
Training epoch 13 [21120/67107 (31.49%)]	 Loss: 0.000292
Training epoch 13 [21760/67107 (32.44%)]	 Loss: 0.000092
Training epoch 13 [22400/67107 (33.40%)]	 Loss: 0.000070
Training epoch 13 [23040/67107 (34.35%)]	 Loss: 0.000022
Training epoch 13 [23680/67107 (35.31%)]	 Loss: 0.000032
Training epoch 13 [24320/67107 (36.26%)]	 Loss: 0.000020
Training epoch 13 [24960/67107 (37.21%)]	 Loss: 0.000380
Training epoch 13 [25600/67107 (38.17%)]	 Loss: 0.000082
Training epoch 13 [26240/67107 (39.12%)]	 Loss: 0.000111
Training epoch 13 [26880/67107 (40.08%)]	 Loss: 0.000045
Training epoch 13 [27520/67107 (41.03%)]	 Loss: 0.000195
Training epoch 13 [28160/67107 (41.98%)]	 Loss: 0.000040
Training epoch 13 [28800/67107 (42.94%)]	 Loss: 0.000014
Training epoch 13 [29440/67107 (43.89%)]	 Loss: 0.000559
Training epoch 13 [30080/67107 (44.85%)]	 Loss: 0.000108
Training epoch 13 [30720/67107 (45.80%)]	 Loss: 0.000213
Training epoch 13 [31360/67107 (46.76%)]	 Loss: 0.000088
Training epoch 13 [32000/67107 (47.71%)]	 Loss: 0.000872
Training epoch 13 [32640/67107 (48.66%)]	 Loss: 0.000228
Training epoch 13 [33280/67107 (49.62%)]	 Loss: 0.000380
Training epoch 13 [33920/67107 (50.57%)]	 Loss: 0.000029
Training epoch 13 [34560/67107 (51.53%)]	 Loss: 0.000020
Training epoch 13 [35200/67107 (52.48%)]	 Loss: 0.000012
Training epoch 13 [35840/67107 (53.44%)]	 Loss: 0.000006
Training epoch 13 [36480/67107 (54.39%)]	 Loss: 0.000008
Training epoch 13 [37120/67107 (55.34%)]	 Loss: 0.000006
Training epoch 13 [37760/67107 (56.30%)]	 Loss: 0.000328
Training epoch 13 [38400/67107 (57.25%)]	 Loss: 0.000062
Training epoch 13 [39040/67107 (58.21%)]	 Loss: 0.000071
Training epoch 13 [39680/67107 (59.16%)]	 Loss: 0.000013
Training epoch 13 [40320/67107 (60.11%)]	 Loss: 0.000095
Training epoch 13 [40960/67107 (61.07%)]	 Loss: 0.000033
Training epoch 13 [41600/67107 (62.02%)]	 Loss: 0.000040
Training epoch 13 [42240/67107 (62.98%)]	 Loss: 0.000321
Training epoch 13 [42880/67107 (63.93%)]	 Loss: 0.000032
Training epoch 13 [43520/67107 (64.89%)]	 Loss: 0.000053
Training epoch 13 [44160/67107 (65.84%)]	 Loss: 0.000055
Training epoch 13 [44800/67107 (66.79%)]	 Loss: 0.000016
Training epoch 13 [45440/67107 (67.75%)]	 Loss: 0.000381
Training epoch 13 [46080/67107 (68.70%)]	 Loss: 0.000116
Training epoch 13 [46720/67107 (69.66%)]	 Loss: 0.000299
Training epoch 13 [47360/67107 (70.61%)]	 Loss: 0.000075
Training epoch 13 [48000/67107 (71.56%)]	 Loss: 0.000057
Training epoch 13 [48640/67107 (72.52%)]	 Loss: 0.000029
Training epoch 13 [49280/67107 (73.47%)]	 Loss: 0.000025
Training epoch 13 [49920/67107 (74.43%)]	 Loss: 0.000018
Training epoch 13 [50560/67107 (75.38%)]	 Loss: 0.000015
Training epoch 13 [51200/67107 (76.34%)]	 Loss: 0.000806
Training epoch 13 [51840/67107 (77.29%)]	 Loss: 0.000209
Training epoch 13 [52480/67107 (78.24%)]	 Loss: 0.000133
Training epoch 13 [53120/67107 (79.20%)]	 Loss: 0.000146
Training epoch 13 [53760/67107 (80.15%)]	 Loss: 0.000246
Training epoch 13 [54400/67107 (81.11%)]	 Loss: 0.000033
Training epoch 13 [55040/67107 (82.06%)]	 Loss: 0.000015
Training epoch 13 [55680/67107 (83.02%)]	 Loss: 0.000116
Training epoch 13 [56320/67107 (83.97%)]	 Loss: 0.000049
Training epoch 13 [56960/67107 (84.92%)]	 Loss: 0.000044
Training epoch 13 [57600/67107 (85.88%)]	 Loss: 0.000091
Training epoch 13 [58240/67107 (86.83%)]	 Loss: 0.000201
Training epoch 13 [58880/67107 (87.79%)]	 Loss: 0.000300
Training epoch 13 [59520/67107 (88.74%)]	 Loss: 0.000111
Training epoch 13 [60160/67107 (89.69%)]	 Loss: 0.000306
Training epoch 13 [60800/67107 (90.65%)]	 Loss: 0.000067
Training epoch 13 [61440/67107 (91.60%)]	 Loss: 0.000096
Training epoch 13 [62080/67107 (92.56%)]	 Loss: 0.000072
Training epoch 13 [62720/67107 (93.51%)]	 Loss: 0.000009
Training epoch 13 [63360/67107 (94.47%)]	 Loss: 0.000015
Training epoch 13 [64000/67107 (95.42%)]	 Loss: 0.000023
Training epoch 13 [64640/67107 (96.37%)]	 Loss: 0.000090
Training epoch 13 [65280/67107 (97.33%)]	 Loss: 0.000129
Training epoch 13 [65920/67107 (98.28%)]	 Loss: 0.000802
Training epoch 13 [66560/67107 (99.24%)]	 Loss: 0.000188
Test set: Average Loss: 0.002094
Training epoch 14 [0/67107 (0.00%)]	 Loss: 0.000145
Training epoch 14 [640/67107 (0.95%)]	 Loss: 0.000088
Training epoch 14 [1280/67107 (1.91%)]	 Loss: 0.000386
Training epoch 14 [1920/67107 (2.86%)]	 Loss: 0.000057
Training epoch 14 [2560/67107 (3.82%)]	 Loss: 0.000157
Training epoch 14 [3200/67107 (4.77%)]	 Loss: 0.000261
Training epoch 14 [3840/67107 (5.73%)]	 Loss: 0.000026
Training epoch 14 [4480/67107 (6.68%)]	 Loss: 0.000016
Training epoch 14 [5120/67107 (7.63%)]	 Loss: 0.000028
Training epoch 14 [5760/67107 (8.59%)]	 Loss: 0.000048
Training epoch 14 [6400/67107 (9.54%)]	 Loss: 0.000118
Training epoch 14 [7040/67107 (10.50%)]	 Loss: 0.000071
Training epoch 14 [7680/67107 (11.45%)]	 Loss: 0.000083
Training epoch 14 [8320/67107 (12.40%)]	 Loss: 0.000285
Training epoch 14 [8960/67107 (13.36%)]	 Loss: 0.000152
Training epoch 14 [9600/67107 (14.31%)]	 Loss: 0.001357
Training epoch 14 [10240/67107 (15.27%)]	 Loss: 0.000060
Training epoch 14 [10880/67107 (16.22%)]	 Loss: 0.000058
Training epoch 14 [11520/67107 (17.18%)]	 Loss: 0.000121
Training epoch 14 [12160/67107 (18.13%)]	 Loss: 0.000091
Training epoch 14 [12800/67107 (19.08%)]	 Loss: 0.000154
Training epoch 14 [13440/67107 (20.04%)]	 Loss: 0.000199
Training epoch 14 [14080/67107 (20.99%)]	 Loss: 0.000105
Training epoch 14 [14720/67107 (21.95%)]	 Loss: 0.000156
Training epoch 14 [15360/67107 (22.90%)]	 Loss: 0.000014
Training epoch 14 [16000/67107 (23.85%)]	 Loss: 0.000009
Training epoch 14 [16640/67107 (24.81%)]	 Loss: 0.000021
Training epoch 14 [17280/67107 (25.76%)]	 Loss: 0.000195
Training epoch 14 [17920/67107 (26.72%)]	 Loss: 0.000066
Training epoch 14 [18560/67107 (27.67%)]	 Loss: 0.000044
Training epoch 14 [19200/67107 (28.63%)]	 Loss: 0.000530
Training epoch 14 [19840/67107 (29.58%)]	 Loss: 0.000098
Training epoch 14 [20480/67107 (30.53%)]	 Loss: 0.000023
Training epoch 14 [21120/67107 (31.49%)]	 Loss: 0.000123
Training epoch 14 [21760/67107 (32.44%)]	 Loss: 0.000025
Training epoch 14 [22400/67107 (33.40%)]	 Loss: 0.000045
Training epoch 14 [23040/67107 (34.35%)]	 Loss: 0.000038
Training epoch 14 [23680/67107 (35.31%)]	 Loss: 0.000011
Training epoch 14 [24320/67107 (36.26%)]	 Loss: 0.000055
Training epoch 14 [24960/67107 (37.21%)]	 Loss: 0.000011
Training epoch 14 [25600/67107 (38.17%)]	 Loss: 0.000483
Training epoch 14 [26240/67107 (39.12%)]	 Loss: 0.000102
Training epoch 14 [26880/67107 (40.08%)]	 Loss: 0.000047
Training epoch 14 [27520/67107 (41.03%)]	 Loss: 0.000063
Training epoch 14 [28160/67107 (41.98%)]	 Loss: 0.000100
Training epoch 14 [28800/67107 (42.94%)]	 Loss: 0.000059
Training epoch 14 [29440/67107 (43.89%)]	 Loss: 0.000073
Training epoch 14 [30080/67107 (44.85%)]	 Loss: 0.000077
Training epoch 14 [30720/67107 (45.80%)]	 Loss: 0.000039
Training epoch 14 [31360/67107 (46.76%)]	 Loss: 0.000035
Training epoch 14 [32000/67107 (47.71%)]	 Loss: 0.000032
Training epoch 14 [32640/67107 (48.66%)]	 Loss: 0.000079
Training epoch 14 [33280/67107 (49.62%)]	 Loss: 0.000123
Training epoch 14 [33920/67107 (50.57%)]	 Loss: 0.000050
Training epoch 14 [34560/67107 (51.53%)]	 Loss: 0.000127
Training epoch 14 [35200/67107 (52.48%)]	 Loss: 0.000022
Training epoch 14 [35840/67107 (53.44%)]	 Loss: 0.000012
Training epoch 14 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 14 [37120/67107 (55.34%)]	 Loss: 0.000008
Training epoch 14 [37760/67107 (56.30%)]	 Loss: 0.000044
Training epoch 14 [38400/67107 (57.25%)]	 Loss: 0.000037
Training epoch 14 [39040/67107 (58.21%)]	 Loss: 0.000049
Training epoch 14 [39680/67107 (59.16%)]	 Loss: 0.000045
Training epoch 14 [40320/67107 (60.11%)]	 Loss: 0.000196
Training epoch 14 [40960/67107 (61.07%)]	 Loss: 0.000086
Training epoch 14 [41600/67107 (62.02%)]	 Loss: 0.000069
Training epoch 14 [42240/67107 (62.98%)]	 Loss: 0.000115
Training epoch 14 [42880/67107 (63.93%)]	 Loss: 0.000007
Training epoch 14 [43520/67107 (64.89%)]	 Loss: 0.000030
Training epoch 14 [44160/67107 (65.84%)]	 Loss: 0.000070
Training epoch 14 [44800/67107 (66.79%)]	 Loss: 0.000007
Training epoch 14 [45440/67107 (67.75%)]	 Loss: 0.000034
Training epoch 14 [46080/67107 (68.70%)]	 Loss: 0.000017
Training epoch 14 [46720/67107 (69.66%)]	 Loss: 0.000111
Training epoch 14 [47360/67107 (70.61%)]	 Loss: 0.000032
Training epoch 14 [48000/67107 (71.56%)]	 Loss: 0.000131
Training epoch 14 [48640/67107 (72.52%)]	 Loss: 0.000097
Training epoch 14 [49280/67107 (73.47%)]	 Loss: 0.000110
Training epoch 14 [49920/67107 (74.43%)]	 Loss: 0.000084
Training epoch 14 [50560/67107 (75.38%)]	 Loss: 0.000135
Training epoch 14 [51200/67107 (76.34%)]	 Loss: 0.000018
Training epoch 14 [51840/67107 (77.29%)]	 Loss: 0.000098
Training epoch 14 [52480/67107 (78.24%)]	 Loss: 0.000076
Training epoch 14 [53120/67107 (79.20%)]	 Loss: 0.000185
Training epoch 14 [53760/67107 (80.15%)]	 Loss: 0.000115
Training epoch 14 [54400/67107 (81.11%)]	 Loss: 0.000096
Training epoch 14 [55040/67107 (82.06%)]	 Loss: 0.000048
Training epoch 14 [55680/67107 (83.02%)]	 Loss: 0.000010
Training epoch 14 [56320/67107 (83.97%)]	 Loss: 0.000017
Training epoch 14 [56960/67107 (84.92%)]	 Loss: 0.000012
Training epoch 14 [57600/67107 (85.88%)]	 Loss: 0.000029
Training epoch 14 [58240/67107 (86.83%)]	 Loss: 0.000008
Training epoch 14 [58880/67107 (87.79%)]	 Loss: 0.000117
Training epoch 14 [59520/67107 (88.74%)]	 Loss: 0.000223
Training epoch 14 [60160/67107 (89.69%)]	 Loss: 0.000014
Training epoch 14 [60800/67107 (90.65%)]	 Loss: 0.000022
Training epoch 14 [61440/67107 (91.60%)]	 Loss: 0.000057
Training epoch 14 [62080/67107 (92.56%)]	 Loss: 0.000029
Training epoch 14 [62720/67107 (93.51%)]	 Loss: 0.000105
Training epoch 14 [63360/67107 (94.47%)]	 Loss: 0.000032
Training epoch 14 [64000/67107 (95.42%)]	 Loss: 0.000059
Training epoch 14 [64640/67107 (96.37%)]	 Loss: 0.000037
Training epoch 14 [65280/67107 (97.33%)]	 Loss: 0.000016
Training epoch 14 [65920/67107 (98.28%)]	 Loss: 0.000017
Training epoch 14 [66560/67107 (99.24%)]	 Loss: 0.000027
Test set: Average Loss: 0.001255
Training epoch 15 [0/67107 (0.00%)]	 Loss: 0.000039
Training epoch 15 [640/67107 (0.95%)]	 Loss: 0.000030
Training epoch 15 [1280/67107 (1.91%)]	 Loss: 0.000020
Training epoch 15 [1920/67107 (2.86%)]	 Loss: 0.000033
Training epoch 15 [2560/67107 (3.82%)]	 Loss: 0.000135
Training epoch 15 [3200/67107 (4.77%)]	 Loss: 0.000076
Training epoch 15 [3840/67107 (5.73%)]	 Loss: 0.000032
Training epoch 15 [4480/67107 (6.68%)]	 Loss: 0.000022
Training epoch 15 [5120/67107 (7.63%)]	 Loss: 0.000029
Training epoch 15 [5760/67107 (8.59%)]	 Loss: 0.000103
Training epoch 15 [6400/67107 (9.54%)]	 Loss: 0.000014
Training epoch 15 [7040/67107 (10.50%)]	 Loss: 0.000074
Training epoch 15 [7680/67107 (11.45%)]	 Loss: 0.000027
Training epoch 15 [8320/67107 (12.40%)]	 Loss: 0.000061
Training epoch 15 [8960/67107 (13.36%)]	 Loss: 0.000033
Training epoch 15 [9600/67107 (14.31%)]	 Loss: 0.000132
Training epoch 15 [10240/67107 (15.27%)]	 Loss: 0.000029
Training epoch 15 [10880/67107 (16.22%)]	 Loss: 0.000011
Training epoch 15 [11520/67107 (17.18%)]	 Loss: 0.000024
Training epoch 15 [12160/67107 (18.13%)]	 Loss: 0.000011
Training epoch 15 [12800/67107 (19.08%)]	 Loss: 0.000059
Training epoch 15 [13440/67107 (20.04%)]	 Loss: 0.000071
Training epoch 15 [14080/67107 (20.99%)]	 Loss: 0.000355
Training epoch 15 [14720/67107 (21.95%)]	 Loss: 0.000159
Training epoch 15 [15360/67107 (22.90%)]	 Loss: 0.000067
Training epoch 15 [16000/67107 (23.85%)]	 Loss: 0.000053
Training epoch 15 [16640/67107 (24.81%)]	 Loss: 0.000018
Training epoch 15 [17280/67107 (25.76%)]	 Loss: 0.000013
Training epoch 15 [17920/67107 (26.72%)]	 Loss: 0.000052
Training epoch 15 [18560/67107 (27.67%)]	 Loss: 0.000037
Training epoch 15 [19200/67107 (28.63%)]	 Loss: 0.000015
Training epoch 15 [19840/67107 (29.58%)]	 Loss: 0.000020
Training epoch 15 [20480/67107 (30.53%)]	 Loss: 0.000123
Training epoch 15 [21120/67107 (31.49%)]	 Loss: 0.000091
Training epoch 15 [21760/67107 (32.44%)]	 Loss: 0.000030
Training epoch 15 [22400/67107 (33.40%)]	 Loss: 0.000007
Training epoch 15 [23040/67107 (34.35%)]	 Loss: 0.000014
Training epoch 15 [23680/67107 (35.31%)]	 Loss: 0.000011
Training epoch 15 [24320/67107 (36.26%)]	 Loss: 0.000024
Training epoch 15 [24960/67107 (37.21%)]	 Loss: 0.000299
Training epoch 15 [25600/67107 (38.17%)]	 Loss: 0.000098
Training epoch 15 [26240/67107 (39.12%)]	 Loss: 0.000060
Training epoch 15 [26880/67107 (40.08%)]	 Loss: 0.000165
Training epoch 15 [27520/67107 (41.03%)]	 Loss: 0.000170
Training epoch 15 [28160/67107 (41.98%)]	 Loss: 0.000272
Training epoch 15 [28800/67107 (42.94%)]	 Loss: 0.000073
Training epoch 15 [29440/67107 (43.89%)]	 Loss: 0.000012
Training epoch 15 [30080/67107 (44.85%)]	 Loss: 0.000012
Training epoch 15 [30720/67107 (45.80%)]	 Loss: 0.000015
Training epoch 15 [31360/67107 (46.76%)]	 Loss: 0.000012
Training epoch 15 [32000/67107 (47.71%)]	 Loss: 0.000012
Training epoch 15 [32640/67107 (48.66%)]	 Loss: 0.000025
Training epoch 15 [33280/67107 (49.62%)]	 Loss: 0.000006
Training epoch 15 [33920/67107 (50.57%)]	 Loss: 0.000016
Training epoch 15 [34560/67107 (51.53%)]	 Loss: 0.000145
Training epoch 15 [35200/67107 (52.48%)]	 Loss: 0.000113
Training epoch 15 [35840/67107 (53.44%)]	 Loss: 0.000027
Training epoch 15 [36480/67107 (54.39%)]	 Loss: 0.000019
Training epoch 15 [37120/67107 (55.34%)]	 Loss: 0.000101
Training epoch 15 [37760/67107 (56.30%)]	 Loss: 0.000031
Training epoch 15 [38400/67107 (57.25%)]	 Loss: 0.000041
Training epoch 15 [39040/67107 (58.21%)]	 Loss: 0.000198
Training epoch 15 [39680/67107 (59.16%)]	 Loss: 0.000044
Training epoch 15 [40320/67107 (60.11%)]	 Loss: 0.000084
Training epoch 15 [40960/67107 (61.07%)]	 Loss: 0.000080
Training epoch 15 [41600/67107 (62.02%)]	 Loss: 0.000149
Training epoch 15 [42240/67107 (62.98%)]	 Loss: 0.000154
Training epoch 15 [42880/67107 (63.93%)]	 Loss: 0.000064
Training epoch 15 [43520/67107 (64.89%)]	 Loss: 0.000013
Training epoch 15 [44160/67107 (65.84%)]	 Loss: 0.000014
Training epoch 15 [44800/67107 (66.79%)]	 Loss: 0.000006
Training epoch 15 [45440/67107 (67.75%)]	 Loss: 0.000026
Training epoch 15 [46080/67107 (68.70%)]	 Loss: 0.000039
Training epoch 15 [46720/67107 (69.66%)]	 Loss: 0.000005
Training epoch 15 [47360/67107 (70.61%)]	 Loss: 0.000027
Training epoch 15 [48000/67107 (71.56%)]	 Loss: 0.000196
Training epoch 15 [48640/67107 (72.52%)]	 Loss: 0.000088
Training epoch 15 [49280/67107 (73.47%)]	 Loss: 0.000053
Training epoch 15 [49920/67107 (74.43%)]	 Loss: 0.000035
Training epoch 15 [50560/67107 (75.38%)]	 Loss: 0.000023
Training epoch 15 [51200/67107 (76.34%)]	 Loss: 0.000186
Training epoch 15 [51840/67107 (77.29%)]	 Loss: 0.000029
Training epoch 15 [52480/67107 (78.24%)]	 Loss: 0.000124
Training epoch 15 [53120/67107 (79.20%)]	 Loss: 0.000012
Training epoch 15 [53760/67107 (80.15%)]	 Loss: 0.000137
Training epoch 15 [54400/67107 (81.11%)]	 Loss: 0.000076
Training epoch 15 [55040/67107 (82.06%)]	 Loss: 0.000757
Training epoch 15 [55680/67107 (83.02%)]	 Loss: 0.000234
Training epoch 15 [56320/67107 (83.97%)]	 Loss: 0.000163
Training epoch 15 [56960/67107 (84.92%)]	 Loss: 0.000018
Training epoch 15 [57600/67107 (85.88%)]	 Loss: 0.000066
Training epoch 15 [58240/67107 (86.83%)]	 Loss: 0.000023
Training epoch 15 [58880/67107 (87.79%)]	 Loss: 0.000010
Training epoch 15 [59520/67107 (88.74%)]	 Loss: 0.000020
Training epoch 15 [60160/67107 (89.69%)]	 Loss: 0.000011
Training epoch 15 [60800/67107 (90.65%)]	 Loss: 0.000013
Training epoch 15 [61440/67107 (91.60%)]	 Loss: 0.000018
Training epoch 15 [62080/67107 (92.56%)]	 Loss: 0.000181
Training epoch 15 [62720/67107 (93.51%)]	 Loss: 0.000020
Training epoch 15 [63360/67107 (94.47%)]	 Loss: 0.000211
Training epoch 15 [64000/67107 (95.42%)]	 Loss: 0.000063
Training epoch 15 [64640/67107 (96.37%)]	 Loss: 0.000039
Training epoch 15 [65280/67107 (97.33%)]	 Loss: 0.000034
Training epoch 15 [65920/67107 (98.28%)]	 Loss: 0.000046
Training epoch 15 [66560/67107 (99.24%)]	 Loss: 0.000035
Test set: Average Loss: 0.000313
Training epoch 16 [0/67107 (0.00%)]	 Loss: 0.000036
Training epoch 16 [640/67107 (0.95%)]	 Loss: 0.000086
Training epoch 16 [1280/67107 (1.91%)]	 Loss: 0.000096
Training epoch 16 [1920/67107 (2.86%)]	 Loss: 0.000055
Training epoch 16 [2560/67107 (3.82%)]	 Loss: 0.000015
Training epoch 16 [3200/67107 (4.77%)]	 Loss: 0.000072
Training epoch 16 [3840/67107 (5.73%)]	 Loss: 0.000041
Training epoch 16 [4480/67107 (6.68%)]	 Loss: 0.000009
Training epoch 16 [5120/67107 (7.63%)]	 Loss: 0.000014
Training epoch 16 [5760/67107 (8.59%)]	 Loss: 0.000009
Training epoch 16 [6400/67107 (9.54%)]	 Loss: 0.000024
Training epoch 16 [7040/67107 (10.50%)]	 Loss: 0.000015
Training epoch 16 [7680/67107 (11.45%)]	 Loss: 0.000028
Training epoch 16 [8320/67107 (12.40%)]	 Loss: 0.000025
Training epoch 16 [8960/67107 (13.36%)]	 Loss: 0.000065
Training epoch 16 [9600/67107 (14.31%)]	 Loss: 0.000072
Training epoch 16 [10240/67107 (15.27%)]	 Loss: 0.000025
Training epoch 16 [10880/67107 (16.22%)]	 Loss: 0.000036
Training epoch 16 [11520/67107 (17.18%)]	 Loss: 0.000052
Training epoch 16 [12160/67107 (18.13%)]	 Loss: 0.000038
Training epoch 16 [12800/67107 (19.08%)]	 Loss: 0.000009
Training epoch 16 [13440/67107 (20.04%)]	 Loss: 0.000019
Training epoch 16 [14080/67107 (20.99%)]	 Loss: 0.000017
Training epoch 16 [14720/67107 (21.95%)]	 Loss: 0.000007
Training epoch 16 [15360/67107 (22.90%)]	 Loss: 0.000009
Training epoch 16 [16000/67107 (23.85%)]	 Loss: 0.000008
Training epoch 16 [16640/67107 (24.81%)]	 Loss: 0.000063
Training epoch 16 [17280/67107 (25.76%)]	 Loss: 0.000094
Training epoch 16 [17920/67107 (26.72%)]	 Loss: 0.000016
Training epoch 16 [18560/67107 (27.67%)]	 Loss: 0.000080
Training epoch 16 [19200/67107 (28.63%)]	 Loss: 0.000068
Training epoch 16 [19840/67107 (29.58%)]	 Loss: 0.000065
Training epoch 16 [20480/67107 (30.53%)]	 Loss: 0.000065
Training epoch 16 [21120/67107 (31.49%)]	 Loss: 0.000030
Training epoch 16 [21760/67107 (32.44%)]	 Loss: 0.000011
Training epoch 16 [22400/67107 (33.40%)]	 Loss: 0.000014
Training epoch 16 [23040/67107 (34.35%)]	 Loss: 0.000026
Training epoch 16 [23680/67107 (35.31%)]	 Loss: 0.000074
Training epoch 16 [24320/67107 (36.26%)]	 Loss: 0.000069
Training epoch 16 [24960/67107 (37.21%)]	 Loss: 0.000038
Training epoch 16 [25600/67107 (38.17%)]	 Loss: 0.000032
Training epoch 16 [26240/67107 (39.12%)]	 Loss: 0.000034
Training epoch 16 [26880/67107 (40.08%)]	 Loss: 0.000030
Training epoch 16 [27520/67107 (41.03%)]	 Loss: 0.000151
Training epoch 16 [28160/67107 (41.98%)]	 Loss: 0.000038
Training epoch 16 [28800/67107 (42.94%)]	 Loss: 0.000035
Training epoch 16 [29440/67107 (43.89%)]	 Loss: 0.000013
Training epoch 16 [30080/67107 (44.85%)]	 Loss: 0.000011
Training epoch 16 [30720/67107 (45.80%)]	 Loss: 0.000048
Training epoch 16 [31360/67107 (46.76%)]	 Loss: 0.000086
Training epoch 16 [32000/67107 (47.71%)]	 Loss: 0.000015
Training epoch 16 [32640/67107 (48.66%)]	 Loss: 0.000010
Training epoch 16 [33280/67107 (49.62%)]	 Loss: 0.000042
Training epoch 16 [33920/67107 (50.57%)]	 Loss: 0.000014
Training epoch 16 [34560/67107 (51.53%)]	 Loss: 0.000154
Training epoch 16 [35200/67107 (52.48%)]	 Loss: 0.000081
Training epoch 16 [35840/67107 (53.44%)]	 Loss: 0.000013
Training epoch 16 [36480/67107 (54.39%)]	 Loss: 0.000018
Training epoch 16 [37120/67107 (55.34%)]	 Loss: 0.000128
Training epoch 16 [37760/67107 (56.30%)]	 Loss: 0.000029
Training epoch 16 [38400/67107 (57.25%)]	 Loss: 0.000027
Training epoch 16 [39040/67107 (58.21%)]	 Loss: 0.000078
Training epoch 16 [39680/67107 (59.16%)]	 Loss: 0.000024
Training epoch 16 [40320/67107 (60.11%)]	 Loss: 0.000141
Training epoch 16 [40960/67107 (61.07%)]	 Loss: 0.000161
Training epoch 16 [41600/67107 (62.02%)]	 Loss: 0.000015
Training epoch 16 [42240/67107 (62.98%)]	 Loss: 0.000023
Training epoch 16 [42880/67107 (63.93%)]	 Loss: 0.000064
Training epoch 16 [43520/67107 (64.89%)]	 Loss: 0.000071
Training epoch 16 [44160/67107 (65.84%)]	 Loss: 0.000122
Training epoch 16 [44800/67107 (66.79%)]	 Loss: 0.000222
Training epoch 16 [45440/67107 (67.75%)]	 Loss: 0.000076
Training epoch 16 [46080/67107 (68.70%)]	 Loss: 0.000053
Training epoch 16 [46720/67107 (69.66%)]	 Loss: 0.000023
Training epoch 16 [47360/67107 (70.61%)]	 Loss: 0.000042
Training epoch 16 [48000/67107 (71.56%)]	 Loss: 0.000087
Training epoch 16 [48640/67107 (72.52%)]	 Loss: 0.000014
Training epoch 16 [49280/67107 (73.47%)]	 Loss: 0.000005
Training epoch 16 [49920/67107 (74.43%)]	 Loss: 0.000048
Training epoch 16 [50560/67107 (75.38%)]	 Loss: 0.000092
Training epoch 16 [51200/67107 (76.34%)]	 Loss: 0.000127
Training epoch 16 [51840/67107 (77.29%)]	 Loss: 0.000023
Training epoch 16 [52480/67107 (78.24%)]	 Loss: 0.000032
Training epoch 16 [53120/67107 (79.20%)]	 Loss: 0.000137
Training epoch 16 [53760/67107 (80.15%)]	 Loss: 0.000066
Training epoch 16 [54400/67107 (81.11%)]	 Loss: 0.000188
Training epoch 16 [55040/67107 (82.06%)]	 Loss: 0.000040
Training epoch 16 [55680/67107 (83.02%)]	 Loss: 0.000027
Training epoch 16 [56320/67107 (83.97%)]	 Loss: 0.000023
Training epoch 16 [56960/67107 (84.92%)]	 Loss: 0.000030
Training epoch 16 [57600/67107 (85.88%)]	 Loss: 0.000035
Training epoch 16 [58240/67107 (86.83%)]	 Loss: 0.000107
Training epoch 16 [58880/67107 (87.79%)]	 Loss: 0.000333
Training epoch 16 [59520/67107 (88.74%)]	 Loss: 0.000255
Training epoch 16 [60160/67107 (89.69%)]	 Loss: 0.000044
Training epoch 16 [60800/67107 (90.65%)]	 Loss: 0.000048
Training epoch 16 [61440/67107 (91.60%)]	 Loss: 0.000017
Training epoch 16 [62080/67107 (92.56%)]	 Loss: 0.000017
Training epoch 16 [62720/67107 (93.51%)]	 Loss: 0.000058
Training epoch 16 [63360/67107 (94.47%)]	 Loss: 0.000023
Training epoch 16 [64000/67107 (95.42%)]	 Loss: 0.000008
Training epoch 16 [64640/67107 (96.37%)]	 Loss: 0.000042
Training epoch 16 [65280/67107 (97.33%)]	 Loss: 0.000033
Training epoch 16 [65920/67107 (98.28%)]	 Loss: 0.000033
Training epoch 16 [66560/67107 (99.24%)]	 Loss: 0.000027
Test set: Average Loss: 0.001100
Training epoch 17 [0/67107 (0.00%)]	 Loss: 0.000038
Training epoch 17 [640/67107 (0.95%)]	 Loss: 0.000148
Training epoch 17 [1280/67107 (1.91%)]	 Loss: 0.000033
Training epoch 17 [1920/67107 (2.86%)]	 Loss: 0.000018
Training epoch 17 [2560/67107 (3.82%)]	 Loss: 0.000121
Training epoch 17 [3200/67107 (4.77%)]	 Loss: 0.000011
Training epoch 17 [3840/67107 (5.73%)]	 Loss: 0.000011
Training epoch 17 [4480/67107 (6.68%)]	 Loss: 0.000031
Training epoch 17 [5120/67107 (7.63%)]	 Loss: 0.000062
Training epoch 17 [5760/67107 (8.59%)]	 Loss: 0.000043
Training epoch 17 [6400/67107 (9.54%)]	 Loss: 0.000088
Training epoch 17 [7040/67107 (10.50%)]	 Loss: 0.000036
Training epoch 17 [7680/67107 (11.45%)]	 Loss: 0.000128
Training epoch 17 [8320/67107 (12.40%)]	 Loss: 0.000055
Training epoch 17 [8960/67107 (13.36%)]	 Loss: 0.000069
Training epoch 17 [9600/67107 (14.31%)]	 Loss: 0.000034
Training epoch 17 [10240/67107 (15.27%)]	 Loss: 0.000020
Training epoch 17 [10880/67107 (16.22%)]	 Loss: 0.000143
Training epoch 17 [11520/67107 (17.18%)]	 Loss: 0.000014
Training epoch 17 [12160/67107 (18.13%)]	 Loss: 0.000153
Training epoch 17 [12800/67107 (19.08%)]	 Loss: 0.000045
Training epoch 17 [13440/67107 (20.04%)]	 Loss: 0.000043
Training epoch 17 [14080/67107 (20.99%)]	 Loss: 0.000060
Training epoch 17 [14720/67107 (21.95%)]	 Loss: 0.000057
Training epoch 17 [15360/67107 (22.90%)]	 Loss: 0.000017
Training epoch 17 [16000/67107 (23.85%)]	 Loss: 0.000017
Training epoch 17 [16640/67107 (24.81%)]	 Loss: 0.000023
Training epoch 17 [17280/67107 (25.76%)]	 Loss: 0.000062
Training epoch 17 [17920/67107 (26.72%)]	 Loss: 0.000016
Training epoch 17 [18560/67107 (27.67%)]	 Loss: 0.000091
Training epoch 17 [19200/67107 (28.63%)]	 Loss: 0.000009
Training epoch 17 [19840/67107 (29.58%)]	 Loss: 0.000045
Training epoch 17 [20480/67107 (30.53%)]	 Loss: 0.000107
Training epoch 17 [21120/67107 (31.49%)]	 Loss: 0.000012
Training epoch 17 [21760/67107 (32.44%)]	 Loss: 0.000021
Training epoch 17 [22400/67107 (33.40%)]	 Loss: 0.000011
Training epoch 17 [23040/67107 (34.35%)]	 Loss: 0.000011
Training epoch 17 [23680/67107 (35.31%)]	 Loss: 0.000010
Training epoch 17 [24320/67107 (36.26%)]	 Loss: 0.000054
Training epoch 17 [24960/67107 (37.21%)]	 Loss: 0.000029
Training epoch 17 [25600/67107 (38.17%)]	 Loss: 0.000023
Training epoch 17 [26240/67107 (39.12%)]	 Loss: 0.000370
Training epoch 17 [26880/67107 (40.08%)]	 Loss: 0.000097
Training epoch 17 [27520/67107 (41.03%)]	 Loss: 0.000009
Training epoch 17 [28160/67107 (41.98%)]	 Loss: 0.000019
Training epoch 17 [28800/67107 (42.94%)]	 Loss: 0.000036
Training epoch 17 [29440/67107 (43.89%)]	 Loss: 0.000022
Training epoch 17 [30080/67107 (44.85%)]	 Loss: 0.000016
Training epoch 17 [30720/67107 (45.80%)]	 Loss: 0.000474
Training epoch 17 [31360/67107 (46.76%)]	 Loss: 0.000217
Training epoch 17 [32000/67107 (47.71%)]	 Loss: 0.000016
Training epoch 17 [32640/67107 (48.66%)]	 Loss: 0.000146
Training epoch 17 [33280/67107 (49.62%)]	 Loss: 0.000094
Training epoch 17 [33920/67107 (50.57%)]	 Loss: 0.000068
Training epoch 17 [34560/67107 (51.53%)]	 Loss: 0.000043
Training epoch 17 [35200/67107 (52.48%)]	 Loss: 0.000077
Training epoch 17 [35840/67107 (53.44%)]	 Loss: 0.000037
Training epoch 17 [36480/67107 (54.39%)]	 Loss: 0.000006
Training epoch 17 [37120/67107 (55.34%)]	 Loss: 0.000006
Training epoch 17 [37760/67107 (56.30%)]	 Loss: 0.000074
Training epoch 17 [38400/67107 (57.25%)]	 Loss: 0.000051
Training epoch 17 [39040/67107 (58.21%)]	 Loss: 0.000086
Training epoch 17 [39680/67107 (59.16%)]	 Loss: 0.000088
Training epoch 17 [40320/67107 (60.11%)]	 Loss: 0.000047
Training epoch 17 [40960/67107 (61.07%)]	 Loss: 0.000044
Training epoch 17 [41600/67107 (62.02%)]	 Loss: 0.000139
Training epoch 17 [42240/67107 (62.98%)]	 Loss: 0.000006
Training epoch 17 [42880/67107 (63.93%)]	 Loss: 0.000008
Training epoch 17 [43520/67107 (64.89%)]	 Loss: 0.000033
Training epoch 17 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 17 [44800/67107 (66.79%)]	 Loss: 0.000036
Training epoch 17 [45440/67107 (67.75%)]	 Loss: 0.000145
Training epoch 17 [46080/67107 (68.70%)]	 Loss: 0.000002
Training epoch 17 [46720/67107 (69.66%)]	 Loss: 0.000048
Training epoch 17 [47360/67107 (70.61%)]	 Loss: 0.000022
Training epoch 17 [48000/67107 (71.56%)]	 Loss: 0.000086
Training epoch 17 [48640/67107 (72.52%)]	 Loss: 0.000027
Training epoch 17 [49280/67107 (73.47%)]	 Loss: 0.000007
Training epoch 17 [49920/67107 (74.43%)]	 Loss: 0.000017
Training epoch 17 [50560/67107 (75.38%)]	 Loss: 0.000021
Training epoch 17 [51200/67107 (76.34%)]	 Loss: 0.000002
Training epoch 17 [51840/67107 (77.29%)]	 Loss: 0.000008
Training epoch 17 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 17 [53120/67107 (79.20%)]	 Loss: 0.000027
Training epoch 17 [53760/67107 (80.15%)]	 Loss: 0.000023
Training epoch 17 [54400/67107 (81.11%)]	 Loss: 0.000004
Training epoch 17 [55040/67107 (82.06%)]	 Loss: 0.000161
Training epoch 17 [55680/67107 (83.02%)]	 Loss: 0.000095
Training epoch 17 [56320/67107 (83.97%)]	 Loss: 0.000029
Training epoch 17 [56960/67107 (84.92%)]	 Loss: 0.000020
Training epoch 17 [57600/67107 (85.88%)]	 Loss: 0.000050
Training epoch 17 [58240/67107 (86.83%)]	 Loss: 0.000031
Training epoch 17 [58880/67107 (87.79%)]	 Loss: 0.000037
Training epoch 17 [59520/67107 (88.74%)]	 Loss: 0.000112
Training epoch 17 [60160/67107 (89.69%)]	 Loss: 0.000119
Training epoch 17 [60800/67107 (90.65%)]	 Loss: 0.000080
Training epoch 17 [61440/67107 (91.60%)]	 Loss: 0.000027
Training epoch 17 [62080/67107 (92.56%)]	 Loss: 0.000073
Training epoch 17 [62720/67107 (93.51%)]	 Loss: 0.000117
Training epoch 17 [63360/67107 (94.47%)]	 Loss: 0.000044
Training epoch 17 [64000/67107 (95.42%)]	 Loss: 0.000014
Training epoch 17 [64640/67107 (96.37%)]	 Loss: 0.000024
Training epoch 17 [65280/67107 (97.33%)]	 Loss: 0.000017
Training epoch 17 [65920/67107 (98.28%)]	 Loss: 0.000013
Training epoch 17 [66560/67107 (99.24%)]	 Loss: 0.000003
Test set: Average Loss: 0.000979
Training epoch 18 [0/67107 (0.00%)]	 Loss: 0.000025
Training epoch 18 [640/67107 (0.95%)]	 Loss: 0.000007
Training epoch 18 [1280/67107 (1.91%)]	 Loss: 0.000185
Training epoch 18 [1920/67107 (2.86%)]	 Loss: 0.000040
Training epoch 18 [2560/67107 (3.82%)]	 Loss: 0.000011
Training epoch 18 [3200/67107 (4.77%)]	 Loss: 0.000085
Training epoch 18 [3840/67107 (5.73%)]	 Loss: 0.000158
Training epoch 18 [4480/67107 (6.68%)]	 Loss: 0.000069
Training epoch 18 [5120/67107 (7.63%)]	 Loss: 0.000024
Training epoch 18 [5760/67107 (8.59%)]	 Loss: 0.000175
Training epoch 18 [6400/67107 (9.54%)]	 Loss: 0.000025
Training epoch 18 [7040/67107 (10.50%)]	 Loss: 0.000011
Training epoch 18 [7680/67107 (11.45%)]	 Loss: 0.000017
Training epoch 18 [8320/67107 (12.40%)]	 Loss: 0.000007
Training epoch 18 [8960/67107 (13.36%)]	 Loss: 0.000011
Training epoch 18 [9600/67107 (14.31%)]	 Loss: 0.000046
Training epoch 18 [10240/67107 (15.27%)]	 Loss: 0.000155
Training epoch 18 [10880/67107 (16.22%)]	 Loss: 0.000048
Training epoch 18 [11520/67107 (17.18%)]	 Loss: 0.000063
Training epoch 18 [12160/67107 (18.13%)]	 Loss: 0.000029
Training epoch 18 [12800/67107 (19.08%)]	 Loss: 0.000021
Training epoch 18 [13440/67107 (20.04%)]	 Loss: 0.000081
Training epoch 18 [14080/67107 (20.99%)]	 Loss: 0.000034
Training epoch 18 [14720/67107 (21.95%)]	 Loss: 0.000047
Training epoch 18 [15360/67107 (22.90%)]	 Loss: 0.000031
Training epoch 18 [16000/67107 (23.85%)]	 Loss: 0.000045
Training epoch 18 [16640/67107 (24.81%)]	 Loss: 0.000016
Training epoch 18 [17280/67107 (25.76%)]	 Loss: 0.000039
Training epoch 18 [17920/67107 (26.72%)]	 Loss: 0.000108
Training epoch 18 [18560/67107 (27.67%)]	 Loss: 0.000174
Training epoch 18 [19200/67107 (28.63%)]	 Loss: 0.000008
Training epoch 18 [19840/67107 (29.58%)]	 Loss: 0.000040
Training epoch 18 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 18 [21120/67107 (31.49%)]	 Loss: 0.000010
Training epoch 18 [21760/67107 (32.44%)]	 Loss: 0.000008
Training epoch 18 [22400/67107 (33.40%)]	 Loss: 0.000019
Training epoch 18 [23040/67107 (34.35%)]	 Loss: 0.000009
Training epoch 18 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 18 [24320/67107 (36.26%)]	 Loss: 0.000028
Training epoch 18 [24960/67107 (37.21%)]	 Loss: 0.000134
Training epoch 18 [25600/67107 (38.17%)]	 Loss: 0.000070
Training epoch 18 [26240/67107 (39.12%)]	 Loss: 0.000036
Training epoch 18 [26880/67107 (40.08%)]	 Loss: 0.000101
Training epoch 18 [27520/67107 (41.03%)]	 Loss: 0.000018
Training epoch 18 [28160/67107 (41.98%)]	 Loss: 0.000046
Training epoch 18 [28800/67107 (42.94%)]	 Loss: 0.000168
Training epoch 18 [29440/67107 (43.89%)]	 Loss: 0.000037
Training epoch 18 [30080/67107 (44.85%)]	 Loss: 0.000164
Training epoch 18 [30720/67107 (45.80%)]	 Loss: 0.000025
Training epoch 18 [31360/67107 (46.76%)]	 Loss: 0.000038
Training epoch 18 [32000/67107 (47.71%)]	 Loss: 0.000086
Training epoch 18 [32640/67107 (48.66%)]	 Loss: 0.000006
Training epoch 18 [33280/67107 (49.62%)]	 Loss: 0.000008
Training epoch 18 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 18 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 18 [35200/67107 (52.48%)]	 Loss: 0.000025
Training epoch 18 [35840/67107 (53.44%)]	 Loss: 0.000107
Training epoch 18 [36480/67107 (54.39%)]	 Loss: 0.000039
Training epoch 18 [37120/67107 (55.34%)]	 Loss: 0.000031
Training epoch 18 [37760/67107 (56.30%)]	 Loss: 0.000046
Training epoch 18 [38400/67107 (57.25%)]	 Loss: 0.000081
Training epoch 18 [39040/67107 (58.21%)]	 Loss: 0.000136
Training epoch 18 [39680/67107 (59.16%)]	 Loss: 0.000090
Training epoch 18 [40320/67107 (60.11%)]	 Loss: 0.000013
Training epoch 18 [40960/67107 (61.07%)]	 Loss: 0.000006
Training epoch 18 [41600/67107 (62.02%)]	 Loss: 0.000047
Training epoch 18 [42240/67107 (62.98%)]	 Loss: 0.000032
Training epoch 18 [42880/67107 (63.93%)]	 Loss: 0.000081
Training epoch 18 [43520/67107 (64.89%)]	 Loss: 0.000023
Training epoch 18 [44160/67107 (65.84%)]	 Loss: 0.000035
Training epoch 18 [44800/67107 (66.79%)]	 Loss: 0.000007
Training epoch 18 [45440/67107 (67.75%)]	 Loss: 0.000033
Training epoch 18 [46080/67107 (68.70%)]	 Loss: 0.000042
Training epoch 18 [46720/67107 (69.66%)]	 Loss: 0.000013
Training epoch 18 [47360/67107 (70.61%)]	 Loss: 0.000010
Training epoch 18 [48000/67107 (71.56%)]	 Loss: 0.000014
Training epoch 18 [48640/67107 (72.52%)]	 Loss: 0.000006
Training epoch 18 [49280/67107 (73.47%)]	 Loss: 0.000030
Training epoch 18 [49920/67107 (74.43%)]	 Loss: 0.000133
Training epoch 18 [50560/67107 (75.38%)]	 Loss: 0.000074
Training epoch 18 [51200/67107 (76.34%)]	 Loss: 0.000053
Training epoch 18 [51840/67107 (77.29%)]	 Loss: 0.000035
Training epoch 18 [52480/67107 (78.24%)]	 Loss: 0.000072
Training epoch 18 [53120/67107 (79.20%)]	 Loss: 0.000172
Training epoch 18 [53760/67107 (80.15%)]	 Loss: 0.000116
Training epoch 18 [54400/67107 (81.11%)]	 Loss: 0.000012
Training epoch 18 [55040/67107 (82.06%)]	 Loss: 0.000025
Training epoch 18 [55680/67107 (83.02%)]	 Loss: 0.000016
Training epoch 18 [56320/67107 (83.97%)]	 Loss: 0.000015
Training epoch 18 [56960/67107 (84.92%)]	 Loss: 0.000017
Training epoch 18 [57600/67107 (85.88%)]	 Loss: 0.000027
Training epoch 18 [58240/67107 (86.83%)]	 Loss: 0.000002
Training epoch 18 [58880/67107 (87.79%)]	 Loss: 0.000007
Training epoch 18 [59520/67107 (88.74%)]	 Loss: 0.000057
Training epoch 18 [60160/67107 (89.69%)]	 Loss: 0.000099
Training epoch 18 [60800/67107 (90.65%)]	 Loss: 0.000026
Training epoch 18 [61440/67107 (91.60%)]	 Loss: 0.000056
Training epoch 18 [62080/67107 (92.56%)]	 Loss: 0.000013
Training epoch 18 [62720/67107 (93.51%)]	 Loss: 0.000080
Training epoch 18 [63360/67107 (94.47%)]	 Loss: 0.000022
Training epoch 18 [64000/67107 (95.42%)]	 Loss: 0.000084
Training epoch 18 [64640/67107 (96.37%)]	 Loss: 0.000022
Training epoch 18 [65280/67107 (97.33%)]	 Loss: 0.000052
Training epoch 18 [65920/67107 (98.28%)]	 Loss: 0.000003
Training epoch 18 [66560/67107 (99.24%)]	 Loss: 0.000027
Test set: Average Loss: 0.037387
Training epoch 19 [0/67107 (0.00%)]	 Loss: 0.000064
Training epoch 19 [640/67107 (0.95%)]	 Loss: 0.000075
Training epoch 19 [1280/67107 (1.91%)]	 Loss: 0.000022
Training epoch 19 [1920/67107 (2.86%)]	 Loss: 0.000038
Training epoch 19 [2560/67107 (3.82%)]	 Loss: 0.000008
Training epoch 19 [3200/67107 (4.77%)]	 Loss: 0.000007
Training epoch 19 [3840/67107 (5.73%)]	 Loss: 0.000018
Training epoch 19 [4480/67107 (6.68%)]	 Loss: 0.000050
Training epoch 19 [5120/67107 (7.63%)]	 Loss: 0.000006
Training epoch 19 [5760/67107 (8.59%)]	 Loss: 0.000163
Training epoch 19 [6400/67107 (9.54%)]	 Loss: 0.000101
Training epoch 19 [7040/67107 (10.50%)]	 Loss: 0.000066
Training epoch 19 [7680/67107 (11.45%)]	 Loss: 0.000102
Training epoch 19 [8320/67107 (12.40%)]	 Loss: 0.000160
Training epoch 19 [8960/67107 (13.36%)]	 Loss: 0.000043
Training epoch 19 [9600/67107 (14.31%)]	 Loss: 0.000119
Training epoch 19 [10240/67107 (15.27%)]	 Loss: 0.000070
Training epoch 19 [10880/67107 (16.22%)]	 Loss: 0.000020
Training epoch 19 [11520/67107 (17.18%)]	 Loss: 0.000022
Training epoch 19 [12160/67107 (18.13%)]	 Loss: 0.000020
Training epoch 19 [12800/67107 (19.08%)]	 Loss: 0.000037
Training epoch 19 [13440/67107 (20.04%)]	 Loss: 0.000123
Training epoch 19 [14080/67107 (20.99%)]	 Loss: 0.000061
Training epoch 19 [14720/67107 (21.95%)]	 Loss: 0.000361
Training epoch 19 [15360/67107 (22.90%)]	 Loss: 0.000281
Training epoch 19 [16000/67107 (23.85%)]	 Loss: 0.000153
Training epoch 19 [16640/67107 (24.81%)]	 Loss: 0.000022
Training epoch 19 [17280/67107 (25.76%)]	 Loss: 0.000028
Training epoch 19 [17920/67107 (26.72%)]	 Loss: 0.000013
Training epoch 19 [18560/67107 (27.67%)]	 Loss: 0.000014
Training epoch 19 [19200/67107 (28.63%)]	 Loss: 0.000022
Training epoch 19 [19840/67107 (29.58%)]	 Loss: 0.000033
Training epoch 19 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 19 [21120/67107 (31.49%)]	 Loss: 0.000068
Training epoch 19 [21760/67107 (32.44%)]	 Loss: 0.000044
Training epoch 19 [22400/67107 (33.40%)]	 Loss: 0.000346
Training epoch 19 [23040/67107 (34.35%)]	 Loss: 0.000020
Training epoch 19 [23680/67107 (35.31%)]	 Loss: 0.000026
Training epoch 19 [24320/67107 (36.26%)]	 Loss: 0.000014
Training epoch 19 [24960/67107 (37.21%)]	 Loss: 0.000012
Training epoch 19 [25600/67107 (38.17%)]	 Loss: 0.000063
Training epoch 19 [26240/67107 (39.12%)]	 Loss: 0.000145
Training epoch 19 [26880/67107 (40.08%)]	 Loss: 0.000067
Training epoch 19 [27520/67107 (41.03%)]	 Loss: 0.000055
Training epoch 19 [28160/67107 (41.98%)]	 Loss: 0.000117
Training epoch 19 [28800/67107 (42.94%)]	 Loss: 0.000028
Training epoch 19 [29440/67107 (43.89%)]	 Loss: 0.000036
Training epoch 19 [30080/67107 (44.85%)]	 Loss: 0.000019
Training epoch 19 [30720/67107 (45.80%)]	 Loss: 0.000020
Training epoch 19 [31360/67107 (46.76%)]	 Loss: 0.000006
Training epoch 19 [32000/67107 (47.71%)]	 Loss: 0.000012
Training epoch 19 [32640/67107 (48.66%)]	 Loss: 0.000236
Training epoch 19 [33280/67107 (49.62%)]	 Loss: 0.000147
Training epoch 19 [33920/67107 (50.57%)]	 Loss: 0.000036
Training epoch 19 [34560/67107 (51.53%)]	 Loss: 0.000164
Training epoch 19 [35200/67107 (52.48%)]	 Loss: 0.000083
Training epoch 19 [35840/67107 (53.44%)]	 Loss: 0.000011
Training epoch 19 [36480/67107 (54.39%)]	 Loss: 0.000011
Training epoch 19 [37120/67107 (55.34%)]	 Loss: 0.000145
Training epoch 19 [37760/67107 (56.30%)]	 Loss: 0.000023
Training epoch 19 [38400/67107 (57.25%)]	 Loss: 0.000017
Training epoch 19 [39040/67107 (58.21%)]	 Loss: 0.000026
Training epoch 19 [39680/67107 (59.16%)]	 Loss: 0.000003
Training epoch 19 [40320/67107 (60.11%)]	 Loss: 0.000184
Training epoch 19 [40960/67107 (61.07%)]	 Loss: 0.000215
Training epoch 19 [41600/67107 (62.02%)]	 Loss: 0.000031
Training epoch 19 [42240/67107 (62.98%)]	 Loss: 0.000105
Training epoch 19 [42880/67107 (63.93%)]	 Loss: 0.000136
Training epoch 19 [43520/67107 (64.89%)]	 Loss: 0.000074
Training epoch 19 [44160/67107 (65.84%)]	 Loss: 0.000027
Training epoch 19 [44800/67107 (66.79%)]	 Loss: 0.000126
Training epoch 19 [45440/67107 (67.75%)]	 Loss: 0.000026
Training epoch 19 [46080/67107 (68.70%)]	 Loss: 0.000016
Training epoch 19 [46720/67107 (69.66%)]	 Loss: 0.000016
Training epoch 19 [47360/67107 (70.61%)]	 Loss: 0.000007
Training epoch 19 [48000/67107 (71.56%)]	 Loss: 0.000010
Training epoch 19 [48640/67107 (72.52%)]	 Loss: 0.000062
Training epoch 19 [49280/67107 (73.47%)]	 Loss: 0.000081
Training epoch 19 [49920/67107 (74.43%)]	 Loss: 0.000049
Training epoch 19 [50560/67107 (75.38%)]	 Loss: 0.000042
Training epoch 19 [51200/67107 (76.34%)]	 Loss: 0.000037
Training epoch 19 [51840/67107 (77.29%)]	 Loss: 0.000013
Training epoch 19 [52480/67107 (78.24%)]	 Loss: 0.000062
Training epoch 19 [53120/67107 (79.20%)]	 Loss: 0.000065
Training epoch 19 [53760/67107 (80.15%)]	 Loss: 0.000046
Training epoch 19 [54400/67107 (81.11%)]	 Loss: 0.000026
Training epoch 19 [55040/67107 (82.06%)]	 Loss: 0.000041
Training epoch 19 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 19 [56320/67107 (83.97%)]	 Loss: 0.000051
Training epoch 19 [56960/67107 (84.92%)]	 Loss: 0.000140
Training epoch 19 [57600/67107 (85.88%)]	 Loss: 0.000046
Training epoch 19 [58240/67107 (86.83%)]	 Loss: 0.000007
Training epoch 19 [58880/67107 (87.79%)]	 Loss: 0.000036
Training epoch 19 [59520/67107 (88.74%)]	 Loss: 0.000007
Training epoch 19 [60160/67107 (89.69%)]	 Loss: 0.000018
Training epoch 19 [60800/67107 (90.65%)]	 Loss: 0.000020
Training epoch 19 [61440/67107 (91.60%)]	 Loss: 0.000037
Training epoch 19 [62080/67107 (92.56%)]	 Loss: 0.000008
Training epoch 19 [62720/67107 (93.51%)]	 Loss: 0.000039
Training epoch 19 [63360/67107 (94.47%)]	 Loss: 0.000042
Training epoch 19 [64000/67107 (95.42%)]	 Loss: 0.000114
Training epoch 19 [64640/67107 (96.37%)]	 Loss: 0.000071
Training epoch 19 [65280/67107 (97.33%)]	 Loss: 0.000040
Training epoch 19 [65920/67107 (98.28%)]	 Loss: 0.000057
Training epoch 19 [66560/67107 (99.24%)]	 Loss: 0.000023
Test set: Average Loss: 0.000273
Training epoch 20 [0/67107 (0.00%)]	 Loss: 0.000052
Training epoch 20 [640/67107 (0.95%)]	 Loss: 0.000032
Training epoch 20 [1280/67107 (1.91%)]	 Loss: 0.000007
Training epoch 20 [1920/67107 (2.86%)]	 Loss: 0.000074
Training epoch 20 [2560/67107 (3.82%)]	 Loss: 0.000011
Training epoch 20 [3200/67107 (4.77%)]	 Loss: 0.000219
Training epoch 20 [3840/67107 (5.73%)]	 Loss: 0.000055
Training epoch 20 [4480/67107 (6.68%)]	 Loss: 0.000013
Training epoch 20 [5120/67107 (7.63%)]	 Loss: 0.000040
Training epoch 20 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 20 [6400/67107 (9.54%)]	 Loss: 0.000052
Training epoch 20 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 20 [7680/67107 (11.45%)]	 Loss: 0.000011
Training epoch 20 [8320/67107 (12.40%)]	 Loss: 0.000054
Training epoch 20 [8960/67107 (13.36%)]	 Loss: 0.000045
Training epoch 20 [9600/67107 (14.31%)]	 Loss: 0.000026
Training epoch 20 [10240/67107 (15.27%)]	 Loss: 0.000043
Training epoch 20 [10880/67107 (16.22%)]	 Loss: 0.000031
Training epoch 20 [11520/67107 (17.18%)]	 Loss: 0.000012
Training epoch 20 [12160/67107 (18.13%)]	 Loss: 0.000012
Training epoch 20 [12800/67107 (19.08%)]	 Loss: 0.000010
Training epoch 20 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 20 [14080/67107 (20.99%)]	 Loss: 0.000020
Training epoch 20 [14720/67107 (21.95%)]	 Loss: 0.000003
Training epoch 20 [15360/67107 (22.90%)]	 Loss: 0.000198
Training epoch 20 [16000/67107 (23.85%)]	 Loss: 0.000035
Training epoch 20 [16640/67107 (24.81%)]	 Loss: 0.000038
Training epoch 20 [17280/67107 (25.76%)]	 Loss: 0.000117
Training epoch 20 [17920/67107 (26.72%)]	 Loss: 0.000020
Training epoch 20 [18560/67107 (27.67%)]	 Loss: 0.000024
Training epoch 20 [19200/67107 (28.63%)]	 Loss: 0.000009
Training epoch 20 [19840/67107 (29.58%)]	 Loss: 0.000133
Training epoch 20 [20480/67107 (30.53%)]	 Loss: 0.000007
Training epoch 20 [21120/67107 (31.49%)]	 Loss: 0.000012
Training epoch 20 [21760/67107 (32.44%)]	 Loss: 0.000008
Training epoch 20 [22400/67107 (33.40%)]	 Loss: 0.000018
Training epoch 20 [23040/67107 (34.35%)]	 Loss: 0.000026
Training epoch 20 [23680/67107 (35.31%)]	 Loss: 0.000055
Training epoch 20 [24320/67107 (36.26%)]	 Loss: 0.000029
Training epoch 20 [24960/67107 (37.21%)]	 Loss: 0.000036
Training epoch 20 [25600/67107 (38.17%)]	 Loss: 0.000054
Training epoch 20 [26240/67107 (39.12%)]	 Loss: 0.000069
Training epoch 20 [26880/67107 (40.08%)]	 Loss: 0.000021
Training epoch 20 [27520/67107 (41.03%)]	 Loss: 0.000051
Training epoch 20 [28160/67107 (41.98%)]	 Loss: 0.000007
Training epoch 20 [28800/67107 (42.94%)]	 Loss: 0.000006
Training epoch 20 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 20 [30080/67107 (44.85%)]	 Loss: 0.000005
Training epoch 20 [30720/67107 (45.80%)]	 Loss: 0.000072
Training epoch 20 [31360/67107 (46.76%)]	 Loss: 0.000028
Training epoch 20 [32000/67107 (47.71%)]	 Loss: 0.000025
Training epoch 20 [32640/67107 (48.66%)]	 Loss: 0.000087
Training epoch 20 [33280/67107 (49.62%)]	 Loss: 0.000040
Training epoch 20 [33920/67107 (50.57%)]	 Loss: 0.000208
Training epoch 20 [34560/67107 (51.53%)]	 Loss: 0.000091
Training epoch 20 [35200/67107 (52.48%)]	 Loss: 0.000029
Training epoch 20 [35840/67107 (53.44%)]	 Loss: 0.000022
Training epoch 20 [36480/67107 (54.39%)]	 Loss: 0.000017
Training epoch 20 [37120/67107 (55.34%)]	 Loss: 0.000022
Training epoch 20 [37760/67107 (56.30%)]	 Loss: 0.000016
Training epoch 20 [38400/67107 (57.25%)]	 Loss: 0.000139
Training epoch 20 [39040/67107 (58.21%)]	 Loss: 0.000025
Training epoch 20 [39680/67107 (59.16%)]	 Loss: 0.000271
Training epoch 20 [40320/67107 (60.11%)]	 Loss: 0.000003
Training epoch 20 [40960/67107 (61.07%)]	 Loss: 0.000022
Training epoch 20 [41600/67107 (62.02%)]	 Loss: 0.000012
Training epoch 20 [42240/67107 (62.98%)]	 Loss: 0.000007
Training epoch 20 [42880/67107 (63.93%)]	 Loss: 0.000010
Training epoch 20 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 20 [44160/67107 (65.84%)]	 Loss: 0.000033
Training epoch 20 [44800/67107 (66.79%)]	 Loss: 0.000005
Training epoch 20 [45440/67107 (67.75%)]	 Loss: 0.000016
Training epoch 20 [46080/67107 (68.70%)]	 Loss: 0.000009
Training epoch 20 [46720/67107 (69.66%)]	 Loss: 0.000022
Training epoch 20 [47360/67107 (70.61%)]	 Loss: 0.000010
Training epoch 20 [48000/67107 (71.56%)]	 Loss: 0.000010
Training epoch 20 [48640/67107 (72.52%)]	 Loss: 0.000060
Training epoch 20 [49280/67107 (73.47%)]	 Loss: 0.000013
Training epoch 20 [49920/67107 (74.43%)]	 Loss: 0.000026
Training epoch 20 [50560/67107 (75.38%)]	 Loss: 0.000029
Training epoch 20 [51200/67107 (76.34%)]	 Loss: 0.000019
Training epoch 20 [51840/67107 (77.29%)]	 Loss: 0.000097
Training epoch 20 [52480/67107 (78.24%)]	 Loss: 0.000017
Training epoch 20 [53120/67107 (79.20%)]	 Loss: 0.000089
Training epoch 20 [53760/67107 (80.15%)]	 Loss: 0.000030
Training epoch 20 [54400/67107 (81.11%)]	 Loss: 0.000026
Training epoch 20 [55040/67107 (82.06%)]	 Loss: 0.000026
Training epoch 20 [55680/67107 (83.02%)]	 Loss: 0.000022
Training epoch 20 [56320/67107 (83.97%)]	 Loss: 0.000012
Training epoch 20 [56960/67107 (84.92%)]	 Loss: 0.000008
Training epoch 20 [57600/67107 (85.88%)]	 Loss: 0.000009
Training epoch 20 [58240/67107 (86.83%)]	 Loss: 0.000004
Training epoch 20 [58880/67107 (87.79%)]	 Loss: 0.000007
Training epoch 20 [59520/67107 (88.74%)]	 Loss: 0.000049
Training epoch 20 [60160/67107 (89.69%)]	 Loss: 0.000007
Training epoch 20 [60800/67107 (90.65%)]	 Loss: 0.000111
Training epoch 20 [61440/67107 (91.60%)]	 Loss: 0.000143
Training epoch 20 [62080/67107 (92.56%)]	 Loss: 0.000008
Training epoch 20 [62720/67107 (93.51%)]	 Loss: 0.000007
Training epoch 20 [63360/67107 (94.47%)]	 Loss: 0.000036
Training epoch 20 [64000/67107 (95.42%)]	 Loss: 0.000017
Training epoch 20 [64640/67107 (96.37%)]	 Loss: 0.000051
Training epoch 20 [65280/67107 (97.33%)]	 Loss: 0.000122
Training epoch 20 [65920/67107 (98.28%)]	 Loss: 0.000059
Training epoch 20 [66560/67107 (99.24%)]	 Loss: 0.000038
Test set: Average Loss: 0.000378
Training epoch 21 [0/67107 (0.00%)]	 Loss: 0.000057
Training epoch 21 [640/67107 (0.95%)]	 Loss: 0.000066
Training epoch 21 [1280/67107 (1.91%)]	 Loss: 0.000116
Training epoch 21 [1920/67107 (2.86%)]	 Loss: 0.000017
Training epoch 21 [2560/67107 (3.82%)]	 Loss: 0.000008
Training epoch 21 [3200/67107 (4.77%)]	 Loss: 0.000009
Training epoch 21 [3840/67107 (5.73%)]	 Loss: 0.000008
Training epoch 21 [4480/67107 (6.68%)]	 Loss: 0.000011
Training epoch 21 [5120/67107 (7.63%)]	 Loss: 0.000009
Training epoch 21 [5760/67107 (8.59%)]	 Loss: 0.000007
Training epoch 21 [6400/67107 (9.54%)]	 Loss: 0.000003
Training epoch 21 [7040/67107 (10.50%)]	 Loss: 0.000033
Training epoch 21 [7680/67107 (11.45%)]	 Loss: 0.000204
Training epoch 21 [8320/67107 (12.40%)]	 Loss: 0.000030
Training epoch 21 [8960/67107 (13.36%)]	 Loss: 0.000034
Training epoch 21 [9600/67107 (14.31%)]	 Loss: 0.000062
Training epoch 21 [10240/67107 (15.27%)]	 Loss: 0.000015
Training epoch 21 [10880/67107 (16.22%)]	 Loss: 0.000016
Training epoch 21 [11520/67107 (17.18%)]	 Loss: 0.000014
Training epoch 21 [12160/67107 (18.13%)]	 Loss: 0.000011
Training epoch 21 [12800/67107 (19.08%)]	 Loss: 0.000016
Training epoch 21 [13440/67107 (20.04%)]	 Loss: 0.000017
Training epoch 21 [14080/67107 (20.99%)]	 Loss: 0.000042
Training epoch 21 [14720/67107 (21.95%)]	 Loss: 0.000103
Training epoch 21 [15360/67107 (22.90%)]	 Loss: 0.000013
Training epoch 21 [16000/67107 (23.85%)]	 Loss: 0.000013
Training epoch 21 [16640/67107 (24.81%)]	 Loss: 0.000111
Training epoch 21 [17280/67107 (25.76%)]	 Loss: 0.000040
Training epoch 21 [17920/67107 (26.72%)]	 Loss: 0.000009
Training epoch 21 [18560/67107 (27.67%)]	 Loss: 0.000046
Training epoch 21 [19200/67107 (28.63%)]	 Loss: 0.000116
Training epoch 21 [19840/67107 (29.58%)]	 Loss: 0.000019
Training epoch 21 [20480/67107 (30.53%)]	 Loss: 0.000026
Training epoch 21 [21120/67107 (31.49%)]	 Loss: 0.000015
Training epoch 21 [21760/67107 (32.44%)]	 Loss: 0.000214
Training epoch 21 [22400/67107 (33.40%)]	 Loss: 0.000164
Training epoch 21 [23040/67107 (34.35%)]	 Loss: 0.000032
Training epoch 21 [23680/67107 (35.31%)]	 Loss: 0.000035
Training epoch 21 [24320/67107 (36.26%)]	 Loss: 0.000018
Training epoch 21 [24960/67107 (37.21%)]	 Loss: 0.000120
Training epoch 21 [25600/67107 (38.17%)]	 Loss: 0.000017
Training epoch 21 [26240/67107 (39.12%)]	 Loss: 0.000016
Training epoch 21 [26880/67107 (40.08%)]	 Loss: 0.000020
Training epoch 21 [27520/67107 (41.03%)]	 Loss: 0.000023
Training epoch 21 [28160/67107 (41.98%)]	 Loss: 0.000058
Training epoch 21 [28800/67107 (42.94%)]	 Loss: 0.000066
Training epoch 21 [29440/67107 (43.89%)]	 Loss: 0.000035
Training epoch 21 [30080/67107 (44.85%)]	 Loss: 0.000021
Training epoch 21 [30720/67107 (45.80%)]	 Loss: 0.000015
Training epoch 21 [31360/67107 (46.76%)]	 Loss: 0.000003
Training epoch 21 [32000/67107 (47.71%)]	 Loss: 0.000006
Training epoch 21 [32640/67107 (48.66%)]	 Loss: 0.000012
Training epoch 21 [33280/67107 (49.62%)]	 Loss: 0.000017
Training epoch 21 [33920/67107 (50.57%)]	 Loss: 0.000087
Training epoch 21 [34560/67107 (51.53%)]	 Loss: 0.000055
Training epoch 21 [35200/67107 (52.48%)]	 Loss: 0.000008
Training epoch 21 [35840/67107 (53.44%)]	 Loss: 0.000012
Training epoch 21 [36480/67107 (54.39%)]	 Loss: 0.000012
Training epoch 21 [37120/67107 (55.34%)]	 Loss: 0.000027
Training epoch 21 [37760/67107 (56.30%)]	 Loss: 0.000165
Training epoch 21 [38400/67107 (57.25%)]	 Loss: 0.000064
Training epoch 21 [39040/67107 (58.21%)]	 Loss: 0.000010
Training epoch 21 [39680/67107 (59.16%)]	 Loss: 0.000005
Training epoch 21 [40320/67107 (60.11%)]	 Loss: 0.000006
Training epoch 21 [40960/67107 (61.07%)]	 Loss: 0.000016
Training epoch 21 [41600/67107 (62.02%)]	 Loss: 0.000059
Training epoch 21 [42240/67107 (62.98%)]	 Loss: 0.000054
Training epoch 21 [42880/67107 (63.93%)]	 Loss: 0.000052
Training epoch 21 [43520/67107 (64.89%)]	 Loss: 0.000024
Training epoch 21 [44160/67107 (65.84%)]	 Loss: 0.000023
Training epoch 21 [44800/67107 (66.79%)]	 Loss: 0.000024
Training epoch 21 [45440/67107 (67.75%)]	 Loss: 0.000052
Training epoch 21 [46080/67107 (68.70%)]	 Loss: 0.000021
Training epoch 21 [46720/67107 (69.66%)]	 Loss: 0.000006
Training epoch 21 [47360/67107 (70.61%)]	 Loss: 0.000038
Training epoch 21 [48000/67107 (71.56%)]	 Loss: 0.000015
Training epoch 21 [48640/67107 (72.52%)]	 Loss: 0.000007
Training epoch 21 [49280/67107 (73.47%)]	 Loss: 0.000006
Training epoch 21 [49920/67107 (74.43%)]	 Loss: 0.000026
Training epoch 21 [50560/67107 (75.38%)]	 Loss: 0.000052
Training epoch 21 [51200/67107 (76.34%)]	 Loss: 0.000120
Training epoch 21 [51840/67107 (77.29%)]	 Loss: 0.000029
Training epoch 21 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 21 [53120/67107 (79.20%)]	 Loss: 0.000016
Training epoch 21 [53760/67107 (80.15%)]	 Loss: 0.000055
Training epoch 21 [54400/67107 (81.11%)]	 Loss: 0.000010
Training epoch 21 [55040/67107 (82.06%)]	 Loss: 0.000015
Training epoch 21 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 21 [56320/67107 (83.97%)]	 Loss: 0.000002
Training epoch 21 [56960/67107 (84.92%)]	 Loss: 0.000024
Training epoch 21 [57600/67107 (85.88%)]	 Loss: 0.000039
Training epoch 21 [58240/67107 (86.83%)]	 Loss: 0.000004
Training epoch 21 [58880/67107 (87.79%)]	 Loss: 0.000034
Training epoch 21 [59520/67107 (88.74%)]	 Loss: 0.000014
Training epoch 21 [60160/67107 (89.69%)]	 Loss: 0.000149
Training epoch 21 [60800/67107 (90.65%)]	 Loss: 0.000052
Training epoch 21 [61440/67107 (91.60%)]	 Loss: 0.000023
Training epoch 21 [62080/67107 (92.56%)]	 Loss: 0.000025
Training epoch 21 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 21 [63360/67107 (94.47%)]	 Loss: 0.000014
Training epoch 21 [64000/67107 (95.42%)]	 Loss: 0.000003
Training epoch 21 [64640/67107 (96.37%)]	 Loss: 0.000061
Training epoch 21 [65280/67107 (97.33%)]	 Loss: 0.000026
Training epoch 21 [65920/67107 (98.28%)]	 Loss: 0.000258
Training epoch 21 [66560/67107 (99.24%)]	 Loss: 0.000047
Test set: Average Loss: 0.001422
Training epoch 22 [0/67107 (0.00%)]	 Loss: 0.000018
Training epoch 22 [640/67107 (0.95%)]	 Loss: 0.000092
Training epoch 22 [1280/67107 (1.91%)]	 Loss: 0.000017
Training epoch 22 [1920/67107 (2.86%)]	 Loss: 0.000054
Training epoch 22 [2560/67107 (3.82%)]	 Loss: 0.000024
Training epoch 22 [3200/67107 (4.77%)]	 Loss: 0.000040
Training epoch 22 [3840/67107 (5.73%)]	 Loss: 0.000011
Training epoch 22 [4480/67107 (6.68%)]	 Loss: 0.000034
Training epoch 22 [5120/67107 (7.63%)]	 Loss: 0.000024
Training epoch 22 [5760/67107 (8.59%)]	 Loss: 0.000008
Training epoch 22 [6400/67107 (9.54%)]	 Loss: 0.000180
Training epoch 22 [7040/67107 (10.50%)]	 Loss: 0.000120
Training epoch 22 [7680/67107 (11.45%)]	 Loss: 0.000019
Training epoch 22 [8320/67107 (12.40%)]	 Loss: 0.000009
Training epoch 22 [8960/67107 (13.36%)]	 Loss: 0.000040
Training epoch 22 [9600/67107 (14.31%)]	 Loss: 0.000049
Training epoch 22 [10240/67107 (15.27%)]	 Loss: 0.000010
Training epoch 22 [10880/67107 (16.22%)]	 Loss: 0.000069
Training epoch 22 [11520/67107 (17.18%)]	 Loss: 0.000041
Training epoch 22 [12160/67107 (18.13%)]	 Loss: 0.000307
Training epoch 22 [12800/67107 (19.08%)]	 Loss: 0.000061
Training epoch 22 [13440/67107 (20.04%)]	 Loss: 0.000065
Training epoch 22 [14080/67107 (20.99%)]	 Loss: 0.000149
Training epoch 22 [14720/67107 (21.95%)]	 Loss: 0.000032
Training epoch 22 [15360/67107 (22.90%)]	 Loss: 0.000054
Training epoch 22 [16000/67107 (23.85%)]	 Loss: 0.000027
Training epoch 22 [16640/67107 (24.81%)]	 Loss: 0.000269
Training epoch 22 [17280/67107 (25.76%)]	 Loss: 0.000017
Training epoch 22 [17920/67107 (26.72%)]	 Loss: 0.000013
Training epoch 22 [18560/67107 (27.67%)]	 Loss: 0.000007
Training epoch 22 [19200/67107 (28.63%)]	 Loss: 0.000044
Training epoch 22 [19840/67107 (29.58%)]	 Loss: 0.000006
Training epoch 22 [20480/67107 (30.53%)]	 Loss: 0.000021
Training epoch 22 [21120/67107 (31.49%)]	 Loss: 0.000019
Training epoch 22 [21760/67107 (32.44%)]	 Loss: 0.000076
Training epoch 22 [22400/67107 (33.40%)]	 Loss: 0.000119
Training epoch 22 [23040/67107 (34.35%)]	 Loss: 0.000056
Training epoch 22 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 22 [24320/67107 (36.26%)]	 Loss: 0.000010
Training epoch 22 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 22 [25600/67107 (38.17%)]	 Loss: 0.000021
Training epoch 22 [26240/67107 (39.12%)]	 Loss: 0.000063
Training epoch 22 [26880/67107 (40.08%)]	 Loss: 0.000043
Training epoch 22 [27520/67107 (41.03%)]	 Loss: 0.000029
Training epoch 22 [28160/67107 (41.98%)]	 Loss: 0.000016
Training epoch 22 [28800/67107 (42.94%)]	 Loss: 0.000017
Training epoch 22 [29440/67107 (43.89%)]	 Loss: 0.000048
Training epoch 22 [30080/67107 (44.85%)]	 Loss: 0.000040
Training epoch 22 [30720/67107 (45.80%)]	 Loss: 0.000009
Training epoch 22 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 22 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 22 [32640/67107 (48.66%)]	 Loss: 0.000006
Training epoch 22 [33280/67107 (49.62%)]	 Loss: 0.000003
Training epoch 22 [33920/67107 (50.57%)]	 Loss: 0.000004
Training epoch 22 [34560/67107 (51.53%)]	 Loss: 0.000049
Training epoch 22 [35200/67107 (52.48%)]	 Loss: 0.000117
Training epoch 22 [35840/67107 (53.44%)]	 Loss: 0.000021
Training epoch 22 [36480/67107 (54.39%)]	 Loss: 0.000060
Training epoch 22 [37120/67107 (55.34%)]	 Loss: 0.000028
Training epoch 22 [37760/67107 (56.30%)]	 Loss: 0.000010
Training epoch 22 [38400/67107 (57.25%)]	 Loss: 0.000005
Training epoch 22 [39040/67107 (58.21%)]	 Loss: 0.000033
Training epoch 22 [39680/67107 (59.16%)]	 Loss: 0.000014
Training epoch 22 [40320/67107 (60.11%)]	 Loss: 0.000010
Training epoch 22 [40960/67107 (61.07%)]	 Loss: 0.000029
Training epoch 22 [41600/67107 (62.02%)]	 Loss: 0.000019
Training epoch 22 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 22 [42880/67107 (63.93%)]	 Loss: 0.000036
Training epoch 22 [43520/67107 (64.89%)]	 Loss: 0.000023
Training epoch 22 [44160/67107 (65.84%)]	 Loss: 0.000061
Training epoch 22 [44800/67107 (66.79%)]	 Loss: 0.000009
Training epoch 22 [45440/67107 (67.75%)]	 Loss: 0.000007
Training epoch 22 [46080/67107 (68.70%)]	 Loss: 0.000007
Training epoch 22 [46720/67107 (69.66%)]	 Loss: 0.000033
Training epoch 22 [47360/67107 (70.61%)]	 Loss: 0.000020
Training epoch 22 [48000/67107 (71.56%)]	 Loss: 0.000061
Training epoch 22 [48640/67107 (72.52%)]	 Loss: 0.000020
Training epoch 22 [49280/67107 (73.47%)]	 Loss: 0.000020
Training epoch 22 [49920/67107 (74.43%)]	 Loss: 0.000072
Training epoch 22 [50560/67107 (75.38%)]	 Loss: 0.000026
Training epoch 22 [51200/67107 (76.34%)]	 Loss: 0.000044
Training epoch 22 [51840/67107 (77.29%)]	 Loss: 0.000020
Training epoch 22 [52480/67107 (78.24%)]	 Loss: 0.000016
Training epoch 22 [53120/67107 (79.20%)]	 Loss: 0.000014
Training epoch 22 [53760/67107 (80.15%)]	 Loss: 0.000010
Training epoch 22 [54400/67107 (81.11%)]	 Loss: 0.000025
Training epoch 22 [55040/67107 (82.06%)]	 Loss: 0.000011
Training epoch 22 [55680/67107 (83.02%)]	 Loss: 0.000012
Training epoch 22 [56320/67107 (83.97%)]	 Loss: 0.000082
Training epoch 22 [56960/67107 (84.92%)]	 Loss: 0.000012
Training epoch 22 [57600/67107 (85.88%)]	 Loss: 0.000045
Training epoch 22 [58240/67107 (86.83%)]	 Loss: 0.000010
Training epoch 22 [58880/67107 (87.79%)]	 Loss: 0.000018
Training epoch 22 [59520/67107 (88.74%)]	 Loss: 0.000041
Training epoch 22 [60160/67107 (89.69%)]	 Loss: 0.000014
Training epoch 22 [60800/67107 (90.65%)]	 Loss: 0.000014
Training epoch 22 [61440/67107 (91.60%)]	 Loss: 0.000009
Training epoch 22 [62080/67107 (92.56%)]	 Loss: 0.000122
Training epoch 22 [62720/67107 (93.51%)]	 Loss: 0.000014
Training epoch 22 [63360/67107 (94.47%)]	 Loss: 0.000027
Training epoch 22 [64000/67107 (95.42%)]	 Loss: 0.000006
Training epoch 22 [64640/67107 (96.37%)]	 Loss: 0.000007
Training epoch 22 [65280/67107 (97.33%)]	 Loss: 0.000066
Training epoch 22 [65920/67107 (98.28%)]	 Loss: 0.000057
Training epoch 22 [66560/67107 (99.24%)]	 Loss: 0.000019
Test set: Average Loss: 0.000486
Training epoch 23 [0/67107 (0.00%)]	 Loss: 0.000006
Training epoch 23 [640/67107 (0.95%)]	 Loss: 0.000003
Training epoch 23 [1280/67107 (1.91%)]	 Loss: 0.000009
Training epoch 23 [1920/67107 (2.86%)]	 Loss: 0.000059
Training epoch 23 [2560/67107 (3.82%)]	 Loss: 0.000007
Training epoch 23 [3200/67107 (4.77%)]	 Loss: 0.000023
Training epoch 23 [3840/67107 (5.73%)]	 Loss: 0.000015
Training epoch 23 [4480/67107 (6.68%)]	 Loss: 0.000022
Training epoch 23 [5120/67107 (7.63%)]	 Loss: 0.000029
Training epoch 23 [5760/67107 (8.59%)]	 Loss: 0.000123
Training epoch 23 [6400/67107 (9.54%)]	 Loss: 0.000102
Training epoch 23 [7040/67107 (10.50%)]	 Loss: 0.000008
Training epoch 23 [7680/67107 (11.45%)]	 Loss: 0.000019
Training epoch 23 [8320/67107 (12.40%)]	 Loss: 0.000002
Training epoch 23 [8960/67107 (13.36%)]	 Loss: 0.000008
Training epoch 23 [9600/67107 (14.31%)]	 Loss: 0.000069
Training epoch 23 [10240/67107 (15.27%)]	 Loss: 0.000037
Training epoch 23 [10880/67107 (16.22%)]	 Loss: 0.000056
Training epoch 23 [11520/67107 (17.18%)]	 Loss: 0.000025
Training epoch 23 [12160/67107 (18.13%)]	 Loss: 0.000014
Training epoch 23 [12800/67107 (19.08%)]	 Loss: 0.000035
Training epoch 23 [13440/67107 (20.04%)]	 Loss: 0.000015
Training epoch 23 [14080/67107 (20.99%)]	 Loss: 0.000009
Training epoch 23 [14720/67107 (21.95%)]	 Loss: 0.000013
Training epoch 23 [15360/67107 (22.90%)]	 Loss: 0.000003
Training epoch 23 [16000/67107 (23.85%)]	 Loss: 0.000008
Training epoch 23 [16640/67107 (24.81%)]	 Loss: 0.000002
Training epoch 23 [17280/67107 (25.76%)]	 Loss: 0.000006
Training epoch 23 [17920/67107 (26.72%)]	 Loss: 0.000042
Training epoch 23 [18560/67107 (27.67%)]	 Loss: 0.000025
Training epoch 23 [19200/67107 (28.63%)]	 Loss: 0.000006
Training epoch 23 [19840/67107 (29.58%)]	 Loss: 0.000016
Training epoch 23 [20480/67107 (30.53%)]	 Loss: 0.000012
Training epoch 23 [21120/67107 (31.49%)]	 Loss: 0.000058
Training epoch 23 [21760/67107 (32.44%)]	 Loss: 0.000077
Training epoch 23 [22400/67107 (33.40%)]	 Loss: 0.000012
Training epoch 23 [23040/67107 (34.35%)]	 Loss: 0.000007
Training epoch 23 [23680/67107 (35.31%)]	 Loss: 0.000007
Training epoch 23 [24320/67107 (36.26%)]	 Loss: 0.000006
Training epoch 23 [24960/67107 (37.21%)]	 Loss: 0.000043
Training epoch 23 [25600/67107 (38.17%)]	 Loss: 0.000187
Training epoch 23 [26240/67107 (39.12%)]	 Loss: 0.000018
Training epoch 23 [26880/67107 (40.08%)]	 Loss: 0.000004
Training epoch 23 [27520/67107 (41.03%)]	 Loss: 0.000042
Training epoch 23 [28160/67107 (41.98%)]	 Loss: 0.000078
Training epoch 23 [28800/67107 (42.94%)]	 Loss: 0.000056
Training epoch 23 [29440/67107 (43.89%)]	 Loss: 0.000022
Training epoch 23 [30080/67107 (44.85%)]	 Loss: 0.000017
Training epoch 23 [30720/67107 (45.80%)]	 Loss: 0.000031
Training epoch 23 [31360/67107 (46.76%)]	 Loss: 0.000014
Training epoch 23 [32000/67107 (47.71%)]	 Loss: 0.000042
Training epoch 23 [32640/67107 (48.66%)]	 Loss: 0.000039
Training epoch 23 [33280/67107 (49.62%)]	 Loss: 0.000103
Training epoch 23 [33920/67107 (50.57%)]	 Loss: 0.000058
Training epoch 23 [34560/67107 (51.53%)]	 Loss: 0.000020
Training epoch 23 [35200/67107 (52.48%)]	 Loss: 0.000015
Training epoch 23 [35840/67107 (53.44%)]	 Loss: 0.000065
Training epoch 23 [36480/67107 (54.39%)]	 Loss: 0.000029
Training epoch 23 [37120/67107 (55.34%)]	 Loss: 0.000023
Training epoch 23 [37760/67107 (56.30%)]	 Loss: 0.000133
Training epoch 23 [38400/67107 (57.25%)]	 Loss: 0.000050
Training epoch 23 [39040/67107 (58.21%)]	 Loss: 0.000009
Training epoch 23 [39680/67107 (59.16%)]	 Loss: 0.000019
Training epoch 23 [40320/67107 (60.11%)]	 Loss: 0.000077
Training epoch 23 [40960/67107 (61.07%)]	 Loss: 0.000072
Training epoch 23 [41600/67107 (62.02%)]	 Loss: 0.000008
Training epoch 23 [42240/67107 (62.98%)]	 Loss: 0.000006
Training epoch 23 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 23 [43520/67107 (64.89%)]	 Loss: 0.000012
Training epoch 23 [44160/67107 (65.84%)]	 Loss: 0.000010
Training epoch 23 [44800/67107 (66.79%)]	 Loss: 0.000002
Training epoch 23 [45440/67107 (67.75%)]	 Loss: 0.000002
Training epoch 23 [46080/67107 (68.70%)]	 Loss: 0.000006
Training epoch 23 [46720/67107 (69.66%)]	 Loss: 0.000029
Training epoch 23 [47360/67107 (70.61%)]	 Loss: 0.000018
Training epoch 23 [48000/67107 (71.56%)]	 Loss: 0.000110
Training epoch 23 [48640/67107 (72.52%)]	 Loss: 0.000010
Training epoch 23 [49280/67107 (73.47%)]	 Loss: 0.000011
Training epoch 23 [49920/67107 (74.43%)]	 Loss: 0.000334
Training epoch 23 [50560/67107 (75.38%)]	 Loss: 0.000022
Training epoch 23 [51200/67107 (76.34%)]	 Loss: 0.000035
Training epoch 23 [51840/67107 (77.29%)]	 Loss: 0.000012
Training epoch 23 [52480/67107 (78.24%)]	 Loss: 0.000064
Training epoch 23 [53120/67107 (79.20%)]	 Loss: 0.000027
Training epoch 23 [53760/67107 (80.15%)]	 Loss: 0.000121
Training epoch 23 [54400/67107 (81.11%)]	 Loss: 0.000018
Training epoch 23 [55040/67107 (82.06%)]	 Loss: 0.000042
Training epoch 23 [55680/67107 (83.02%)]	 Loss: 0.000037
Training epoch 23 [56320/67107 (83.97%)]	 Loss: 0.000034
Training epoch 23 [56960/67107 (84.92%)]	 Loss: 0.000029
Training epoch 23 [57600/67107 (85.88%)]	 Loss: 0.000039
Training epoch 23 [58240/67107 (86.83%)]	 Loss: 0.000010
Training epoch 23 [58880/67107 (87.79%)]	 Loss: 0.000008
Training epoch 23 [59520/67107 (88.74%)]	 Loss: 0.000013
Training epoch 23 [60160/67107 (89.69%)]	 Loss: 0.000021
Training epoch 23 [60800/67107 (90.65%)]	 Loss: 0.000074
Training epoch 23 [61440/67107 (91.60%)]	 Loss: 0.000193
Training epoch 23 [62080/67107 (92.56%)]	 Loss: 0.000016
Training epoch 23 [62720/67107 (93.51%)]	 Loss: 0.000028
Training epoch 23 [63360/67107 (94.47%)]	 Loss: 0.000013
Training epoch 23 [64000/67107 (95.42%)]	 Loss: 0.000163
Training epoch 23 [64640/67107 (96.37%)]	 Loss: 0.000045
Training epoch 23 [65280/67107 (97.33%)]	 Loss: 0.000014
Training epoch 23 [65920/67107 (98.28%)]	 Loss: 0.000008
Training epoch 23 [66560/67107 (99.24%)]	 Loss: 0.000006
Test set: Average Loss: 0.000391
Training epoch 24 [0/67107 (0.00%)]	 Loss: 0.000007
Training epoch 24 [640/67107 (0.95%)]	 Loss: 0.000064
Training epoch 24 [1280/67107 (1.91%)]	 Loss: 0.000010
Training epoch 24 [1920/67107 (2.86%)]	 Loss: 0.000211
Training epoch 24 [2560/67107 (3.82%)]	 Loss: 0.000054
Training epoch 24 [3200/67107 (4.77%)]	 Loss: 0.000020
Training epoch 24 [3840/67107 (5.73%)]	 Loss: 0.000031
Training epoch 24 [4480/67107 (6.68%)]	 Loss: 0.000012
Training epoch 24 [5120/67107 (7.63%)]	 Loss: 0.000230
Training epoch 24 [5760/67107 (8.59%)]	 Loss: 0.000024
Training epoch 24 [6400/67107 (9.54%)]	 Loss: 0.000015
Training epoch 24 [7040/67107 (10.50%)]	 Loss: 0.000057
Training epoch 24 [7680/67107 (11.45%)]	 Loss: 0.000041
Training epoch 24 [8320/67107 (12.40%)]	 Loss: 0.000047
Training epoch 24 [8960/67107 (13.36%)]	 Loss: 0.000051
Training epoch 24 [9600/67107 (14.31%)]	 Loss: 0.000023
Training epoch 24 [10240/67107 (15.27%)]	 Loss: 0.000015
Training epoch 24 [10880/67107 (16.22%)]	 Loss: 0.000016
Training epoch 24 [11520/67107 (17.18%)]	 Loss: 0.000008
Training epoch 24 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 24 [12800/67107 (19.08%)]	 Loss: 0.000002
Training epoch 24 [13440/67107 (20.04%)]	 Loss: 0.000002
Training epoch 24 [14080/67107 (20.99%)]	 Loss: 0.000001
Training epoch 24 [14720/67107 (21.95%)]	 Loss: 0.000007
Training epoch 24 [15360/67107 (22.90%)]	 Loss: 0.000020
Training epoch 24 [16000/67107 (23.85%)]	 Loss: 0.000004
Training epoch 24 [16640/67107 (24.81%)]	 Loss: 0.000071
Training epoch 24 [17280/67107 (25.76%)]	 Loss: 0.000058
Training epoch 24 [17920/67107 (26.72%)]	 Loss: 0.000010
Training epoch 24 [18560/67107 (27.67%)]	 Loss: 0.000012
Training epoch 24 [19200/67107 (28.63%)]	 Loss: 0.000012
Training epoch 24 [19840/67107 (29.58%)]	 Loss: 0.000010
Training epoch 24 [20480/67107 (30.53%)]	 Loss: 0.000008
Training epoch 24 [21120/67107 (31.49%)]	 Loss: 0.000056
Training epoch 24 [21760/67107 (32.44%)]	 Loss: 0.000041
Training epoch 24 [22400/67107 (33.40%)]	 Loss: 0.000034
Training epoch 24 [23040/67107 (34.35%)]	 Loss: 0.000077
Training epoch 24 [23680/67107 (35.31%)]	 Loss: 0.000025
Training epoch 24 [24320/67107 (36.26%)]	 Loss: 0.000024
Training epoch 24 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 24 [25600/67107 (38.17%)]	 Loss: 0.000009
Training epoch 24 [26240/67107 (39.12%)]	 Loss: 0.000007
Training epoch 24 [26880/67107 (40.08%)]	 Loss: 0.000014
Training epoch 24 [27520/67107 (41.03%)]	 Loss: 0.000010
Training epoch 24 [28160/67107 (41.98%)]	 Loss: 0.000051
Training epoch 24 [28800/67107 (42.94%)]	 Loss: 0.000024
Training epoch 24 [29440/67107 (43.89%)]	 Loss: 0.000018
Training epoch 24 [30080/67107 (44.85%)]	 Loss: 0.000022
Training epoch 24 [30720/67107 (45.80%)]	 Loss: 0.000031
Training epoch 24 [31360/67107 (46.76%)]	 Loss: 0.000021
Training epoch 24 [32000/67107 (47.71%)]	 Loss: 0.000009
Training epoch 24 [32640/67107 (48.66%)]	 Loss: 0.000010
Training epoch 24 [33280/67107 (49.62%)]	 Loss: 0.000008
Training epoch 24 [33920/67107 (50.57%)]	 Loss: 0.000009
Training epoch 24 [34560/67107 (51.53%)]	 Loss: 0.000042
Training epoch 24 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 24 [35840/67107 (53.44%)]	 Loss: 0.000020
Training epoch 24 [36480/67107 (54.39%)]	 Loss: 0.000018
Training epoch 24 [37120/67107 (55.34%)]	 Loss: 0.000017
Training epoch 24 [37760/67107 (56.30%)]	 Loss: 0.000034
Training epoch 24 [38400/67107 (57.25%)]	 Loss: 0.000057
Training epoch 24 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 24 [39680/67107 (59.16%)]	 Loss: 0.000008
Training epoch 24 [40320/67107 (60.11%)]	 Loss: 0.000005
Training epoch 24 [40960/67107 (61.07%)]	 Loss: 0.000006
Training epoch 24 [41600/67107 (62.02%)]	 Loss: 0.000002
Training epoch 24 [42240/67107 (62.98%)]	 Loss: 0.000010
Training epoch 24 [42880/67107 (63.93%)]	 Loss: 0.000006
Training epoch 24 [43520/67107 (64.89%)]	 Loss: 0.000017
Training epoch 24 [44160/67107 (65.84%)]	 Loss: 0.000007
Training epoch 24 [44800/67107 (66.79%)]	 Loss: 0.000008
Training epoch 24 [45440/67107 (67.75%)]	 Loss: 0.000007
Training epoch 24 [46080/67107 (68.70%)]	 Loss: 0.000055
Training epoch 24 [46720/67107 (69.66%)]	 Loss: 0.000067
Training epoch 24 [47360/67107 (70.61%)]	 Loss: 0.000012
Training epoch 24 [48000/67107 (71.56%)]	 Loss: 0.000010
Training epoch 24 [48640/67107 (72.52%)]	 Loss: 0.000008
Training epoch 24 [49280/67107 (73.47%)]	 Loss: 0.000041
Training epoch 24 [49920/67107 (74.43%)]	 Loss: 0.000009
Training epoch 24 [50560/67107 (75.38%)]	 Loss: 0.000037
Training epoch 24 [51200/67107 (76.34%)]	 Loss: 0.000009
Training epoch 24 [51840/67107 (77.29%)]	 Loss: 0.000013
Training epoch 24 [52480/67107 (78.24%)]	 Loss: 0.000015
Training epoch 24 [53120/67107 (79.20%)]	 Loss: 0.000017
Training epoch 24 [53760/67107 (80.15%)]	 Loss: 0.000025
Training epoch 24 [54400/67107 (81.11%)]	 Loss: 0.000019
Training epoch 24 [55040/67107 (82.06%)]	 Loss: 0.000024
Training epoch 24 [55680/67107 (83.02%)]	 Loss: 0.000043
Training epoch 24 [56320/67107 (83.97%)]	 Loss: 0.000076
Training epoch 24 [56960/67107 (84.92%)]	 Loss: 0.000060
Training epoch 24 [57600/67107 (85.88%)]	 Loss: 0.000023
Training epoch 24 [58240/67107 (86.83%)]	 Loss: 0.000012
Training epoch 24 [58880/67107 (87.79%)]	 Loss: 0.000282
Training epoch 24 [59520/67107 (88.74%)]	 Loss: 0.000042
Training epoch 24 [60160/67107 (89.69%)]	 Loss: 0.000014
Training epoch 24 [60800/67107 (90.65%)]	 Loss: 0.000020
Training epoch 24 [61440/67107 (91.60%)]	 Loss: 0.000008
Training epoch 24 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 24 [62720/67107 (93.51%)]	 Loss: 0.000002
Training epoch 24 [63360/67107 (94.47%)]	 Loss: 0.000002
Training epoch 24 [64000/67107 (95.42%)]	 Loss: 0.000002
Training epoch 24 [64640/67107 (96.37%)]	 Loss: 0.000010
Training epoch 24 [65280/67107 (97.33%)]	 Loss: 0.000035
Training epoch 24 [65920/67107 (98.28%)]	 Loss: 0.000021
Training epoch 24 [66560/67107 (99.24%)]	 Loss: 0.000023
Test set: Average Loss: 0.000143
Training epoch 25 [0/67107 (0.00%)]	 Loss: 0.000060
Training epoch 25 [640/67107 (0.95%)]	 Loss: 0.000028
Training epoch 25 [1280/67107 (1.91%)]	 Loss: 0.000009
Training epoch 25 [1920/67107 (2.86%)]	 Loss: 0.000048
Training epoch 25 [2560/67107 (3.82%)]	 Loss: 0.000040
Training epoch 25 [3200/67107 (4.77%)]	 Loss: 0.000031
Training epoch 25 [3840/67107 (5.73%)]	 Loss: 0.000018
Training epoch 25 [4480/67107 (6.68%)]	 Loss: 0.000019
Training epoch 25 [5120/67107 (7.63%)]	 Loss: 0.000057
Training epoch 25 [5760/67107 (8.59%)]	 Loss: 0.000056
Training epoch 25 [6400/67107 (9.54%)]	 Loss: 0.000021
Training epoch 25 [7040/67107 (10.50%)]	 Loss: 0.000010
Training epoch 25 [7680/67107 (11.45%)]	 Loss: 0.000007
Training epoch 25 [8320/67107 (12.40%)]	 Loss: 0.000007
Training epoch 25 [8960/67107 (13.36%)]	 Loss: 0.000006
Training epoch 25 [9600/67107 (14.31%)]	 Loss: 0.000016
Training epoch 25 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 25 [10880/67107 (16.22%)]	 Loss: 0.000030
Training epoch 25 [11520/67107 (17.18%)]	 Loss: 0.000011
Training epoch 25 [12160/67107 (18.13%)]	 Loss: 0.000017
Training epoch 25 [12800/67107 (19.08%)]	 Loss: 0.000061
Training epoch 25 [13440/67107 (20.04%)]	 Loss: 0.000052
Training epoch 25 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 25 [14720/67107 (21.95%)]	 Loss: 0.000005
Training epoch 25 [15360/67107 (22.90%)]	 Loss: 0.000009
Training epoch 25 [16000/67107 (23.85%)]	 Loss: 0.000006
Training epoch 25 [16640/67107 (24.81%)]	 Loss: 0.000010
Training epoch 25 [17280/67107 (25.76%)]	 Loss: 0.000023
Training epoch 25 [17920/67107 (26.72%)]	 Loss: 0.000003
Training epoch 25 [18560/67107 (27.67%)]	 Loss: 0.000014
Training epoch 25 [19200/67107 (28.63%)]	 Loss: 0.000013
Training epoch 25 [19840/67107 (29.58%)]	 Loss: 0.000012
Training epoch 25 [20480/67107 (30.53%)]	 Loss: 0.000082
Training epoch 25 [21120/67107 (31.49%)]	 Loss: 0.000020
Training epoch 25 [21760/67107 (32.44%)]	 Loss: 0.000013
Training epoch 25 [22400/67107 (33.40%)]	 Loss: 0.000034
Training epoch 25 [23040/67107 (34.35%)]	 Loss: 0.000028
Training epoch 25 [23680/67107 (35.31%)]	 Loss: 0.000022
Training epoch 25 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 25 [24960/67107 (37.21%)]	 Loss: 0.000007
Training epoch 25 [25600/67107 (38.17%)]	 Loss: 0.000002
Training epoch 25 [26240/67107 (39.12%)]	 Loss: 0.000041
Training epoch 25 [26880/67107 (40.08%)]	 Loss: 0.000016
Training epoch 25 [27520/67107 (41.03%)]	 Loss: 0.000017
Training epoch 25 [28160/67107 (41.98%)]	 Loss: 0.000029
Training epoch 25 [28800/67107 (42.94%)]	 Loss: 0.000057
Training epoch 25 [29440/67107 (43.89%)]	 Loss: 0.000070
Training epoch 25 [30080/67107 (44.85%)]	 Loss: 0.000133
Training epoch 25 [30720/67107 (45.80%)]	 Loss: 0.000038
Training epoch 25 [31360/67107 (46.76%)]	 Loss: 0.000022
Training epoch 25 [32000/67107 (47.71%)]	 Loss: 0.000040
Training epoch 25 [32640/67107 (48.66%)]	 Loss: 0.000013
Training epoch 25 [33280/67107 (49.62%)]	 Loss: 0.000042
Training epoch 25 [33920/67107 (50.57%)]	 Loss: 0.000432
Training epoch 25 [34560/67107 (51.53%)]	 Loss: 0.000015
Training epoch 25 [35200/67107 (52.48%)]	 Loss: 0.000347
Training epoch 25 [35840/67107 (53.44%)]	 Loss: 0.000117
Training epoch 25 [36480/67107 (54.39%)]	 Loss: 0.000018
Training epoch 25 [37120/67107 (55.34%)]	 Loss: 0.000031
Training epoch 25 [37760/67107 (56.30%)]	 Loss: 0.000099
Training epoch 25 [38400/67107 (57.25%)]	 Loss: 0.000095
Training epoch 25 [39040/67107 (58.21%)]	 Loss: 0.000025
Training epoch 25 [39680/67107 (59.16%)]	 Loss: 0.000081
Training epoch 25 [40320/67107 (60.11%)]	 Loss: 0.000106
Training epoch 25 [40960/67107 (61.07%)]	 Loss: 0.000071
Training epoch 25 [41600/67107 (62.02%)]	 Loss: 0.000013
Training epoch 25 [42240/67107 (62.98%)]	 Loss: 0.000002
Training epoch 25 [42880/67107 (63.93%)]	 Loss: 0.000011
Training epoch 25 [43520/67107 (64.89%)]	 Loss: 0.000106
Training epoch 25 [44160/67107 (65.84%)]	 Loss: 0.000097
Training epoch 25 [44800/67107 (66.79%)]	 Loss: 0.000021
Training epoch 25 [45440/67107 (67.75%)]	 Loss: 0.000019
Training epoch 25 [46080/67107 (68.70%)]	 Loss: 0.000013
Training epoch 25 [46720/67107 (69.66%)]	 Loss: 0.000033
Training epoch 25 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 25 [48000/67107 (71.56%)]	 Loss: 0.000096
Training epoch 25 [48640/67107 (72.52%)]	 Loss: 0.000015
Training epoch 25 [49280/67107 (73.47%)]	 Loss: 0.000006
Training epoch 25 [49920/67107 (74.43%)]	 Loss: 0.000251
Training epoch 25 [50560/67107 (75.38%)]	 Loss: 0.000074
Training epoch 25 [51200/67107 (76.34%)]	 Loss: 0.000121
Training epoch 25 [51840/67107 (77.29%)]	 Loss: 0.000109
Training epoch 25 [52480/67107 (78.24%)]	 Loss: 0.000052
Training epoch 25 [53120/67107 (79.20%)]	 Loss: 0.000013
Training epoch 25 [53760/67107 (80.15%)]	 Loss: 0.000004
Training epoch 25 [54400/67107 (81.11%)]	 Loss: 0.000005
Training epoch 25 [55040/67107 (82.06%)]	 Loss: 0.000007
Training epoch 25 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 25 [56320/67107 (83.97%)]	 Loss: 0.000005
Training epoch 25 [56960/67107 (84.92%)]	 Loss: 0.000002
Training epoch 25 [57600/67107 (85.88%)]	 Loss: 0.000041
Training epoch 25 [58240/67107 (86.83%)]	 Loss: 0.000045
Training epoch 25 [58880/67107 (87.79%)]	 Loss: 0.000005
Training epoch 25 [59520/67107 (88.74%)]	 Loss: 0.000136
Training epoch 25 [60160/67107 (89.69%)]	 Loss: 0.000049
Training epoch 25 [60800/67107 (90.65%)]	 Loss: 0.000029
Training epoch 25 [61440/67107 (91.60%)]	 Loss: 0.000009
Training epoch 25 [62080/67107 (92.56%)]	 Loss: 0.000144
Training epoch 25 [62720/67107 (93.51%)]	 Loss: 0.000036
Training epoch 25 [63360/67107 (94.47%)]	 Loss: 0.000012
Training epoch 25 [64000/67107 (95.42%)]	 Loss: 0.000008
Training epoch 25 [64640/67107 (96.37%)]	 Loss: 0.000008
Training epoch 25 [65280/67107 (97.33%)]	 Loss: 0.000082
Training epoch 25 [65920/67107 (98.28%)]	 Loss: 0.000042
Training epoch 25 [66560/67107 (99.24%)]	 Loss: 0.000029
Test set: Average Loss: 0.000599
Training epoch 26 [0/67107 (0.00%)]	 Loss: 0.000158
Training epoch 26 [640/67107 (0.95%)]	 Loss: 0.000049
Training epoch 26 [1280/67107 (1.91%)]	 Loss: 0.000030
Training epoch 26 [1920/67107 (2.86%)]	 Loss: 0.000083
Training epoch 26 [2560/67107 (3.82%)]	 Loss: 0.000016
Training epoch 26 [3200/67107 (4.77%)]	 Loss: 0.000016
Training epoch 26 [3840/67107 (5.73%)]	 Loss: 0.000008
Training epoch 26 [4480/67107 (6.68%)]	 Loss: 0.000024
Training epoch 26 [5120/67107 (7.63%)]	 Loss: 0.000009
Training epoch 26 [5760/67107 (8.59%)]	 Loss: 0.000021
Training epoch 26 [6400/67107 (9.54%)]	 Loss: 0.000065
Training epoch 26 [7040/67107 (10.50%)]	 Loss: 0.000157
Training epoch 26 [7680/67107 (11.45%)]	 Loss: 0.000012
Training epoch 26 [8320/67107 (12.40%)]	 Loss: 0.000029
Training epoch 26 [8960/67107 (13.36%)]	 Loss: 0.000134
Training epoch 26 [9600/67107 (14.31%)]	 Loss: 0.000014
Training epoch 26 [10240/67107 (15.27%)]	 Loss: 0.000019
Training epoch 26 [10880/67107 (16.22%)]	 Loss: 0.000038
Training epoch 26 [11520/67107 (17.18%)]	 Loss: 0.000029
Training epoch 26 [12160/67107 (18.13%)]	 Loss: 0.000008
Training epoch 26 [12800/67107 (19.08%)]	 Loss: 0.000067
Training epoch 26 [13440/67107 (20.04%)]	 Loss: 0.000049
Training epoch 26 [14080/67107 (20.99%)]	 Loss: 0.000012
Training epoch 26 [14720/67107 (21.95%)]	 Loss: 0.000007
Training epoch 26 [15360/67107 (22.90%)]	 Loss: 0.000015
Training epoch 26 [16000/67107 (23.85%)]	 Loss: 0.000081
Training epoch 26 [16640/67107 (24.81%)]	 Loss: 0.000019
Training epoch 26 [17280/67107 (25.76%)]	 Loss: 0.000010
Training epoch 26 [17920/67107 (26.72%)]	 Loss: 0.000075
Training epoch 26 [18560/67107 (27.67%)]	 Loss: 0.000128
Training epoch 26 [19200/67107 (28.63%)]	 Loss: 0.000028
Training epoch 26 [19840/67107 (29.58%)]	 Loss: 0.000042
Training epoch 26 [20480/67107 (30.53%)]	 Loss: 0.000120
Training epoch 26 [21120/67107 (31.49%)]	 Loss: 0.000065
Training epoch 26 [21760/67107 (32.44%)]	 Loss: 0.000010
Training epoch 26 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 26 [23040/67107 (34.35%)]	 Loss: 0.000005
Training epoch 26 [23680/67107 (35.31%)]	 Loss: 0.000013
Training epoch 26 [24320/67107 (36.26%)]	 Loss: 0.000005
Training epoch 26 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 26 [25600/67107 (38.17%)]	 Loss: 0.000025
Training epoch 26 [26240/67107 (39.12%)]	 Loss: 0.000061
Training epoch 26 [26880/67107 (40.08%)]	 Loss: 0.000094
Training epoch 26 [27520/67107 (41.03%)]	 Loss: 0.000033
Training epoch 26 [28160/67107 (41.98%)]	 Loss: 0.000011
Training epoch 26 [28800/67107 (42.94%)]	 Loss: 0.000015
Training epoch 26 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 26 [30080/67107 (44.85%)]	 Loss: 0.000011
Training epoch 26 [30720/67107 (45.80%)]	 Loss: 0.000049
Training epoch 26 [31360/67107 (46.76%)]	 Loss: 0.000054
Training epoch 26 [32000/67107 (47.71%)]	 Loss: 0.000025
Training epoch 26 [32640/67107 (48.66%)]	 Loss: 0.000063
Training epoch 26 [33280/67107 (49.62%)]	 Loss: 0.000018
Training epoch 26 [33920/67107 (50.57%)]	 Loss: 0.000033
Training epoch 26 [34560/67107 (51.53%)]	 Loss: 0.000016
Training epoch 26 [35200/67107 (52.48%)]	 Loss: 0.000036
Training epoch 26 [35840/67107 (53.44%)]	 Loss: 0.000010
Training epoch 26 [36480/67107 (54.39%)]	 Loss: 0.000004
Training epoch 26 [37120/67107 (55.34%)]	 Loss: 0.000020
Training epoch 26 [37760/67107 (56.30%)]	 Loss: 0.000128
Training epoch 26 [38400/67107 (57.25%)]	 Loss: 0.000027
Training epoch 26 [39040/67107 (58.21%)]	 Loss: 0.000043
Training epoch 26 [39680/67107 (59.16%)]	 Loss: 0.000080
Training epoch 26 [40320/67107 (60.11%)]	 Loss: 0.000017
Training epoch 26 [40960/67107 (61.07%)]	 Loss: 0.000044
Training epoch 26 [41600/67107 (62.02%)]	 Loss: 0.000015
Training epoch 26 [42240/67107 (62.98%)]	 Loss: 0.000006
Training epoch 26 [42880/67107 (63.93%)]	 Loss: 0.000004
Training epoch 26 [43520/67107 (64.89%)]	 Loss: 0.000125
Training epoch 26 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 26 [44800/67107 (66.79%)]	 Loss: 0.000019
Training epoch 26 [45440/67107 (67.75%)]	 Loss: 0.000008
Training epoch 26 [46080/67107 (68.70%)]	 Loss: 0.000012
Training epoch 26 [46720/67107 (69.66%)]	 Loss: 0.000026
Training epoch 26 [47360/67107 (70.61%)]	 Loss: 0.000059
Training epoch 26 [48000/67107 (71.56%)]	 Loss: 0.000032
Training epoch 26 [48640/67107 (72.52%)]	 Loss: 0.000003
Training epoch 26 [49280/67107 (73.47%)]	 Loss: 0.000006
Training epoch 26 [49920/67107 (74.43%)]	 Loss: 0.000006
Training epoch 26 [50560/67107 (75.38%)]	 Loss: 0.000008
Training epoch 26 [51200/67107 (76.34%)]	 Loss: 0.000011
Training epoch 26 [51840/67107 (77.29%)]	 Loss: 0.000009
Training epoch 26 [52480/67107 (78.24%)]	 Loss: 0.000014
Training epoch 26 [53120/67107 (79.20%)]	 Loss: 0.000017
Training epoch 26 [53760/67107 (80.15%)]	 Loss: 0.000018
Training epoch 26 [54400/67107 (81.11%)]	 Loss: 0.000020
Training epoch 26 [55040/67107 (82.06%)]	 Loss: 0.000018
Training epoch 26 [55680/67107 (83.02%)]	 Loss: 0.000051
Training epoch 26 [56320/67107 (83.97%)]	 Loss: 0.000032
Training epoch 26 [56960/67107 (84.92%)]	 Loss: 0.000026
Training epoch 26 [57600/67107 (85.88%)]	 Loss: 0.000027
Training epoch 26 [58240/67107 (86.83%)]	 Loss: 0.000033
Training epoch 26 [58880/67107 (87.79%)]	 Loss: 0.000217
Training epoch 26 [59520/67107 (88.74%)]	 Loss: 0.000023
Training epoch 26 [60160/67107 (89.69%)]	 Loss: 0.000056
Training epoch 26 [60800/67107 (90.65%)]	 Loss: 0.000070
Training epoch 26 [61440/67107 (91.60%)]	 Loss: 0.000168
Training epoch 26 [62080/67107 (92.56%)]	 Loss: 0.000010
Training epoch 26 [62720/67107 (93.51%)]	 Loss: 0.000002
Training epoch 26 [63360/67107 (94.47%)]	 Loss: 0.000013
Training epoch 26 [64000/67107 (95.42%)]	 Loss: 0.000063
Training epoch 26 [64640/67107 (96.37%)]	 Loss: 0.000016
Training epoch 26 [65280/67107 (97.33%)]	 Loss: 0.000027
Training epoch 26 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 26 [66560/67107 (99.24%)]	 Loss: 0.000006
Test set: Average Loss: 0.000245
Training epoch 27 [0/67107 (0.00%)]	 Loss: 0.000006
Training epoch 27 [640/67107 (0.95%)]	 Loss: 0.000078
Training epoch 27 [1280/67107 (1.91%)]	 Loss: 0.000063
Training epoch 27 [1920/67107 (2.86%)]	 Loss: 0.000031
Training epoch 27 [2560/67107 (3.82%)]	 Loss: 0.000055
Training epoch 27 [3200/67107 (4.77%)]	 Loss: 0.000033
Training epoch 27 [3840/67107 (5.73%)]	 Loss: 0.000026
Training epoch 27 [4480/67107 (6.68%)]	 Loss: 0.000007
Training epoch 27 [5120/67107 (7.63%)]	 Loss: 0.000036
Training epoch 27 [5760/67107 (8.59%)]	 Loss: 0.000011
Training epoch 27 [6400/67107 (9.54%)]	 Loss: 0.000006
Training epoch 27 [7040/67107 (10.50%)]	 Loss: 0.000009
Training epoch 27 [7680/67107 (11.45%)]	 Loss: 0.000036
Training epoch 27 [8320/67107 (12.40%)]	 Loss: 0.000107
Training epoch 27 [8960/67107 (13.36%)]	 Loss: 0.000032
Training epoch 27 [9600/67107 (14.31%)]	 Loss: 0.000137
Training epoch 27 [10240/67107 (15.27%)]	 Loss: 0.000010
Training epoch 27 [10880/67107 (16.22%)]	 Loss: 0.000036
Training epoch 27 [11520/67107 (17.18%)]	 Loss: 0.000015
Training epoch 27 [12160/67107 (18.13%)]	 Loss: 0.000021
Training epoch 27 [12800/67107 (19.08%)]	 Loss: 0.000003
Training epoch 27 [13440/67107 (20.04%)]	 Loss: 0.000008
Training epoch 27 [14080/67107 (20.99%)]	 Loss: 0.000003
Training epoch 27 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 27 [15360/67107 (22.90%)]	 Loss: 0.000065
Training epoch 27 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 27 [16640/67107 (24.81%)]	 Loss: 0.000018
Training epoch 27 [17280/67107 (25.76%)]	 Loss: 0.000015
Training epoch 27 [17920/67107 (26.72%)]	 Loss: 0.000056
Training epoch 27 [18560/67107 (27.67%)]	 Loss: 0.000037
Training epoch 27 [19200/67107 (28.63%)]	 Loss: 0.000008
Training epoch 27 [19840/67107 (29.58%)]	 Loss: 0.000066
Training epoch 27 [20480/67107 (30.53%)]	 Loss: 0.000019
Training epoch 27 [21120/67107 (31.49%)]	 Loss: 0.000050
Training epoch 27 [21760/67107 (32.44%)]	 Loss: 0.000013
Training epoch 27 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 27 [23040/67107 (34.35%)]	 Loss: 0.000005
Training epoch 27 [23680/67107 (35.31%)]	 Loss: 0.000044
Training epoch 27 [24320/67107 (36.26%)]	 Loss: 0.000033
Training epoch 27 [24960/67107 (37.21%)]	 Loss: 0.000053
Training epoch 27 [25600/67107 (38.17%)]	 Loss: 0.000025
Training epoch 27 [26240/67107 (39.12%)]	 Loss: 0.000009
Training epoch 27 [26880/67107 (40.08%)]	 Loss: 0.000004
Training epoch 27 [27520/67107 (41.03%)]	 Loss: 0.000027
Training epoch 27 [28160/67107 (41.98%)]	 Loss: 0.000055
Training epoch 27 [28800/67107 (42.94%)]	 Loss: 0.000039
Training epoch 27 [29440/67107 (43.89%)]	 Loss: 0.000171
Training epoch 27 [30080/67107 (44.85%)]	 Loss: 0.000018
Training epoch 27 [30720/67107 (45.80%)]	 Loss: 0.000067
Training epoch 27 [31360/67107 (46.76%)]	 Loss: 0.000111
Training epoch 27 [32000/67107 (47.71%)]	 Loss: 0.000125
Training epoch 27 [32640/67107 (48.66%)]	 Loss: 0.000018
Training epoch 27 [33280/67107 (49.62%)]	 Loss: 0.000003
Training epoch 27 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 27 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 27 [35200/67107 (52.48%)]	 Loss: 0.000009
Training epoch 27 [35840/67107 (53.44%)]	 Loss: 0.000138
Training epoch 27 [36480/67107 (54.39%)]	 Loss: 0.000043
Training epoch 27 [37120/67107 (55.34%)]	 Loss: 0.000027
Training epoch 27 [37760/67107 (56.30%)]	 Loss: 0.000049
Training epoch 27 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 27 [39040/67107 (58.21%)]	 Loss: 0.000007
Training epoch 27 [39680/67107 (59.16%)]	 Loss: 0.000013
Training epoch 27 [40320/67107 (60.11%)]	 Loss: 0.000024
Training epoch 27 [40960/67107 (61.07%)]	 Loss: 0.000030
Training epoch 27 [41600/67107 (62.02%)]	 Loss: 0.000007
Training epoch 27 [42240/67107 (62.98%)]	 Loss: 0.000024
Training epoch 27 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 27 [43520/67107 (64.89%)]	 Loss: 0.000099
Training epoch 27 [44160/67107 (65.84%)]	 Loss: 0.000028
Training epoch 27 [44800/67107 (66.79%)]	 Loss: 0.000030
Training epoch 27 [45440/67107 (67.75%)]	 Loss: 0.000003
Training epoch 27 [46080/67107 (68.70%)]	 Loss: 0.000059
Training epoch 27 [46720/67107 (69.66%)]	 Loss: 0.000022
Training epoch 27 [47360/67107 (70.61%)]	 Loss: 0.000029
Training epoch 27 [48000/67107 (71.56%)]	 Loss: 0.000125
Training epoch 27 [48640/67107 (72.52%)]	 Loss: 0.000026
Training epoch 27 [49280/67107 (73.47%)]	 Loss: 0.000029
Training epoch 27 [49920/67107 (74.43%)]	 Loss: 0.000026
Training epoch 27 [50560/67107 (75.38%)]	 Loss: 0.000006
Training epoch 27 [51200/67107 (76.34%)]	 Loss: 0.000065
Training epoch 27 [51840/67107 (77.29%)]	 Loss: 0.000009
Training epoch 27 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 27 [53120/67107 (79.20%)]	 Loss: 0.000002
Training epoch 27 [53760/67107 (80.15%)]	 Loss: 0.000013
Training epoch 27 [54400/67107 (81.11%)]	 Loss: 0.000003
Training epoch 27 [55040/67107 (82.06%)]	 Loss: 0.000024
Training epoch 27 [55680/67107 (83.02%)]	 Loss: 0.000006
Training epoch 27 [56320/67107 (83.97%)]	 Loss: 0.000011
Training epoch 27 [56960/67107 (84.92%)]	 Loss: 0.000050
Training epoch 27 [57600/67107 (85.88%)]	 Loss: 0.000009
Training epoch 27 [58240/67107 (86.83%)]	 Loss: 0.000046
Training epoch 27 [58880/67107 (87.79%)]	 Loss: 0.000046
Training epoch 27 [59520/67107 (88.74%)]	 Loss: 0.000032
Training epoch 27 [60160/67107 (89.69%)]	 Loss: 0.000022
Training epoch 27 [60800/67107 (90.65%)]	 Loss: 0.000030
Training epoch 27 [61440/67107 (91.60%)]	 Loss: 0.000008
Training epoch 27 [62080/67107 (92.56%)]	 Loss: 0.000013
Training epoch 27 [62720/67107 (93.51%)]	 Loss: 0.000012
Training epoch 27 [63360/67107 (94.47%)]	 Loss: 0.000005
Training epoch 27 [64000/67107 (95.42%)]	 Loss: 0.000026
Training epoch 27 [64640/67107 (96.37%)]	 Loss: 0.000108
Training epoch 27 [65280/67107 (97.33%)]	 Loss: 0.000020
Training epoch 27 [65920/67107 (98.28%)]	 Loss: 0.000160
Training epoch 27 [66560/67107 (99.24%)]	 Loss: 0.000165
Test set: Average Loss: 0.001161
Training epoch 28 [0/67107 (0.00%)]	 Loss: 0.000059
Training epoch 28 [640/67107 (0.95%)]	 Loss: 0.000015
Training epoch 28 [1280/67107 (1.91%)]	 Loss: 0.000034
Training epoch 28 [1920/67107 (2.86%)]	 Loss: 0.000077
Training epoch 28 [2560/67107 (3.82%)]	 Loss: 0.000025
Training epoch 28 [3200/67107 (4.77%)]	 Loss: 0.000146
Training epoch 28 [3840/67107 (5.73%)]	 Loss: 0.000044
Training epoch 28 [4480/67107 (6.68%)]	 Loss: 0.000019
Training epoch 28 [5120/67107 (7.63%)]	 Loss: 0.000018
Training epoch 28 [5760/67107 (8.59%)]	 Loss: 0.000114
Training epoch 28 [6400/67107 (9.54%)]	 Loss: 0.000069
Training epoch 28 [7040/67107 (10.50%)]	 Loss: 0.000037
Training epoch 28 [7680/67107 (11.45%)]	 Loss: 0.000011
Training epoch 28 [8320/67107 (12.40%)]	 Loss: 0.000022
Training epoch 28 [8960/67107 (13.36%)]	 Loss: 0.000007
Training epoch 28 [9600/67107 (14.31%)]	 Loss: 0.000003
Training epoch 28 [10240/67107 (15.27%)]	 Loss: 0.000009
Training epoch 28 [10880/67107 (16.22%)]	 Loss: 0.000010
Training epoch 28 [11520/67107 (17.18%)]	 Loss: 0.000020
Training epoch 28 [12160/67107 (18.13%)]	 Loss: 0.000265
Training epoch 28 [12800/67107 (19.08%)]	 Loss: 0.000033
Training epoch 28 [13440/67107 (20.04%)]	 Loss: 0.000090
Training epoch 28 [14080/67107 (20.99%)]	 Loss: 0.000043
Training epoch 28 [14720/67107 (21.95%)]	 Loss: 0.000035
Training epoch 28 [15360/67107 (22.90%)]	 Loss: 0.000090
Training epoch 28 [16000/67107 (23.85%)]	 Loss: 0.000147
Training epoch 28 [16640/67107 (24.81%)]	 Loss: 0.000017
Training epoch 28 [17280/67107 (25.76%)]	 Loss: 0.000022
Training epoch 28 [17920/67107 (26.72%)]	 Loss: 0.000022
Training epoch 28 [18560/67107 (27.67%)]	 Loss: 0.000102
Training epoch 28 [19200/67107 (28.63%)]	 Loss: 0.000042
Training epoch 28 [19840/67107 (29.58%)]	 Loss: 0.000170
Training epoch 28 [20480/67107 (30.53%)]	 Loss: 0.000140
Training epoch 28 [21120/67107 (31.49%)]	 Loss: 0.000073
Training epoch 28 [21760/67107 (32.44%)]	 Loss: 0.000011
Training epoch 28 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 28 [23040/67107 (34.35%)]	 Loss: 0.000008
Training epoch 28 [23680/67107 (35.31%)]	 Loss: 0.000007
Training epoch 28 [24320/67107 (36.26%)]	 Loss: 0.000010
Training epoch 28 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 28 [25600/67107 (38.17%)]	 Loss: 0.000046
Training epoch 28 [26240/67107 (39.12%)]	 Loss: 0.000099
Training epoch 28 [26880/67107 (40.08%)]	 Loss: 0.000039
Training epoch 28 [27520/67107 (41.03%)]	 Loss: 0.000027
Training epoch 28 [28160/67107 (41.98%)]	 Loss: 0.000020
Training epoch 28 [28800/67107 (42.94%)]	 Loss: 0.000031
Training epoch 28 [29440/67107 (43.89%)]	 Loss: 0.000009
Training epoch 28 [30080/67107 (44.85%)]	 Loss: 0.000102
Training epoch 28 [30720/67107 (45.80%)]	 Loss: 0.000013
Training epoch 28 [31360/67107 (46.76%)]	 Loss: 0.000008
Training epoch 28 [32000/67107 (47.71%)]	 Loss: 0.000010
Training epoch 28 [32640/67107 (48.66%)]	 Loss: 0.000009
Training epoch 28 [33280/67107 (49.62%)]	 Loss: 0.000003
Training epoch 28 [33920/67107 (50.57%)]	 Loss: 0.000038
Training epoch 28 [34560/67107 (51.53%)]	 Loss: 0.000029
Training epoch 28 [35200/67107 (52.48%)]	 Loss: 0.000048
Training epoch 28 [35840/67107 (53.44%)]	 Loss: 0.000081
Training epoch 28 [36480/67107 (54.39%)]	 Loss: 0.000018
Training epoch 28 [37120/67107 (55.34%)]	 Loss: 0.000041
Training epoch 28 [37760/67107 (56.30%)]	 Loss: 0.000011
Training epoch 28 [38400/67107 (57.25%)]	 Loss: 0.000021
Training epoch 28 [39040/67107 (58.21%)]	 Loss: 0.000066
Training epoch 28 [39680/67107 (59.16%)]	 Loss: 0.000022
Training epoch 28 [40320/67107 (60.11%)]	 Loss: 0.000038
Training epoch 28 [40960/67107 (61.07%)]	 Loss: 0.000008
Training epoch 28 [41600/67107 (62.02%)]	 Loss: 0.000025
Training epoch 28 [42240/67107 (62.98%)]	 Loss: 0.000061
Training epoch 28 [42880/67107 (63.93%)]	 Loss: 0.000038
Training epoch 28 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 28 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 28 [44800/67107 (66.79%)]	 Loss: 0.000003
Training epoch 28 [45440/67107 (67.75%)]	 Loss: 0.000033
Training epoch 28 [46080/67107 (68.70%)]	 Loss: 0.000041
Training epoch 28 [46720/67107 (69.66%)]	 Loss: 0.000043
Training epoch 28 [47360/67107 (70.61%)]	 Loss: 0.000019
Training epoch 28 [48000/67107 (71.56%)]	 Loss: 0.000011
Training epoch 28 [48640/67107 (72.52%)]	 Loss: 0.000013
Training epoch 28 [49280/67107 (73.47%)]	 Loss: 0.000047
Training epoch 28 [49920/67107 (74.43%)]	 Loss: 0.000047
Training epoch 28 [50560/67107 (75.38%)]	 Loss: 0.000008
Training epoch 28 [51200/67107 (76.34%)]	 Loss: 0.000010
Training epoch 28 [51840/67107 (77.29%)]	 Loss: 0.000010
Training epoch 28 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 28 [53120/67107 (79.20%)]	 Loss: 0.000002
Training epoch 28 [53760/67107 (80.15%)]	 Loss: 0.000010
Training epoch 28 [54400/67107 (81.11%)]	 Loss: 0.000042
Training epoch 28 [55040/67107 (82.06%)]	 Loss: 0.000055
Training epoch 28 [55680/67107 (83.02%)]	 Loss: 0.000012
Training epoch 28 [56320/67107 (83.97%)]	 Loss: 0.000037
Training epoch 28 [56960/67107 (84.92%)]	 Loss: 0.000048
Training epoch 28 [57600/67107 (85.88%)]	 Loss: 0.000006
Training epoch 28 [58240/67107 (86.83%)]	 Loss: 0.000002
Training epoch 28 [58880/67107 (87.79%)]	 Loss: 0.000030
Training epoch 28 [59520/67107 (88.74%)]	 Loss: 0.000019
Training epoch 28 [60160/67107 (89.69%)]	 Loss: 0.000012
Training epoch 28 [60800/67107 (90.65%)]	 Loss: 0.000009
Training epoch 28 [61440/67107 (91.60%)]	 Loss: 0.000006
Training epoch 28 [62080/67107 (92.56%)]	 Loss: 0.000089
Training epoch 28 [62720/67107 (93.51%)]	 Loss: 0.000090
Training epoch 28 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 28 [64000/67107 (95.42%)]	 Loss: 0.000012
Training epoch 28 [64640/67107 (96.37%)]	 Loss: 0.000009
Training epoch 28 [65280/67107 (97.33%)]	 Loss: 0.000024
Training epoch 28 [65920/67107 (98.28%)]	 Loss: 0.000020
Training epoch 28 [66560/67107 (99.24%)]	 Loss: 0.000011
Test set: Average Loss: 0.000430
Training epoch 29 [0/67107 (0.00%)]	 Loss: 0.000012
Training epoch 29 [640/67107 (0.95%)]	 Loss: 0.000218
Training epoch 29 [1280/67107 (1.91%)]	 Loss: 0.000029
Training epoch 29 [1920/67107 (2.86%)]	 Loss: 0.000035
Training epoch 29 [2560/67107 (3.82%)]	 Loss: 0.000085
Training epoch 29 [3200/67107 (4.77%)]	 Loss: 0.000033
Training epoch 29 [3840/67107 (5.73%)]	 Loss: 0.000037
Training epoch 29 [4480/67107 (6.68%)]	 Loss: 0.000021
Training epoch 29 [5120/67107 (7.63%)]	 Loss: 0.000003
Training epoch 29 [5760/67107 (8.59%)]	 Loss: 0.000007
Training epoch 29 [6400/67107 (9.54%)]	 Loss: 0.000006
Training epoch 29 [7040/67107 (10.50%)]	 Loss: 0.000075
Training epoch 29 [7680/67107 (11.45%)]	 Loss: 0.000012
Training epoch 29 [8320/67107 (12.40%)]	 Loss: 0.000032
Training epoch 29 [8960/67107 (13.36%)]	 Loss: 0.000060
Training epoch 29 [9600/67107 (14.31%)]	 Loss: 0.000047
Training epoch 29 [10240/67107 (15.27%)]	 Loss: 0.000035
Training epoch 29 [10880/67107 (16.22%)]	 Loss: 0.000007
Training epoch 29 [11520/67107 (17.18%)]	 Loss: 0.000017
Training epoch 29 [12160/67107 (18.13%)]	 Loss: 0.000071
Training epoch 29 [12800/67107 (19.08%)]	 Loss: 0.000076
Training epoch 29 [13440/67107 (20.04%)]	 Loss: 0.000008
Training epoch 29 [14080/67107 (20.99%)]	 Loss: 0.000012
Training epoch 29 [14720/67107 (21.95%)]	 Loss: 0.000001
Training epoch 29 [15360/67107 (22.90%)]	 Loss: 0.000006
Training epoch 29 [16000/67107 (23.85%)]	 Loss: 0.000005
Training epoch 29 [16640/67107 (24.81%)]	 Loss: 0.000009
Training epoch 29 [17280/67107 (25.76%)]	 Loss: 0.000013
Training epoch 29 [17920/67107 (26.72%)]	 Loss: 0.000050
Training epoch 29 [18560/67107 (27.67%)]	 Loss: 0.000019
Training epoch 29 [19200/67107 (28.63%)]	 Loss: 0.000039
Training epoch 29 [19840/67107 (29.58%)]	 Loss: 0.000014
Training epoch 29 [20480/67107 (30.53%)]	 Loss: 0.000023
Training epoch 29 [21120/67107 (31.49%)]	 Loss: 0.000025
Training epoch 29 [21760/67107 (32.44%)]	 Loss: 0.000006
Training epoch 29 [22400/67107 (33.40%)]	 Loss: 0.000315
Training epoch 29 [23040/67107 (34.35%)]	 Loss: 0.000007
Training epoch 29 [23680/67107 (35.31%)]	 Loss: 0.000020
Training epoch 29 [24320/67107 (36.26%)]	 Loss: 0.000002
Training epoch 29 [24960/67107 (37.21%)]	 Loss: 0.000009
Training epoch 29 [25600/67107 (38.17%)]	 Loss: 0.000007
Training epoch 29 [26240/67107 (39.12%)]	 Loss: 0.000068
Training epoch 29 [26880/67107 (40.08%)]	 Loss: 0.000030
Training epoch 29 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 29 [28160/67107 (41.98%)]	 Loss: 0.000033
Training epoch 29 [28800/67107 (42.94%)]	 Loss: 0.000022
Training epoch 29 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 29 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 29 [30720/67107 (45.80%)]	 Loss: 0.000058
Training epoch 29 [31360/67107 (46.76%)]	 Loss: 0.000011
Training epoch 29 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 29 [32640/67107 (48.66%)]	 Loss: 0.000002
Training epoch 29 [33280/67107 (49.62%)]	 Loss: 0.000004
Training epoch 29 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 29 [34560/67107 (51.53%)]	 Loss: 0.000025
Training epoch 29 [35200/67107 (52.48%)]	 Loss: 0.000015
Training epoch 29 [35840/67107 (53.44%)]	 Loss: 0.000013
Training epoch 29 [36480/67107 (54.39%)]	 Loss: 0.000016
Training epoch 29 [37120/67107 (55.34%)]	 Loss: 0.000052
Training epoch 29 [37760/67107 (56.30%)]	 Loss: 0.000071
Training epoch 29 [38400/67107 (57.25%)]	 Loss: 0.000018
Training epoch 29 [39040/67107 (58.21%)]	 Loss: 0.000026
Training epoch 29 [39680/67107 (59.16%)]	 Loss: 0.000187
Training epoch 29 [40320/67107 (60.11%)]	 Loss: 0.000050
Training epoch 29 [40960/67107 (61.07%)]	 Loss: 0.000057
Training epoch 29 [41600/67107 (62.02%)]	 Loss: 0.000097
Training epoch 29 [42240/67107 (62.98%)]	 Loss: 0.000012
Training epoch 29 [42880/67107 (63.93%)]	 Loss: 0.000026
Training epoch 29 [43520/67107 (64.89%)]	 Loss: 0.000026
Training epoch 29 [44160/67107 (65.84%)]	 Loss: 0.000065
Training epoch 29 [44800/67107 (66.79%)]	 Loss: 0.000010
Training epoch 29 [45440/67107 (67.75%)]	 Loss: 0.000010
Training epoch 29 [46080/67107 (68.70%)]	 Loss: 0.000008
Training epoch 29 [46720/67107 (69.66%)]	 Loss: 0.000005
Training epoch 29 [47360/67107 (70.61%)]	 Loss: 0.000003
Training epoch 29 [48000/67107 (71.56%)]	 Loss: 0.000085
Training epoch 29 [48640/67107 (72.52%)]	 Loss: 0.000003
Training epoch 29 [49280/67107 (73.47%)]	 Loss: 0.000036
Training epoch 29 [49920/67107 (74.43%)]	 Loss: 0.000009
Training epoch 29 [50560/67107 (75.38%)]	 Loss: 0.000059
Training epoch 29 [51200/67107 (76.34%)]	 Loss: 0.000043
Training epoch 29 [51840/67107 (77.29%)]	 Loss: 0.000035
Training epoch 29 [52480/67107 (78.24%)]	 Loss: 0.000042
Training epoch 29 [53120/67107 (79.20%)]	 Loss: 0.000016
Training epoch 29 [53760/67107 (80.15%)]	 Loss: 0.000011
Training epoch 29 [54400/67107 (81.11%)]	 Loss: 0.000003
Training epoch 29 [55040/67107 (82.06%)]	 Loss: 0.000035
Training epoch 29 [55680/67107 (83.02%)]	 Loss: 0.000073
Training epoch 29 [56320/67107 (83.97%)]	 Loss: 0.000061
Training epoch 29 [56960/67107 (84.92%)]	 Loss: 0.000022
Training epoch 29 [57600/67107 (85.88%)]	 Loss: 0.000026
Training epoch 29 [58240/67107 (86.83%)]	 Loss: 0.000024
Training epoch 29 [58880/67107 (87.79%)]	 Loss: 0.000295
Training epoch 29 [59520/67107 (88.74%)]	 Loss: 0.000005
Training epoch 29 [60160/67107 (89.69%)]	 Loss: 0.000018
Training epoch 29 [60800/67107 (90.65%)]	 Loss: 0.000037
Training epoch 29 [61440/67107 (91.60%)]	 Loss: 0.000041
Training epoch 29 [62080/67107 (92.56%)]	 Loss: 0.000081
Training epoch 29 [62720/67107 (93.51%)]	 Loss: 0.000054
Training epoch 29 [63360/67107 (94.47%)]	 Loss: 0.000034
Training epoch 29 [64000/67107 (95.42%)]	 Loss: 0.000043
Training epoch 29 [64640/67107 (96.37%)]	 Loss: 0.000007
Training epoch 29 [65280/67107 (97.33%)]	 Loss: 0.000063
Training epoch 29 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 29 [66560/67107 (99.24%)]	 Loss: 0.000018
Test set: Average Loss: 0.000211
Training epoch 30 [0/67107 (0.00%)]	 Loss: 0.000002
Training epoch 30 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 30 [1280/67107 (1.91%)]	 Loss: 0.000012
Training epoch 30 [1920/67107 (2.86%)]	 Loss: 0.000012
Training epoch 30 [2560/67107 (3.82%)]	 Loss: 0.000032
Training epoch 30 [3200/67107 (4.77%)]	 Loss: 0.000020
Training epoch 30 [3840/67107 (5.73%)]	 Loss: 0.000020
Training epoch 30 [4480/67107 (6.68%)]	 Loss: 0.000044
Training epoch 30 [5120/67107 (7.63%)]	 Loss: 0.000036
Training epoch 30 [5760/67107 (8.59%)]	 Loss: 0.000027
Training epoch 30 [6400/67107 (9.54%)]	 Loss: 0.000053
Training epoch 30 [7040/67107 (10.50%)]	 Loss: 0.000013
Training epoch 30 [7680/67107 (11.45%)]	 Loss: 0.000021
Training epoch 30 [8320/67107 (12.40%)]	 Loss: 0.000149
Training epoch 30 [8960/67107 (13.36%)]	 Loss: 0.000041
Training epoch 30 [9600/67107 (14.31%)]	 Loss: 0.000023
Training epoch 30 [10240/67107 (15.27%)]	 Loss: 0.000009
Training epoch 30 [10880/67107 (16.22%)]	 Loss: 0.000025
Training epoch 30 [11520/67107 (17.18%)]	 Loss: 0.000026
Training epoch 30 [12160/67107 (18.13%)]	 Loss: 0.000013
Training epoch 30 [12800/67107 (19.08%)]	 Loss: 0.000009
Training epoch 30 [13440/67107 (20.04%)]	 Loss: 0.000014
Training epoch 30 [14080/67107 (20.99%)]	 Loss: 0.000055
Training epoch 30 [14720/67107 (21.95%)]	 Loss: 0.000077
Training epoch 30 [15360/67107 (22.90%)]	 Loss: 0.000016
Training epoch 30 [16000/67107 (23.85%)]	 Loss: 0.000019
Training epoch 30 [16640/67107 (24.81%)]	 Loss: 0.000030
Training epoch 30 [17280/67107 (25.76%)]	 Loss: 0.000013
Training epoch 30 [17920/67107 (26.72%)]	 Loss: 0.000005
Training epoch 30 [18560/67107 (27.67%)]	 Loss: 0.000029
Training epoch 30 [19200/67107 (28.63%)]	 Loss: 0.000015
Training epoch 30 [19840/67107 (29.58%)]	 Loss: 0.000009
Training epoch 30 [20480/67107 (30.53%)]	 Loss: 0.000045
Training epoch 30 [21120/67107 (31.49%)]	 Loss: 0.000037
Training epoch 30 [21760/67107 (32.44%)]	 Loss: 0.000016
Training epoch 30 [22400/67107 (33.40%)]	 Loss: 0.000097
Training epoch 30 [23040/67107 (34.35%)]	 Loss: 0.000008
Training epoch 30 [23680/67107 (35.31%)]	 Loss: 0.000025
Training epoch 30 [24320/67107 (36.26%)]	 Loss: 0.000007
Training epoch 30 [24960/67107 (37.21%)]	 Loss: 0.000006
Training epoch 30 [25600/67107 (38.17%)]	 Loss: 0.000001
Training epoch 30 [26240/67107 (39.12%)]	 Loss: 0.000052
Training epoch 30 [26880/67107 (40.08%)]	 Loss: 0.000004
Training epoch 30 [27520/67107 (41.03%)]	 Loss: 0.000028
Training epoch 30 [28160/67107 (41.98%)]	 Loss: 0.000004
Training epoch 30 [28800/67107 (42.94%)]	 Loss: 0.000069
Training epoch 30 [29440/67107 (43.89%)]	 Loss: 0.000016
Training epoch 30 [30080/67107 (44.85%)]	 Loss: 0.000018
Training epoch 30 [30720/67107 (45.80%)]	 Loss: 0.000056
Training epoch 30 [31360/67107 (46.76%)]	 Loss: 0.000014
Training epoch 30 [32000/67107 (47.71%)]	 Loss: 0.000016
Training epoch 30 [32640/67107 (48.66%)]	 Loss: 0.000012
Training epoch 30 [33280/67107 (49.62%)]	 Loss: 0.000005
Training epoch 30 [33920/67107 (50.57%)]	 Loss: 0.000049
Training epoch 30 [34560/67107 (51.53%)]	 Loss: 0.000059
Training epoch 30 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 30 [35840/67107 (53.44%)]	 Loss: 0.000022
Training epoch 30 [36480/67107 (54.39%)]	 Loss: 0.000007
Training epoch 30 [37120/67107 (55.34%)]	 Loss: 0.000179
Training epoch 30 [37760/67107 (56.30%)]	 Loss: 0.000040
Training epoch 30 [38400/67107 (57.25%)]	 Loss: 0.000008
Training epoch 30 [39040/67107 (58.21%)]	 Loss: 0.000022
Training epoch 30 [39680/67107 (59.16%)]	 Loss: 0.000027
Training epoch 30 [40320/67107 (60.11%)]	 Loss: 0.000033
Training epoch 30 [40960/67107 (61.07%)]	 Loss: 0.000040
Training epoch 30 [41600/67107 (62.02%)]	 Loss: 0.000024
Training epoch 30 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 30 [42880/67107 (63.93%)]	 Loss: 0.000007
Training epoch 30 [43520/67107 (64.89%)]	 Loss: 0.000012
Training epoch 30 [44160/67107 (65.84%)]	 Loss: 0.000007
Training epoch 30 [44800/67107 (66.79%)]	 Loss: 0.000004
Training epoch 30 [45440/67107 (67.75%)]	 Loss: 0.000012
Training epoch 30 [46080/67107 (68.70%)]	 Loss: 0.000008
Training epoch 30 [46720/67107 (69.66%)]	 Loss: 0.000020
Training epoch 30 [47360/67107 (70.61%)]	 Loss: 0.000018
Training epoch 30 [48000/67107 (71.56%)]	 Loss: 0.000078
Training epoch 30 [48640/67107 (72.52%)]	 Loss: 0.000030
Training epoch 30 [49280/67107 (73.47%)]	 Loss: 0.000013
Training epoch 30 [49920/67107 (74.43%)]	 Loss: 0.000077
Training epoch 30 [50560/67107 (75.38%)]	 Loss: 0.000053
Training epoch 30 [51200/67107 (76.34%)]	 Loss: 0.000011
Training epoch 30 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 30 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 30 [53120/67107 (79.20%)]	 Loss: 0.000013
Training epoch 30 [53760/67107 (80.15%)]	 Loss: 0.000027
Training epoch 30 [54400/67107 (81.11%)]	 Loss: 0.000012
Training epoch 30 [55040/67107 (82.06%)]	 Loss: 0.000008
Training epoch 30 [55680/67107 (83.02%)]	 Loss: 0.000007
Training epoch 30 [56320/67107 (83.97%)]	 Loss: 0.000038
Training epoch 30 [56960/67107 (84.92%)]	 Loss: 0.000035
Training epoch 30 [57600/67107 (85.88%)]	 Loss: 0.000073
Training epoch 30 [58240/67107 (86.83%)]	 Loss: 0.000026
Training epoch 30 [58880/67107 (87.79%)]	 Loss: 0.000029
Training epoch 30 [59520/67107 (88.74%)]	 Loss: 0.000016
Training epoch 30 [60160/67107 (89.69%)]	 Loss: 0.000034
Training epoch 30 [60800/67107 (90.65%)]	 Loss: 0.000012
Training epoch 30 [61440/67107 (91.60%)]	 Loss: 0.000009
Training epoch 30 [62080/67107 (92.56%)]	 Loss: 0.000020
Training epoch 30 [62720/67107 (93.51%)]	 Loss: 0.000023
Training epoch 30 [63360/67107 (94.47%)]	 Loss: 0.000035
Training epoch 30 [64000/67107 (95.42%)]	 Loss: 0.000060
Training epoch 30 [64640/67107 (96.37%)]	 Loss: 0.000028
Training epoch 30 [65280/67107 (97.33%)]	 Loss: 0.000015
Training epoch 30 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 30 [66560/67107 (99.24%)]	 Loss: 0.000005
Test set: Average Loss: 0.001600
Training epoch 31 [0/67107 (0.00%)]	 Loss: 0.000009
Training epoch 31 [640/67107 (0.95%)]	 Loss: 0.000020
Training epoch 31 [1280/67107 (1.91%)]	 Loss: 0.000099
Training epoch 31 [1920/67107 (2.86%)]	 Loss: 0.000015
Training epoch 31 [2560/67107 (3.82%)]	 Loss: 0.000037
Training epoch 31 [3200/67107 (4.77%)]	 Loss: 0.000019
Training epoch 31 [3840/67107 (5.73%)]	 Loss: 0.000011
Training epoch 31 [4480/67107 (6.68%)]	 Loss: 0.000013
Training epoch 31 [5120/67107 (7.63%)]	 Loss: 0.000022
Training epoch 31 [5760/67107 (8.59%)]	 Loss: 0.000002
Training epoch 31 [6400/67107 (9.54%)]	 Loss: 0.000002
Training epoch 31 [7040/67107 (10.50%)]	 Loss: 0.000036
Training epoch 31 [7680/67107 (11.45%)]	 Loss: 0.000043
Training epoch 31 [8320/67107 (12.40%)]	 Loss: 0.000014
Training epoch 31 [8960/67107 (13.36%)]	 Loss: 0.000007
Training epoch 31 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 31 [10240/67107 (15.27%)]	 Loss: 0.000027
Training epoch 31 [10880/67107 (16.22%)]	 Loss: 0.000015
Training epoch 31 [11520/67107 (17.18%)]	 Loss: 0.000007
Training epoch 31 [12160/67107 (18.13%)]	 Loss: 0.000008
Training epoch 31 [12800/67107 (19.08%)]	 Loss: 0.000002
Training epoch 31 [13440/67107 (20.04%)]	 Loss: 0.000018
Training epoch 31 [14080/67107 (20.99%)]	 Loss: 0.000003
Training epoch 31 [14720/67107 (21.95%)]	 Loss: 0.000024
Training epoch 31 [15360/67107 (22.90%)]	 Loss: 0.000003
Training epoch 31 [16000/67107 (23.85%)]	 Loss: 0.000045
Training epoch 31 [16640/67107 (24.81%)]	 Loss: 0.000008
Training epoch 31 [17280/67107 (25.76%)]	 Loss: 0.000010
Training epoch 31 [17920/67107 (26.72%)]	 Loss: 0.000033
Training epoch 31 [18560/67107 (27.67%)]	 Loss: 0.000018
Training epoch 31 [19200/67107 (28.63%)]	 Loss: 0.000028
Training epoch 31 [19840/67107 (29.58%)]	 Loss: 0.000012
Training epoch 31 [20480/67107 (30.53%)]	 Loss: 0.000017
Training epoch 31 [21120/67107 (31.49%)]	 Loss: 0.000003
Training epoch 31 [21760/67107 (32.44%)]	 Loss: 0.000013
Training epoch 31 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 31 [23040/67107 (34.35%)]	 Loss: 0.000003
Training epoch 31 [23680/67107 (35.31%)]	 Loss: 0.000010
Training epoch 31 [24320/67107 (36.26%)]	 Loss: 0.000067
Training epoch 31 [24960/67107 (37.21%)]	 Loss: 0.000007
Training epoch 31 [25600/67107 (38.17%)]	 Loss: 0.000012
Training epoch 31 [26240/67107 (39.12%)]	 Loss: 0.000031
Training epoch 31 [26880/67107 (40.08%)]	 Loss: 0.000006
Training epoch 31 [27520/67107 (41.03%)]	 Loss: 0.000105
Training epoch 31 [28160/67107 (41.98%)]	 Loss: 0.000026
Training epoch 31 [28800/67107 (42.94%)]	 Loss: 0.000005
Training epoch 31 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 31 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 31 [30720/67107 (45.80%)]	 Loss: 0.000009
Training epoch 31 [31360/67107 (46.76%)]	 Loss: 0.000008
Training epoch 31 [32000/67107 (47.71%)]	 Loss: 0.000008
Training epoch 31 [32640/67107 (48.66%)]	 Loss: 0.000014
Training epoch 31 [33280/67107 (49.62%)]	 Loss: 0.000006
Training epoch 31 [33920/67107 (50.57%)]	 Loss: 0.000021
Training epoch 31 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 31 [35200/67107 (52.48%)]	 Loss: 0.000022
Training epoch 31 [35840/67107 (53.44%)]	 Loss: 0.000032
Training epoch 31 [36480/67107 (54.39%)]	 Loss: 0.000039
Training epoch 31 [37120/67107 (55.34%)]	 Loss: 0.000005
Training epoch 31 [37760/67107 (56.30%)]	 Loss: 0.000016
Training epoch 31 [38400/67107 (57.25%)]	 Loss: 0.000018
Training epoch 31 [39040/67107 (58.21%)]	 Loss: 0.000017
Training epoch 31 [39680/67107 (59.16%)]	 Loss: 0.000023
Training epoch 31 [40320/67107 (60.11%)]	 Loss: 0.000024
Training epoch 31 [40960/67107 (61.07%)]	 Loss: 0.000015
Training epoch 31 [41600/67107 (62.02%)]	 Loss: 0.000031
Training epoch 31 [42240/67107 (62.98%)]	 Loss: 0.000027
Training epoch 31 [42880/67107 (63.93%)]	 Loss: 0.000013
Training epoch 31 [43520/67107 (64.89%)]	 Loss: 0.000020
Training epoch 31 [44160/67107 (65.84%)]	 Loss: 0.000023
Training epoch 31 [44800/67107 (66.79%)]	 Loss: 0.000010
Training epoch 31 [45440/67107 (67.75%)]	 Loss: 0.000006
Training epoch 31 [46080/67107 (68.70%)]	 Loss: 0.000018
Training epoch 31 [46720/67107 (69.66%)]	 Loss: 0.000037
Training epoch 31 [47360/67107 (70.61%)]	 Loss: 0.000013
Training epoch 31 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 31 [48640/67107 (72.52%)]	 Loss: 0.000013
Training epoch 31 [49280/67107 (73.47%)]	 Loss: 0.000053
Training epoch 31 [49920/67107 (74.43%)]	 Loss: 0.000037
Training epoch 31 [50560/67107 (75.38%)]	 Loss: 0.000019
Training epoch 31 [51200/67107 (76.34%)]	 Loss: 0.000010
Training epoch 31 [51840/67107 (77.29%)]	 Loss: 0.000020
Training epoch 31 [52480/67107 (78.24%)]	 Loss: 0.000011
Training epoch 31 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 31 [53760/67107 (80.15%)]	 Loss: 0.000009
Training epoch 31 [54400/67107 (81.11%)]	 Loss: 0.000016
Training epoch 31 [55040/67107 (82.06%)]	 Loss: 0.000019
Training epoch 31 [55680/67107 (83.02%)]	 Loss: 0.000008
Training epoch 31 [56320/67107 (83.97%)]	 Loss: 0.000009
Training epoch 31 [56960/67107 (84.92%)]	 Loss: 0.000026
Training epoch 31 [57600/67107 (85.88%)]	 Loss: 0.000044
Training epoch 31 [58240/67107 (86.83%)]	 Loss: 0.000007
Training epoch 31 [58880/67107 (87.79%)]	 Loss: 0.000012
Training epoch 31 [59520/67107 (88.74%)]	 Loss: 0.000010
Training epoch 31 [60160/67107 (89.69%)]	 Loss: 0.000136
Training epoch 31 [60800/67107 (90.65%)]	 Loss: 0.000025
Training epoch 31 [61440/67107 (91.60%)]	 Loss: 0.000023
Training epoch 31 [62080/67107 (92.56%)]	 Loss: 0.000107
Training epoch 31 [62720/67107 (93.51%)]	 Loss: 0.000061
Training epoch 31 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 31 [64000/67107 (95.42%)]	 Loss: 0.000053
Training epoch 31 [64640/67107 (96.37%)]	 Loss: 0.000008
Training epoch 31 [65280/67107 (97.33%)]	 Loss: 0.000029
Training epoch 31 [65920/67107 (98.28%)]	 Loss: 0.000067
Training epoch 31 [66560/67107 (99.24%)]	 Loss: 0.000004
Test set: Average Loss: 0.002190
Training epoch 32 [0/67107 (0.00%)]	 Loss: 0.000007
Training epoch 32 [640/67107 (0.95%)]	 Loss: 0.000001
Training epoch 32 [1280/67107 (1.91%)]	 Loss: 0.000067
Training epoch 32 [1920/67107 (2.86%)]	 Loss: 0.000008
Training epoch 32 [2560/67107 (3.82%)]	 Loss: 0.000032
Training epoch 32 [3200/67107 (4.77%)]	 Loss: 0.000024
Training epoch 32 [3840/67107 (5.73%)]	 Loss: 0.000014
Training epoch 32 [4480/67107 (6.68%)]	 Loss: 0.000023
Training epoch 32 [5120/67107 (7.63%)]	 Loss: 0.000054
Training epoch 32 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 32 [6400/67107 (9.54%)]	 Loss: 0.000004
Training epoch 32 [7040/67107 (10.50%)]	 Loss: 0.000032
Training epoch 32 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 32 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 32 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 32 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 32 [10240/67107 (15.27%)]	 Loss: 0.000005
Training epoch 32 [10880/67107 (16.22%)]	 Loss: 0.000012
Training epoch 32 [11520/67107 (17.18%)]	 Loss: 0.000047
Training epoch 32 [12160/67107 (18.13%)]	 Loss: 0.000006
Training epoch 32 [12800/67107 (19.08%)]	 Loss: 0.000017
Training epoch 32 [13440/67107 (20.04%)]	 Loss: 0.000025
Training epoch 32 [14080/67107 (20.99%)]	 Loss: 0.000005
Training epoch 32 [14720/67107 (21.95%)]	 Loss: 0.000001
Training epoch 32 [15360/67107 (22.90%)]	 Loss: 0.000028
Training epoch 32 [16000/67107 (23.85%)]	 Loss: 0.000013
Training epoch 32 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 32 [17280/67107 (25.76%)]	 Loss: 0.000026
Training epoch 32 [17920/67107 (26.72%)]	 Loss: 0.000051
Training epoch 32 [18560/67107 (27.67%)]	 Loss: 0.000028
Training epoch 32 [19200/67107 (28.63%)]	 Loss: 0.000041
Training epoch 32 [19840/67107 (29.58%)]	 Loss: 0.000019
Training epoch 32 [20480/67107 (30.53%)]	 Loss: 0.000004
Training epoch 32 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 32 [21760/67107 (32.44%)]	 Loss: 0.000006
Training epoch 32 [22400/67107 (33.40%)]	 Loss: 0.000003
Training epoch 32 [23040/67107 (34.35%)]	 Loss: 0.000010
Training epoch 32 [23680/67107 (35.31%)]	 Loss: 0.000010
Training epoch 32 [24320/67107 (36.26%)]	 Loss: 0.000003
Training epoch 32 [24960/67107 (37.21%)]	 Loss: 0.000023
Training epoch 32 [25600/67107 (38.17%)]	 Loss: 0.000042
Training epoch 32 [26240/67107 (39.12%)]	 Loss: 0.000008
Training epoch 32 [26880/67107 (40.08%)]	 Loss: 0.000014
Training epoch 32 [27520/67107 (41.03%)]	 Loss: 0.000006
Training epoch 32 [28160/67107 (41.98%)]	 Loss: 0.000023
Training epoch 32 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 32 [29440/67107 (43.89%)]	 Loss: 0.000128
Training epoch 32 [30080/67107 (44.85%)]	 Loss: 0.000051
Training epoch 32 [30720/67107 (45.80%)]	 Loss: 0.000026
Training epoch 32 [31360/67107 (46.76%)]	 Loss: 0.000017
Training epoch 32 [32000/67107 (47.71%)]	 Loss: 0.000017
Training epoch 32 [32640/67107 (48.66%)]	 Loss: 0.000010
Training epoch 32 [33280/67107 (49.62%)]	 Loss: 0.000005
Training epoch 32 [33920/67107 (50.57%)]	 Loss: 0.000008
Training epoch 32 [34560/67107 (51.53%)]	 Loss: 0.000003
Training epoch 32 [35200/67107 (52.48%)]	 Loss: 0.000008
Training epoch 32 [35840/67107 (53.44%)]	 Loss: 0.000008
Training epoch 32 [36480/67107 (54.39%)]	 Loss: 0.000008
Training epoch 32 [37120/67107 (55.34%)]	 Loss: 0.000032
Training epoch 32 [37760/67107 (56.30%)]	 Loss: 0.000020
Training epoch 32 [38400/67107 (57.25%)]	 Loss: 0.000013
Training epoch 32 [39040/67107 (58.21%)]	 Loss: 0.000010
Training epoch 32 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 32 [40320/67107 (60.11%)]	 Loss: 0.000014
Training epoch 32 [40960/67107 (61.07%)]	 Loss: 0.000017
Training epoch 32 [41600/67107 (62.02%)]	 Loss: 0.000011
Training epoch 32 [42240/67107 (62.98%)]	 Loss: 0.000119
Training epoch 32 [42880/67107 (63.93%)]	 Loss: 0.000065
Training epoch 32 [43520/67107 (64.89%)]	 Loss: 0.000074
Training epoch 32 [44160/67107 (65.84%)]	 Loss: 0.000022
Training epoch 32 [44800/67107 (66.79%)]	 Loss: 0.000027
Training epoch 32 [45440/67107 (67.75%)]	 Loss: 0.000008
Training epoch 32 [46080/67107 (68.70%)]	 Loss: 0.000003
Training epoch 32 [46720/67107 (69.66%)]	 Loss: 0.000007
Training epoch 32 [47360/67107 (70.61%)]	 Loss: 0.000038
Training epoch 32 [48000/67107 (71.56%)]	 Loss: 0.000110
Training epoch 32 [48640/67107 (72.52%)]	 Loss: 0.000019
Training epoch 32 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 32 [49920/67107 (74.43%)]	 Loss: 0.000050
Training epoch 32 [50560/67107 (75.38%)]	 Loss: 0.000045
Training epoch 32 [51200/67107 (76.34%)]	 Loss: 0.000045
Training epoch 32 [51840/67107 (77.29%)]	 Loss: 0.000008
Training epoch 32 [52480/67107 (78.24%)]	 Loss: 0.000014
Training epoch 32 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 32 [53760/67107 (80.15%)]	 Loss: 0.000019
Training epoch 32 [54400/67107 (81.11%)]	 Loss: 0.000006
Training epoch 32 [55040/67107 (82.06%)]	 Loss: 0.000021
Training epoch 32 [55680/67107 (83.02%)]	 Loss: 0.000048
Training epoch 32 [56320/67107 (83.97%)]	 Loss: 0.000033
Training epoch 32 [56960/67107 (84.92%)]	 Loss: 0.000008
Training epoch 32 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 32 [58240/67107 (86.83%)]	 Loss: 0.000032
Training epoch 32 [58880/67107 (87.79%)]	 Loss: 0.000027
Training epoch 32 [59520/67107 (88.74%)]	 Loss: 0.000009
Training epoch 32 [60160/67107 (89.69%)]	 Loss: 0.000011
Training epoch 32 [60800/67107 (90.65%)]	 Loss: 0.000019
Training epoch 32 [61440/67107 (91.60%)]	 Loss: 0.000080
Training epoch 32 [62080/67107 (92.56%)]	 Loss: 0.000017
Training epoch 32 [62720/67107 (93.51%)]	 Loss: 0.000022
Training epoch 32 [63360/67107 (94.47%)]	 Loss: 0.000049
Training epoch 32 [64000/67107 (95.42%)]	 Loss: 0.000034
Training epoch 32 [64640/67107 (96.37%)]	 Loss: 0.000025
Training epoch 32 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 32 [65920/67107 (98.28%)]	 Loss: 0.000007
Training epoch 32 [66560/67107 (99.24%)]	 Loss: 0.000006
Test set: Average Loss: 0.000173
Training epoch 33 [0/67107 (0.00%)]	 Loss: 0.000007
Training epoch 33 [640/67107 (0.95%)]	 Loss: 0.000089
Training epoch 33 [1280/67107 (1.91%)]	 Loss: 0.000012
Training epoch 33 [1920/67107 (2.86%)]	 Loss: 0.000189
Training epoch 33 [2560/67107 (3.82%)]	 Loss: 0.000025
Training epoch 33 [3200/67107 (4.77%)]	 Loss: 0.000016
Training epoch 33 [3840/67107 (5.73%)]	 Loss: 0.000033
Training epoch 33 [4480/67107 (6.68%)]	 Loss: 0.000027
Training epoch 33 [5120/67107 (7.63%)]	 Loss: 0.000010
Training epoch 33 [5760/67107 (8.59%)]	 Loss: 0.000110
Training epoch 33 [6400/67107 (9.54%)]	 Loss: 0.000025
Training epoch 33 [7040/67107 (10.50%)]	 Loss: 0.000135
Training epoch 33 [7680/67107 (11.45%)]	 Loss: 0.000056
Training epoch 33 [8320/67107 (12.40%)]	 Loss: 0.000139
Training epoch 33 [8960/67107 (13.36%)]	 Loss: 0.000022
Training epoch 33 [9600/67107 (14.31%)]	 Loss: 0.000018
Training epoch 33 [10240/67107 (15.27%)]	 Loss: 0.000012
Training epoch 33 [10880/67107 (16.22%)]	 Loss: 0.000006
Training epoch 33 [11520/67107 (17.18%)]	 Loss: 0.000004
Training epoch 33 [12160/67107 (18.13%)]	 Loss: 0.000002
Training epoch 33 [12800/67107 (19.08%)]	 Loss: 0.000019
Training epoch 33 [13440/67107 (20.04%)]	 Loss: 0.000021
Training epoch 33 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 33 [14720/67107 (21.95%)]	 Loss: 0.000033
Training epoch 33 [15360/67107 (22.90%)]	 Loss: 0.000016
Training epoch 33 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 33 [16640/67107 (24.81%)]	 Loss: 0.000035
Training epoch 33 [17280/67107 (25.76%)]	 Loss: 0.000026
Training epoch 33 [17920/67107 (26.72%)]	 Loss: 0.000012
Training epoch 33 [18560/67107 (27.67%)]	 Loss: 0.000020
Training epoch 33 [19200/67107 (28.63%)]	 Loss: 0.000022
Training epoch 33 [19840/67107 (29.58%)]	 Loss: 0.000013
Training epoch 33 [20480/67107 (30.53%)]	 Loss: 0.000004
Training epoch 33 [21120/67107 (31.49%)]	 Loss: 0.000076
Training epoch 33 [21760/67107 (32.44%)]	 Loss: 0.000013
Training epoch 33 [22400/67107 (33.40%)]	 Loss: 0.000033
Training epoch 33 [23040/67107 (34.35%)]	 Loss: 0.000034
Training epoch 33 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 33 [24320/67107 (36.26%)]	 Loss: 0.000009
Training epoch 33 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 33 [25600/67107 (38.17%)]	 Loss: 0.000030
Training epoch 33 [26240/67107 (39.12%)]	 Loss: 0.000015
Training epoch 33 [26880/67107 (40.08%)]	 Loss: 0.000004
Training epoch 33 [27520/67107 (41.03%)]	 Loss: 0.000036
Training epoch 33 [28160/67107 (41.98%)]	 Loss: 0.000038
Training epoch 33 [28800/67107 (42.94%)]	 Loss: 0.000009
Training epoch 33 [29440/67107 (43.89%)]	 Loss: 0.000028
Training epoch 33 [30080/67107 (44.85%)]	 Loss: 0.000014
Training epoch 33 [30720/67107 (45.80%)]	 Loss: 0.000015
Training epoch 33 [31360/67107 (46.76%)]	 Loss: 0.000007
Training epoch 33 [32000/67107 (47.71%)]	 Loss: 0.000106
Training epoch 33 [32640/67107 (48.66%)]	 Loss: 0.000004
Training epoch 33 [33280/67107 (49.62%)]	 Loss: 0.000003
Training epoch 33 [33920/67107 (50.57%)]	 Loss: 0.000003
Training epoch 33 [34560/67107 (51.53%)]	 Loss: 0.000003
Training epoch 33 [35200/67107 (52.48%)]	 Loss: 0.000010
Training epoch 33 [35840/67107 (53.44%)]	 Loss: 0.000007
Training epoch 33 [36480/67107 (54.39%)]	 Loss: 0.000053
Training epoch 33 [37120/67107 (55.34%)]	 Loss: 0.000003
Training epoch 33 [37760/67107 (56.30%)]	 Loss: 0.000032
Training epoch 33 [38400/67107 (57.25%)]	 Loss: 0.000037
Training epoch 33 [39040/67107 (58.21%)]	 Loss: 0.000013
Training epoch 33 [39680/67107 (59.16%)]	 Loss: 0.000010
Training epoch 33 [40320/67107 (60.11%)]	 Loss: 0.000077
Training epoch 33 [40960/67107 (61.07%)]	 Loss: 0.000007
Training epoch 33 [41600/67107 (62.02%)]	 Loss: 0.000002
Training epoch 33 [42240/67107 (62.98%)]	 Loss: 0.000041
Training epoch 33 [42880/67107 (63.93%)]	 Loss: 0.000036
Training epoch 33 [43520/67107 (64.89%)]	 Loss: 0.000039
Training epoch 33 [44160/67107 (65.84%)]	 Loss: 0.000015
Training epoch 33 [44800/67107 (66.79%)]	 Loss: 0.000062
Training epoch 33 [45440/67107 (67.75%)]	 Loss: 0.000011
Training epoch 33 [46080/67107 (68.70%)]	 Loss: 0.000006
Training epoch 33 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 33 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 33 [48000/67107 (71.56%)]	 Loss: 0.000006
Training epoch 33 [48640/67107 (72.52%)]	 Loss: 0.000005
Training epoch 33 [49280/67107 (73.47%)]	 Loss: 0.000046
Training epoch 33 [49920/67107 (74.43%)]	 Loss: 0.000006
Training epoch 33 [50560/67107 (75.38%)]	 Loss: 0.000077
Training epoch 33 [51200/67107 (76.34%)]	 Loss: 0.000032
Training epoch 33 [51840/67107 (77.29%)]	 Loss: 0.000015
Training epoch 33 [52480/67107 (78.24%)]	 Loss: 0.000081
Training epoch 33 [53120/67107 (79.20%)]	 Loss: 0.000034
Training epoch 33 [53760/67107 (80.15%)]	 Loss: 0.000048
Training epoch 33 [54400/67107 (81.11%)]	 Loss: 0.000013
Training epoch 33 [55040/67107 (82.06%)]	 Loss: 0.000007
Training epoch 33 [55680/67107 (83.02%)]	 Loss: 0.000022
Training epoch 33 [56320/67107 (83.97%)]	 Loss: 0.000016
Training epoch 33 [56960/67107 (84.92%)]	 Loss: 0.000101
Training epoch 33 [57600/67107 (85.88%)]	 Loss: 0.000008
Training epoch 33 [58240/67107 (86.83%)]	 Loss: 0.000009
Training epoch 33 [58880/67107 (87.79%)]	 Loss: 0.000012
Training epoch 33 [59520/67107 (88.74%)]	 Loss: 0.000008
Training epoch 33 [60160/67107 (89.69%)]	 Loss: 0.000017
Training epoch 33 [60800/67107 (90.65%)]	 Loss: 0.000044
Training epoch 33 [61440/67107 (91.60%)]	 Loss: 0.000011
Training epoch 33 [62080/67107 (92.56%)]	 Loss: 0.000040
Training epoch 33 [62720/67107 (93.51%)]	 Loss: 0.000012
Training epoch 33 [63360/67107 (94.47%)]	 Loss: 0.000060
Training epoch 33 [64000/67107 (95.42%)]	 Loss: 0.000126
Training epoch 33 [64640/67107 (96.37%)]	 Loss: 0.000066
Training epoch 33 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 33 [65920/67107 (98.28%)]	 Loss: 0.000008
Training epoch 33 [66560/67107 (99.24%)]	 Loss: 0.000001
Test set: Average Loss: 0.000203
Training epoch 34 [0/67107 (0.00%)]	 Loss: 0.000005
Training epoch 34 [640/67107 (0.95%)]	 Loss: 0.000006
Training epoch 34 [1280/67107 (1.91%)]	 Loss: 0.000007
Training epoch 34 [1920/67107 (2.86%)]	 Loss: 0.000007
Training epoch 34 [2560/67107 (3.82%)]	 Loss: 0.000046
Training epoch 34 [3200/67107 (4.77%)]	 Loss: 0.000025
Training epoch 34 [3840/67107 (5.73%)]	 Loss: 0.000092
Training epoch 34 [4480/67107 (6.68%)]	 Loss: 0.000008
Training epoch 34 [5120/67107 (7.63%)]	 Loss: 0.000009
Training epoch 34 [5760/67107 (8.59%)]	 Loss: 0.000013
Training epoch 34 [6400/67107 (9.54%)]	 Loss: 0.000018
Training epoch 34 [7040/67107 (10.50%)]	 Loss: 0.000011
Training epoch 34 [7680/67107 (11.45%)]	 Loss: 0.000084
Training epoch 34 [8320/67107 (12.40%)]	 Loss: 0.000016
Training epoch 34 [8960/67107 (13.36%)]	 Loss: 0.000028
Training epoch 34 [9600/67107 (14.31%)]	 Loss: 0.000033
Training epoch 34 [10240/67107 (15.27%)]	 Loss: 0.000030
Training epoch 34 [10880/67107 (16.22%)]	 Loss: 0.000019
Training epoch 34 [11520/67107 (17.18%)]	 Loss: 0.000056
Training epoch 34 [12160/67107 (18.13%)]	 Loss: 0.000008
Training epoch 34 [12800/67107 (19.08%)]	 Loss: 0.000016
Training epoch 34 [13440/67107 (20.04%)]	 Loss: 0.000006
Training epoch 34 [14080/67107 (20.99%)]	 Loss: 0.000009
Training epoch 34 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 34 [15360/67107 (22.90%)]	 Loss: 0.000012
Training epoch 34 [16000/67107 (23.85%)]	 Loss: 0.000021
Training epoch 34 [16640/67107 (24.81%)]	 Loss: 0.000003
Training epoch 34 [17280/67107 (25.76%)]	 Loss: 0.000055
Training epoch 34 [17920/67107 (26.72%)]	 Loss: 0.000024
Training epoch 34 [18560/67107 (27.67%)]	 Loss: 0.000037
Training epoch 34 [19200/67107 (28.63%)]	 Loss: 0.000055
Training epoch 34 [19840/67107 (29.58%)]	 Loss: 0.000014
Training epoch 34 [20480/67107 (30.53%)]	 Loss: 0.000026
Training epoch 34 [21120/67107 (31.49%)]	 Loss: 0.000012
Training epoch 34 [21760/67107 (32.44%)]	 Loss: 0.000003
Training epoch 34 [22400/67107 (33.40%)]	 Loss: 0.000051
Training epoch 34 [23040/67107 (34.35%)]	 Loss: 0.000100
Training epoch 34 [23680/67107 (35.31%)]	 Loss: 0.000009
Training epoch 34 [24320/67107 (36.26%)]	 Loss: 0.000018
Training epoch 34 [24960/67107 (37.21%)]	 Loss: 0.000011
Training epoch 34 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 34 [26240/67107 (39.12%)]	 Loss: 0.000029
Training epoch 34 [26880/67107 (40.08%)]	 Loss: 0.000027
Training epoch 34 [27520/67107 (41.03%)]	 Loss: 0.000004
Training epoch 34 [28160/67107 (41.98%)]	 Loss: 0.000010
Training epoch 34 [28800/67107 (42.94%)]	 Loss: 0.000009
Training epoch 34 [29440/67107 (43.89%)]	 Loss: 0.000004
Training epoch 34 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 34 [30720/67107 (45.80%)]	 Loss: 0.000029
Training epoch 34 [31360/67107 (46.76%)]	 Loss: 0.000006
Training epoch 34 [32000/67107 (47.71%)]	 Loss: 0.000045
Training epoch 34 [32640/67107 (48.66%)]	 Loss: 0.000013
Training epoch 34 [33280/67107 (49.62%)]	 Loss: 0.000025
Training epoch 34 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 34 [34560/67107 (51.53%)]	 Loss: 0.000009
Training epoch 34 [35200/67107 (52.48%)]	 Loss: 0.000016
Training epoch 34 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 34 [36480/67107 (54.39%)]	 Loss: 0.000004
Training epoch 34 [37120/67107 (55.34%)]	 Loss: 0.000001
Training epoch 34 [37760/67107 (56.30%)]	 Loss: 0.000013
Training epoch 34 [38400/67107 (57.25%)]	 Loss: 0.000024
Training epoch 34 [39040/67107 (58.21%)]	 Loss: 0.000014
Training epoch 34 [39680/67107 (59.16%)]	 Loss: 0.000040
Training epoch 34 [40320/67107 (60.11%)]	 Loss: 0.000010
Training epoch 34 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 34 [41600/67107 (62.02%)]	 Loss: 0.000032
Training epoch 34 [42240/67107 (62.98%)]	 Loss: 0.000024
Training epoch 34 [42880/67107 (63.93%)]	 Loss: 0.000013
Training epoch 34 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 34 [44160/67107 (65.84%)]	 Loss: 0.000041
Training epoch 34 [44800/67107 (66.79%)]	 Loss: 0.000013
Training epoch 34 [45440/67107 (67.75%)]	 Loss: 0.000032
Training epoch 34 [46080/67107 (68.70%)]	 Loss: 0.000051
Training epoch 34 [46720/67107 (69.66%)]	 Loss: 0.000009
Training epoch 34 [47360/67107 (70.61%)]	 Loss: 0.000011
Training epoch 34 [48000/67107 (71.56%)]	 Loss: 0.000007
Training epoch 34 [48640/67107 (72.52%)]	 Loss: 0.000022
Training epoch 34 [49280/67107 (73.47%)]	 Loss: 0.000031
Training epoch 34 [49920/67107 (74.43%)]	 Loss: 0.000014
Training epoch 34 [50560/67107 (75.38%)]	 Loss: 0.000013
Training epoch 34 [51200/67107 (76.34%)]	 Loss: 0.000029
Training epoch 34 [51840/67107 (77.29%)]	 Loss: 0.000009
Training epoch 34 [52480/67107 (78.24%)]	 Loss: 0.000004
Training epoch 34 [53120/67107 (79.20%)]	 Loss: 0.000014
Training epoch 34 [53760/67107 (80.15%)]	 Loss: 0.000005
Training epoch 34 [54400/67107 (81.11%)]	 Loss: 0.000017
Training epoch 34 [55040/67107 (82.06%)]	 Loss: 0.000037
Training epoch 34 [55680/67107 (83.02%)]	 Loss: 0.000007
Training epoch 34 [56320/67107 (83.97%)]	 Loss: 0.000014
Training epoch 34 [56960/67107 (84.92%)]	 Loss: 0.000061
Training epoch 34 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 34 [58240/67107 (86.83%)]	 Loss: 0.000022
Training epoch 34 [58880/67107 (87.79%)]	 Loss: 0.000027
Training epoch 34 [59520/67107 (88.74%)]	 Loss: 0.000021
Training epoch 34 [60160/67107 (89.69%)]	 Loss: 0.000006
Training epoch 34 [60800/67107 (90.65%)]	 Loss: 0.000006
Training epoch 34 [61440/67107 (91.60%)]	 Loss: 0.000005
Training epoch 34 [62080/67107 (92.56%)]	 Loss: 0.000006
Training epoch 34 [62720/67107 (93.51%)]	 Loss: 0.000012
Training epoch 34 [63360/67107 (94.47%)]	 Loss: 0.000014
Training epoch 34 [64000/67107 (95.42%)]	 Loss: 0.000030
Training epoch 34 [64640/67107 (96.37%)]	 Loss: 0.000024
Training epoch 34 [65280/67107 (97.33%)]	 Loss: 0.000016
Training epoch 34 [65920/67107 (98.28%)]	 Loss: 0.000011
Training epoch 34 [66560/67107 (99.24%)]	 Loss: 0.000012
Test set: Average Loss: 0.000308
Training epoch 35 [0/67107 (0.00%)]	 Loss: 0.000009
Training epoch 35 [640/67107 (0.95%)]	 Loss: 0.000013
Training epoch 35 [1280/67107 (1.91%)]	 Loss: 0.000004
Training epoch 35 [1920/67107 (2.86%)]	 Loss: 0.000010
Training epoch 35 [2560/67107 (3.82%)]	 Loss: 0.000002
Training epoch 35 [3200/67107 (4.77%)]	 Loss: 0.000002
Training epoch 35 [3840/67107 (5.73%)]	 Loss: 0.000002
Training epoch 35 [4480/67107 (6.68%)]	 Loss: 0.000040
Training epoch 35 [5120/67107 (7.63%)]	 Loss: 0.000112
Training epoch 35 [5760/67107 (8.59%)]	 Loss: 0.000012
Training epoch 35 [6400/67107 (9.54%)]	 Loss: 0.000001
Training epoch 35 [7040/67107 (10.50%)]	 Loss: 0.000041
Training epoch 35 [7680/67107 (11.45%)]	 Loss: 0.000042
Training epoch 35 [8320/67107 (12.40%)]	 Loss: 0.000043
Training epoch 35 [8960/67107 (13.36%)]	 Loss: 0.000004
Training epoch 35 [9600/67107 (14.31%)]	 Loss: 0.000017
Training epoch 35 [10240/67107 (15.27%)]	 Loss: 0.000003
Training epoch 35 [10880/67107 (16.22%)]	 Loss: 0.000022
Training epoch 35 [11520/67107 (17.18%)]	 Loss: 0.000009
Training epoch 35 [12160/67107 (18.13%)]	 Loss: 0.000045
Training epoch 35 [12800/67107 (19.08%)]	 Loss: 0.000016
Training epoch 35 [13440/67107 (20.04%)]	 Loss: 0.000040
Training epoch 35 [14080/67107 (20.99%)]	 Loss: 0.000026
Training epoch 35 [14720/67107 (21.95%)]	 Loss: 0.000008
Training epoch 35 [15360/67107 (22.90%)]	 Loss: 0.000050
Training epoch 35 [16000/67107 (23.85%)]	 Loss: 0.000022
Training epoch 35 [16640/67107 (24.81%)]	 Loss: 0.000009
Training epoch 35 [17280/67107 (25.76%)]	 Loss: 0.000017
Training epoch 35 [17920/67107 (26.72%)]	 Loss: 0.000018
Training epoch 35 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 35 [19200/67107 (28.63%)]	 Loss: 0.000031
Training epoch 35 [19840/67107 (29.58%)]	 Loss: 0.000028
Training epoch 35 [20480/67107 (30.53%)]	 Loss: 0.000053
Training epoch 35 [21120/67107 (31.49%)]	 Loss: 0.000018
Training epoch 35 [21760/67107 (32.44%)]	 Loss: 0.000010
Training epoch 35 [22400/67107 (33.40%)]	 Loss: 0.000018
Training epoch 35 [23040/67107 (34.35%)]	 Loss: 0.000095
Training epoch 35 [23680/67107 (35.31%)]	 Loss: 0.000005
Training epoch 35 [24320/67107 (36.26%)]	 Loss: 0.000014
Training epoch 35 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 35 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 35 [26240/67107 (39.12%)]	 Loss: 0.000008
Training epoch 35 [26880/67107 (40.08%)]	 Loss: 0.000070
Training epoch 35 [27520/67107 (41.03%)]	 Loss: 0.000067
Training epoch 35 [28160/67107 (41.98%)]	 Loss: 0.000009
Training epoch 35 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 35 [29440/67107 (43.89%)]	 Loss: 0.000010
Training epoch 35 [30080/67107 (44.85%)]	 Loss: 0.000008
Training epoch 35 [30720/67107 (45.80%)]	 Loss: 0.000041
Training epoch 35 [31360/67107 (46.76%)]	 Loss: 0.000022
Training epoch 35 [32000/67107 (47.71%)]	 Loss: 0.000008
Training epoch 35 [32640/67107 (48.66%)]	 Loss: 0.000007
Training epoch 35 [33280/67107 (49.62%)]	 Loss: 0.000004
Training epoch 35 [33920/67107 (50.57%)]	 Loss: 0.000007
Training epoch 35 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 35 [35200/67107 (52.48%)]	 Loss: 0.000083
Training epoch 35 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 35 [36480/67107 (54.39%)]	 Loss: 0.000036
Training epoch 35 [37120/67107 (55.34%)]	 Loss: 0.000006
Training epoch 35 [37760/67107 (56.30%)]	 Loss: 0.000082
Training epoch 35 [38400/67107 (57.25%)]	 Loss: 0.000008
Training epoch 35 [39040/67107 (58.21%)]	 Loss: 0.000009
Training epoch 35 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 35 [40320/67107 (60.11%)]	 Loss: 0.000007
Training epoch 35 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 35 [41600/67107 (62.02%)]	 Loss: 0.000039
Training epoch 35 [42240/67107 (62.98%)]	 Loss: 0.000041
Training epoch 35 [42880/67107 (63.93%)]	 Loss: 0.000053
Training epoch 35 [43520/67107 (64.89%)]	 Loss: 0.000020
Training epoch 35 [44160/67107 (65.84%)]	 Loss: 0.000006
Training epoch 35 [44800/67107 (66.79%)]	 Loss: 0.000068
Training epoch 35 [45440/67107 (67.75%)]	 Loss: 0.000038
Training epoch 35 [46080/67107 (68.70%)]	 Loss: 0.000014
Training epoch 35 [46720/67107 (69.66%)]	 Loss: 0.000023
Training epoch 35 [47360/67107 (70.61%)]	 Loss: 0.000029
Training epoch 35 [48000/67107 (71.56%)]	 Loss: 0.000093
Training epoch 35 [48640/67107 (72.52%)]	 Loss: 0.000009
Training epoch 35 [49280/67107 (73.47%)]	 Loss: 0.000029
Training epoch 35 [49920/67107 (74.43%)]	 Loss: 0.000071
Training epoch 35 [50560/67107 (75.38%)]	 Loss: 0.000029
Training epoch 35 [51200/67107 (76.34%)]	 Loss: 0.000004
Training epoch 35 [51840/67107 (77.29%)]	 Loss: 0.000016
Training epoch 35 [52480/67107 (78.24%)]	 Loss: 0.000007
Training epoch 35 [53120/67107 (79.20%)]	 Loss: 0.000016
Training epoch 35 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 35 [54400/67107 (81.11%)]	 Loss: 0.000008
Training epoch 35 [55040/67107 (82.06%)]	 Loss: 0.000037
Training epoch 35 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 35 [56320/67107 (83.97%)]	 Loss: 0.000016
Training epoch 35 [56960/67107 (84.92%)]	 Loss: 0.000046
Training epoch 35 [57600/67107 (85.88%)]	 Loss: 0.000034
Training epoch 35 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 35 [58880/67107 (87.79%)]	 Loss: 0.000028
Training epoch 35 [59520/67107 (88.74%)]	 Loss: 0.000011
Training epoch 35 [60160/67107 (89.69%)]	 Loss: 0.000006
Training epoch 35 [60800/67107 (90.65%)]	 Loss: 0.000009
Training epoch 35 [61440/67107 (91.60%)]	 Loss: 0.000011
Training epoch 35 [62080/67107 (92.56%)]	 Loss: 0.000028
Training epoch 35 [62720/67107 (93.51%)]	 Loss: 0.000020
Training epoch 35 [63360/67107 (94.47%)]	 Loss: 0.000019
Training epoch 35 [64000/67107 (95.42%)]	 Loss: 0.000015
Training epoch 35 [64640/67107 (96.37%)]	 Loss: 0.000006
Training epoch 35 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 35 [65920/67107 (98.28%)]	 Loss: 0.000045
Training epoch 35 [66560/67107 (99.24%)]	 Loss: 0.000012
Test set: Average Loss: 0.000157
Training epoch 36 [0/67107 (0.00%)]	 Loss: 0.000031
Training epoch 36 [640/67107 (0.95%)]	 Loss: 0.000008
Training epoch 36 [1280/67107 (1.91%)]	 Loss: 0.000010
Training epoch 36 [1920/67107 (2.86%)]	 Loss: 0.000007
Training epoch 36 [2560/67107 (3.82%)]	 Loss: 0.000016
Training epoch 36 [3200/67107 (4.77%)]	 Loss: 0.000140
Training epoch 36 [3840/67107 (5.73%)]	 Loss: 0.000011
Training epoch 36 [4480/67107 (6.68%)]	 Loss: 0.000004
Training epoch 36 [5120/67107 (7.63%)]	 Loss: 0.000011
Training epoch 36 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 36 [6400/67107 (9.54%)]	 Loss: 0.000012
Training epoch 36 [7040/67107 (10.50%)]	 Loss: 0.000007
Training epoch 36 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 36 [8320/67107 (12.40%)]	 Loss: 0.000009
Training epoch 36 [8960/67107 (13.36%)]	 Loss: 0.000004
Training epoch 36 [9600/67107 (14.31%)]	 Loss: 0.000015
Training epoch 36 [10240/67107 (15.27%)]	 Loss: 0.000008
Training epoch 36 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 36 [11520/67107 (17.18%)]	 Loss: 0.000052
Training epoch 36 [12160/67107 (18.13%)]	 Loss: 0.000040
Training epoch 36 [12800/67107 (19.08%)]	 Loss: 0.000036
Training epoch 36 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 36 [14080/67107 (20.99%)]	 Loss: 0.000055
Training epoch 36 [14720/67107 (21.95%)]	 Loss: 0.000024
Training epoch 36 [15360/67107 (22.90%)]	 Loss: 0.000026
Training epoch 36 [16000/67107 (23.85%)]	 Loss: 0.000097
Training epoch 36 [16640/67107 (24.81%)]	 Loss: 0.000021
Training epoch 36 [17280/67107 (25.76%)]	 Loss: 0.000019
Training epoch 36 [17920/67107 (26.72%)]	 Loss: 0.000071
Training epoch 36 [18560/67107 (27.67%)]	 Loss: 0.000008
Training epoch 36 [19200/67107 (28.63%)]	 Loss: 0.000059
Training epoch 36 [19840/67107 (29.58%)]	 Loss: 0.000010
Training epoch 36 [20480/67107 (30.53%)]	 Loss: 0.000039
Training epoch 36 [21120/67107 (31.49%)]	 Loss: 0.000009
Training epoch 36 [21760/67107 (32.44%)]	 Loss: 0.000005
Training epoch 36 [22400/67107 (33.40%)]	 Loss: 0.000014
Training epoch 36 [23040/67107 (34.35%)]	 Loss: 0.000022
Training epoch 36 [23680/67107 (35.31%)]	 Loss: 0.000007
Training epoch 36 [24320/67107 (36.26%)]	 Loss: 0.000006
Training epoch 36 [24960/67107 (37.21%)]	 Loss: 0.000009
Training epoch 36 [25600/67107 (38.17%)]	 Loss: 0.000015
Training epoch 36 [26240/67107 (39.12%)]	 Loss: 0.000035
Training epoch 36 [26880/67107 (40.08%)]	 Loss: 0.000006
Training epoch 36 [27520/67107 (41.03%)]	 Loss: 0.000004
Training epoch 36 [28160/67107 (41.98%)]	 Loss: 0.000006
Training epoch 36 [28800/67107 (42.94%)]	 Loss: 0.000013
Training epoch 36 [29440/67107 (43.89%)]	 Loss: 0.000047
Training epoch 36 [30080/67107 (44.85%)]	 Loss: 0.000116
Training epoch 36 [30720/67107 (45.80%)]	 Loss: 0.000065
Training epoch 36 [31360/67107 (46.76%)]	 Loss: 0.000035
Training epoch 36 [32000/67107 (47.71%)]	 Loss: 0.000095
Training epoch 36 [32640/67107 (48.66%)]	 Loss: 0.000012
Training epoch 36 [33280/67107 (49.62%)]	 Loss: 0.000031
Training epoch 36 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 36 [34560/67107 (51.53%)]	 Loss: 0.000009
Training epoch 36 [35200/67107 (52.48%)]	 Loss: 0.000062
Training epoch 36 [35840/67107 (53.44%)]	 Loss: 0.000042
Training epoch 36 [36480/67107 (54.39%)]	 Loss: 0.000044
Training epoch 36 [37120/67107 (55.34%)]	 Loss: 0.000063
Training epoch 36 [37760/67107 (56.30%)]	 Loss: 0.000145
Training epoch 36 [38400/67107 (57.25%)]	 Loss: 0.000010
Training epoch 36 [39040/67107 (58.21%)]	 Loss: 0.000014
Training epoch 36 [39680/67107 (59.16%)]	 Loss: 0.000007
Training epoch 36 [40320/67107 (60.11%)]	 Loss: 0.000011
Training epoch 36 [40960/67107 (61.07%)]	 Loss: 0.000006
Training epoch 36 [41600/67107 (62.02%)]	 Loss: 0.000011
Training epoch 36 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 36 [42880/67107 (63.93%)]	 Loss: 0.000079
Training epoch 36 [43520/67107 (64.89%)]	 Loss: 0.000020
Training epoch 36 [44160/67107 (65.84%)]	 Loss: 0.000024
Training epoch 36 [44800/67107 (66.79%)]	 Loss: 0.000024
Training epoch 36 [45440/67107 (67.75%)]	 Loss: 0.000082
Training epoch 36 [46080/67107 (68.70%)]	 Loss: 0.000019
Training epoch 36 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 36 [47360/67107 (70.61%)]	 Loss: 0.000013
Training epoch 36 [48000/67107 (71.56%)]	 Loss: 0.000002
Training epoch 36 [48640/67107 (72.52%)]	 Loss: 0.000033
Training epoch 36 [49280/67107 (73.47%)]	 Loss: 0.000002
Training epoch 36 [49920/67107 (74.43%)]	 Loss: 0.000011
Training epoch 36 [50560/67107 (75.38%)]	 Loss: 0.000020
Training epoch 36 [51200/67107 (76.34%)]	 Loss: 0.000044
Training epoch 36 [51840/67107 (77.29%)]	 Loss: 0.000021
Training epoch 36 [52480/67107 (78.24%)]	 Loss: 0.000031
Training epoch 36 [53120/67107 (79.20%)]	 Loss: 0.000036
Training epoch 36 [53760/67107 (80.15%)]	 Loss: 0.000011
Training epoch 36 [54400/67107 (81.11%)]	 Loss: 0.000026
Training epoch 36 [55040/67107 (82.06%)]	 Loss: 0.000017
Training epoch 36 [55680/67107 (83.02%)]	 Loss: 0.000014
Training epoch 36 [56320/67107 (83.97%)]	 Loss: 0.000004
Training epoch 36 [56960/67107 (84.92%)]	 Loss: 0.000078
Training epoch 36 [57600/67107 (85.88%)]	 Loss: 0.000016
Training epoch 36 [58240/67107 (86.83%)]	 Loss: 0.000008
Training epoch 36 [58880/67107 (87.79%)]	 Loss: 0.000095
Training epoch 36 [59520/67107 (88.74%)]	 Loss: 0.000010
Training epoch 36 [60160/67107 (89.69%)]	 Loss: 0.000041
Training epoch 36 [60800/67107 (90.65%)]	 Loss: 0.000004
Training epoch 36 [61440/67107 (91.60%)]	 Loss: 0.000057
Training epoch 36 [62080/67107 (92.56%)]	 Loss: 0.000010
Training epoch 36 [62720/67107 (93.51%)]	 Loss: 0.000010
Training epoch 36 [63360/67107 (94.47%)]	 Loss: 0.000034
Training epoch 36 [64000/67107 (95.42%)]	 Loss: 0.000039
Training epoch 36 [64640/67107 (96.37%)]	 Loss: 0.000017
Training epoch 36 [65280/67107 (97.33%)]	 Loss: 0.000010
Training epoch 36 [65920/67107 (98.28%)]	 Loss: 0.000027
Training epoch 36 [66560/67107 (99.24%)]	 Loss: 0.000079
Test set: Average Loss: 0.004947
Training epoch 37 [0/67107 (0.00%)]	 Loss: 0.000021
Training epoch 37 [640/67107 (0.95%)]	 Loss: 0.000006
Training epoch 37 [1280/67107 (1.91%)]	 Loss: 0.000004
Training epoch 37 [1920/67107 (2.86%)]	 Loss: 0.000003
Training epoch 37 [2560/67107 (3.82%)]	 Loss: 0.000002
Training epoch 37 [3200/67107 (4.77%)]	 Loss: 0.000001
Training epoch 37 [3840/67107 (5.73%)]	 Loss: 0.000004
Training epoch 37 [4480/67107 (6.68%)]	 Loss: 0.000039
Training epoch 37 [5120/67107 (7.63%)]	 Loss: 0.000006
Training epoch 37 [5760/67107 (8.59%)]	 Loss: 0.000016
Training epoch 37 [6400/67107 (9.54%)]	 Loss: 0.000024
Training epoch 37 [7040/67107 (10.50%)]	 Loss: 0.000007
Training epoch 37 [7680/67107 (11.45%)]	 Loss: 0.000017
Training epoch 37 [8320/67107 (12.40%)]	 Loss: 0.000009
Training epoch 37 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 37 [9600/67107 (14.31%)]	 Loss: 0.000013
Training epoch 37 [10240/67107 (15.27%)]	 Loss: 0.000003
Training epoch 37 [10880/67107 (16.22%)]	 Loss: 0.000004
Training epoch 37 [11520/67107 (17.18%)]	 Loss: 0.000003
Training epoch 37 [12160/67107 (18.13%)]	 Loss: 0.000064
Training epoch 37 [12800/67107 (19.08%)]	 Loss: 0.000052
Training epoch 37 [13440/67107 (20.04%)]	 Loss: 0.000004
Training epoch 37 [14080/67107 (20.99%)]	 Loss: 0.000003
Training epoch 37 [14720/67107 (21.95%)]	 Loss: 0.000023
Training epoch 37 [15360/67107 (22.90%)]	 Loss: 0.000022
Training epoch 37 [16000/67107 (23.85%)]	 Loss: 0.000042
Training epoch 37 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 37 [17280/67107 (25.76%)]	 Loss: 0.000014
Training epoch 37 [17920/67107 (26.72%)]	 Loss: 0.000034
Training epoch 37 [18560/67107 (27.67%)]	 Loss: 0.000022
Training epoch 37 [19200/67107 (28.63%)]	 Loss: 0.000009
Training epoch 37 [19840/67107 (29.58%)]	 Loss: 0.000023
Training epoch 37 [20480/67107 (30.53%)]	 Loss: 0.000032
Training epoch 37 [21120/67107 (31.49%)]	 Loss: 0.000003
Training epoch 37 [21760/67107 (32.44%)]	 Loss: 0.000015
Training epoch 37 [22400/67107 (33.40%)]	 Loss: 0.000006
Training epoch 37 [23040/67107 (34.35%)]	 Loss: 0.000016
Training epoch 37 [23680/67107 (35.31%)]	 Loss: 0.000012
Training epoch 37 [24320/67107 (36.26%)]	 Loss: 0.000012
Training epoch 37 [24960/67107 (37.21%)]	 Loss: 0.000028
Training epoch 37 [25600/67107 (38.17%)]	 Loss: 0.000086
Training epoch 37 [26240/67107 (39.12%)]	 Loss: 0.000024
Training epoch 37 [26880/67107 (40.08%)]	 Loss: 0.000016
Training epoch 37 [27520/67107 (41.03%)]	 Loss: 0.000004
Training epoch 37 [28160/67107 (41.98%)]	 Loss: 0.000013
Training epoch 37 [28800/67107 (42.94%)]	 Loss: 0.000010
Training epoch 37 [29440/67107 (43.89%)]	 Loss: 0.000028
Training epoch 37 [30080/67107 (44.85%)]	 Loss: 0.000005
Training epoch 37 [30720/67107 (45.80%)]	 Loss: 0.000011
Training epoch 37 [31360/67107 (46.76%)]	 Loss: 0.000006
Training epoch 37 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 37 [32640/67107 (48.66%)]	 Loss: 0.000022
Training epoch 37 [33280/67107 (49.62%)]	 Loss: 0.000025
Training epoch 37 [33920/67107 (50.57%)]	 Loss: 0.000009
Training epoch 37 [34560/67107 (51.53%)]	 Loss: 0.000016
Training epoch 37 [35200/67107 (52.48%)]	 Loss: 0.000009
Training epoch 37 [35840/67107 (53.44%)]	 Loss: 0.000003
Training epoch 37 [36480/67107 (54.39%)]	 Loss: 0.000033
Training epoch 37 [37120/67107 (55.34%)]	 Loss: 0.000004
Training epoch 37 [37760/67107 (56.30%)]	 Loss: 0.000004
Training epoch 37 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 37 [39040/67107 (58.21%)]	 Loss: 0.000006
Training epoch 37 [39680/67107 (59.16%)]	 Loss: 0.000002
Training epoch 37 [40320/67107 (60.11%)]	 Loss: 0.000027
Training epoch 37 [40960/67107 (61.07%)]	 Loss: 0.000014
Training epoch 37 [41600/67107 (62.02%)]	 Loss: 0.000070
Training epoch 37 [42240/67107 (62.98%)]	 Loss: 0.000007
Training epoch 37 [42880/67107 (63.93%)]	 Loss: 0.000028
Training epoch 37 [43520/67107 (64.89%)]	 Loss: 0.000028
Training epoch 37 [44160/67107 (65.84%)]	 Loss: 0.000023
Training epoch 37 [44800/67107 (66.79%)]	 Loss: 0.000013
Training epoch 37 [45440/67107 (67.75%)]	 Loss: 0.000015
Training epoch 37 [46080/67107 (68.70%)]	 Loss: 0.000037
Training epoch 37 [46720/67107 (69.66%)]	 Loss: 0.000013
Training epoch 37 [47360/67107 (70.61%)]	 Loss: 0.000026
Training epoch 37 [48000/67107 (71.56%)]	 Loss: 0.000074
Training epoch 37 [48640/67107 (72.52%)]	 Loss: 0.000028
Training epoch 37 [49280/67107 (73.47%)]	 Loss: 0.000066
Training epoch 37 [49920/67107 (74.43%)]	 Loss: 0.000042
Training epoch 37 [50560/67107 (75.38%)]	 Loss: 0.000004
Training epoch 37 [51200/67107 (76.34%)]	 Loss: 0.000002
Training epoch 37 [51840/67107 (77.29%)]	 Loss: 0.000002
Training epoch 37 [52480/67107 (78.24%)]	 Loss: 0.000016
Training epoch 37 [53120/67107 (79.20%)]	 Loss: 0.000039
Training epoch 37 [53760/67107 (80.15%)]	 Loss: 0.000020
Training epoch 37 [54400/67107 (81.11%)]	 Loss: 0.000034
Training epoch 37 [55040/67107 (82.06%)]	 Loss: 0.000002
Training epoch 37 [55680/67107 (83.02%)]	 Loss: 0.000014
Training epoch 37 [56320/67107 (83.97%)]	 Loss: 0.000004
Training epoch 37 [56960/67107 (84.92%)]	 Loss: 0.000014
Training epoch 37 [57600/67107 (85.88%)]	 Loss: 0.000006
Training epoch 37 [58240/67107 (86.83%)]	 Loss: 0.000008
Training epoch 37 [58880/67107 (87.79%)]	 Loss: 0.000006
Training epoch 37 [59520/67107 (88.74%)]	 Loss: 0.000016
Training epoch 37 [60160/67107 (89.69%)]	 Loss: 0.000007
Training epoch 37 [60800/67107 (90.65%)]	 Loss: 0.000038
Training epoch 37 [61440/67107 (91.60%)]	 Loss: 0.000024
Training epoch 37 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 37 [62720/67107 (93.51%)]	 Loss: 0.000014
Training epoch 37 [63360/67107 (94.47%)]	 Loss: 0.000013
Training epoch 37 [64000/67107 (95.42%)]	 Loss: 0.000006
Training epoch 37 [64640/67107 (96.37%)]	 Loss: 0.000007
Training epoch 37 [65280/67107 (97.33%)]	 Loss: 0.000039
Training epoch 37 [65920/67107 (98.28%)]	 Loss: 0.000014
Training epoch 37 [66560/67107 (99.24%)]	 Loss: 0.000017
Test set: Average Loss: 0.000390
Training epoch 38 [0/67107 (0.00%)]	 Loss: 0.000025
Training epoch 38 [640/67107 (0.95%)]	 Loss: 0.000033
Training epoch 38 [1280/67107 (1.91%)]	 Loss: 0.000049
Training epoch 38 [1920/67107 (2.86%)]	 Loss: 0.000014
Training epoch 38 [2560/67107 (3.82%)]	 Loss: 0.000049
Training epoch 38 [3200/67107 (4.77%)]	 Loss: 0.000053
Training epoch 38 [3840/67107 (5.73%)]	 Loss: 0.000024
Training epoch 38 [4480/67107 (6.68%)]	 Loss: 0.000004
Training epoch 38 [5120/67107 (7.63%)]	 Loss: 0.000007
Training epoch 38 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 38 [6400/67107 (9.54%)]	 Loss: 0.000013
Training epoch 38 [7040/67107 (10.50%)]	 Loss: 0.000008
Training epoch 38 [7680/67107 (11.45%)]	 Loss: 0.000017
Training epoch 38 [8320/67107 (12.40%)]	 Loss: 0.000016
Training epoch 38 [8960/67107 (13.36%)]	 Loss: 0.000021
Training epoch 38 [9600/67107 (14.31%)]	 Loss: 0.000008
Training epoch 38 [10240/67107 (15.27%)]	 Loss: 0.000003
Training epoch 38 [10880/67107 (16.22%)]	 Loss: 0.000004
Training epoch 38 [11520/67107 (17.18%)]	 Loss: 0.000015
Training epoch 38 [12160/67107 (18.13%)]	 Loss: 0.000060
Training epoch 38 [12800/67107 (19.08%)]	 Loss: 0.000006
Training epoch 38 [13440/67107 (20.04%)]	 Loss: 0.000023
Training epoch 38 [14080/67107 (20.99%)]	 Loss: 0.000008
Training epoch 38 [14720/67107 (21.95%)]	 Loss: 0.000018
Training epoch 38 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 38 [16000/67107 (23.85%)]	 Loss: 0.000028
Training epoch 38 [16640/67107 (24.81%)]	 Loss: 0.000008
Training epoch 38 [17280/67107 (25.76%)]	 Loss: 0.000028
Training epoch 38 [17920/67107 (26.72%)]	 Loss: 0.000036
Training epoch 38 [18560/67107 (27.67%)]	 Loss: 0.000044
Training epoch 38 [19200/67107 (28.63%)]	 Loss: 0.000023
Training epoch 38 [19840/67107 (29.58%)]	 Loss: 0.000012
Training epoch 38 [20480/67107 (30.53%)]	 Loss: 0.000041
Training epoch 38 [21120/67107 (31.49%)]	 Loss: 0.000018
Training epoch 38 [21760/67107 (32.44%)]	 Loss: 0.000016
Training epoch 38 [22400/67107 (33.40%)]	 Loss: 0.000031
Training epoch 38 [23040/67107 (34.35%)]	 Loss: 0.000040
Training epoch 38 [23680/67107 (35.31%)]	 Loss: 0.000009
Training epoch 38 [24320/67107 (36.26%)]	 Loss: 0.000034
Training epoch 38 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 38 [25600/67107 (38.17%)]	 Loss: 0.000016
Training epoch 38 [26240/67107 (39.12%)]	 Loss: 0.000003
Training epoch 38 [26880/67107 (40.08%)]	 Loss: 0.000031
Training epoch 38 [27520/67107 (41.03%)]	 Loss: 0.000038
Training epoch 38 [28160/67107 (41.98%)]	 Loss: 0.000020
Training epoch 38 [28800/67107 (42.94%)]	 Loss: 0.000028
Training epoch 38 [29440/67107 (43.89%)]	 Loss: 0.000030
Training epoch 38 [30080/67107 (44.85%)]	 Loss: 0.000004
Training epoch 38 [30720/67107 (45.80%)]	 Loss: 0.000048
Training epoch 38 [31360/67107 (46.76%)]	 Loss: 0.000014
Training epoch 38 [32000/67107 (47.71%)]	 Loss: 0.000058
Training epoch 38 [32640/67107 (48.66%)]	 Loss: 0.000011
Training epoch 38 [33280/67107 (49.62%)]	 Loss: 0.000029
Training epoch 38 [33920/67107 (50.57%)]	 Loss: 0.000011
Training epoch 38 [34560/67107 (51.53%)]	 Loss: 0.000003
Training epoch 38 [35200/67107 (52.48%)]	 Loss: 0.000004
Training epoch 38 [35840/67107 (53.44%)]	 Loss: 0.000003
Training epoch 38 [36480/67107 (54.39%)]	 Loss: 0.000004
Training epoch 38 [37120/67107 (55.34%)]	 Loss: 0.000022
Training epoch 38 [37760/67107 (56.30%)]	 Loss: 0.000020
Training epoch 38 [38400/67107 (57.25%)]	 Loss: 0.000033
Training epoch 38 [39040/67107 (58.21%)]	 Loss: 0.000118
Training epoch 38 [39680/67107 (59.16%)]	 Loss: 0.000030
Training epoch 38 [40320/67107 (60.11%)]	 Loss: 0.000014
Training epoch 38 [40960/67107 (61.07%)]	 Loss: 0.000009
Training epoch 38 [41600/67107 (62.02%)]	 Loss: 0.000012
Training epoch 38 [42240/67107 (62.98%)]	 Loss: 0.000046
Training epoch 38 [42880/67107 (63.93%)]	 Loss: 0.000024
Training epoch 38 [43520/67107 (64.89%)]	 Loss: 0.000064
Training epoch 38 [44160/67107 (65.84%)]	 Loss: 0.000021
Training epoch 38 [44800/67107 (66.79%)]	 Loss: 0.000010
Training epoch 38 [45440/67107 (67.75%)]	 Loss: 0.000034
Training epoch 38 [46080/67107 (68.70%)]	 Loss: 0.000037
Training epoch 38 [46720/67107 (69.66%)]	 Loss: 0.000007
Training epoch 38 [47360/67107 (70.61%)]	 Loss: 0.000015
Training epoch 38 [48000/67107 (71.56%)]	 Loss: 0.000006
Training epoch 38 [48640/67107 (72.52%)]	 Loss: 0.000009
Training epoch 38 [49280/67107 (73.47%)]	 Loss: 0.000015
Training epoch 38 [49920/67107 (74.43%)]	 Loss: 0.000006
Training epoch 38 [50560/67107 (75.38%)]	 Loss: 0.000019
Training epoch 38 [51200/67107 (76.34%)]	 Loss: 0.000015
Training epoch 38 [51840/67107 (77.29%)]	 Loss: 0.000026
Training epoch 38 [52480/67107 (78.24%)]	 Loss: 0.000023
Training epoch 38 [53120/67107 (79.20%)]	 Loss: 0.000012
Training epoch 38 [53760/67107 (80.15%)]	 Loss: 0.000005
Training epoch 38 [54400/67107 (81.11%)]	 Loss: 0.000007
Training epoch 38 [55040/67107 (82.06%)]	 Loss: 0.000051
Training epoch 38 [55680/67107 (83.02%)]	 Loss: 0.000005
Training epoch 38 [56320/67107 (83.97%)]	 Loss: 0.000047
Training epoch 38 [56960/67107 (84.92%)]	 Loss: 0.000048
Training epoch 38 [57600/67107 (85.88%)]	 Loss: 0.000024
Training epoch 38 [58240/67107 (86.83%)]	 Loss: 0.000053
Training epoch 38 [58880/67107 (87.79%)]	 Loss: 0.000092
Training epoch 38 [59520/67107 (88.74%)]	 Loss: 0.000025
Training epoch 38 [60160/67107 (89.69%)]	 Loss: 0.000031
Training epoch 38 [60800/67107 (90.65%)]	 Loss: 0.000008
Training epoch 38 [61440/67107 (91.60%)]	 Loss: 0.000007
Training epoch 38 [62080/67107 (92.56%)]	 Loss: 0.000039
Training epoch 38 [62720/67107 (93.51%)]	 Loss: 0.000017
Training epoch 38 [63360/67107 (94.47%)]	 Loss: 0.000038
Training epoch 38 [64000/67107 (95.42%)]	 Loss: 0.000042
Training epoch 38 [64640/67107 (96.37%)]	 Loss: 0.000093
Training epoch 38 [65280/67107 (97.33%)]	 Loss: 0.000011
Training epoch 38 [65920/67107 (98.28%)]	 Loss: 0.000004
Training epoch 38 [66560/67107 (99.24%)]	 Loss: 0.000004
Test set: Average Loss: 0.000356
Training epoch 39 [0/67107 (0.00%)]	 Loss: 0.000023
Training epoch 39 [640/67107 (0.95%)]	 Loss: 0.000082
Training epoch 39 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 39 [1920/67107 (2.86%)]	 Loss: 0.000033
Training epoch 39 [2560/67107 (3.82%)]	 Loss: 0.000008
Training epoch 39 [3200/67107 (4.77%)]	 Loss: 0.000021
Training epoch 39 [3840/67107 (5.73%)]	 Loss: 0.000033
Training epoch 39 [4480/67107 (6.68%)]	 Loss: 0.000018
Training epoch 39 [5120/67107 (7.63%)]	 Loss: 0.000045
Training epoch 39 [5760/67107 (8.59%)]	 Loss: 0.000123
Training epoch 39 [6400/67107 (9.54%)]	 Loss: 0.000026
Training epoch 39 [7040/67107 (10.50%)]	 Loss: 0.000003
Training epoch 39 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 39 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 39 [8960/67107 (13.36%)]	 Loss: 0.000011
Training epoch 39 [9600/67107 (14.31%)]	 Loss: 0.000009
Training epoch 39 [10240/67107 (15.27%)]	 Loss: 0.000002
Training epoch 39 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 39 [11520/67107 (17.18%)]	 Loss: 0.000012
Training epoch 39 [12160/67107 (18.13%)]	 Loss: 0.000017
Training epoch 39 [12800/67107 (19.08%)]	 Loss: 0.000003
Training epoch 39 [13440/67107 (20.04%)]	 Loss: 0.000022
Training epoch 39 [14080/67107 (20.99%)]	 Loss: 0.000025
Training epoch 39 [14720/67107 (21.95%)]	 Loss: 0.000041
Training epoch 39 [15360/67107 (22.90%)]	 Loss: 0.000015
Training epoch 39 [16000/67107 (23.85%)]	 Loss: 0.000003
Training epoch 39 [16640/67107 (24.81%)]	 Loss: 0.000006
Training epoch 39 [17280/67107 (25.76%)]	 Loss: 0.000095
Training epoch 39 [17920/67107 (26.72%)]	 Loss: 0.000253
Training epoch 39 [18560/67107 (27.67%)]	 Loss: 0.000044
Training epoch 39 [19200/67107 (28.63%)]	 Loss: 0.000038
Training epoch 39 [19840/67107 (29.58%)]	 Loss: 0.000027
Training epoch 39 [20480/67107 (30.53%)]	 Loss: 0.000010
Training epoch 39 [21120/67107 (31.49%)]	 Loss: 0.000010
Training epoch 39 [21760/67107 (32.44%)]	 Loss: 0.000087
Training epoch 39 [22400/67107 (33.40%)]	 Loss: 0.000050
Training epoch 39 [23040/67107 (34.35%)]	 Loss: 0.000008
Training epoch 39 [23680/67107 (35.31%)]	 Loss: 0.000020
Training epoch 39 [24320/67107 (36.26%)]	 Loss: 0.000036
Training epoch 39 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 39 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 39 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 39 [26880/67107 (40.08%)]	 Loss: 0.000005
Training epoch 39 [27520/67107 (41.03%)]	 Loss: 0.000008
Training epoch 39 [28160/67107 (41.98%)]	 Loss: 0.000005
Training epoch 39 [28800/67107 (42.94%)]	 Loss: 0.000010
Training epoch 39 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 39 [30080/67107 (44.85%)]	 Loss: 0.000003
Training epoch 39 [30720/67107 (45.80%)]	 Loss: 0.000014
Training epoch 39 [31360/67107 (46.76%)]	 Loss: 0.000020
Training epoch 39 [32000/67107 (47.71%)]	 Loss: 0.000021
Training epoch 39 [32640/67107 (48.66%)]	 Loss: 0.000033
Training epoch 39 [33280/67107 (49.62%)]	 Loss: 0.000010
Training epoch 39 [33920/67107 (50.57%)]	 Loss: 0.000011
Training epoch 39 [34560/67107 (51.53%)]	 Loss: 0.000005
Training epoch 39 [35200/67107 (52.48%)]	 Loss: 0.000003
Training epoch 39 [35840/67107 (53.44%)]	 Loss: 0.000013
Training epoch 39 [36480/67107 (54.39%)]	 Loss: 0.000014
Training epoch 39 [37120/67107 (55.34%)]	 Loss: 0.000009
Training epoch 39 [37760/67107 (56.30%)]	 Loss: 0.000029
Training epoch 39 [38400/67107 (57.25%)]	 Loss: 0.000008
Training epoch 39 [39040/67107 (58.21%)]	 Loss: 0.000002
Training epoch 39 [39680/67107 (59.16%)]	 Loss: 0.000026
Training epoch 39 [40320/67107 (60.11%)]	 Loss: 0.000031
Training epoch 39 [40960/67107 (61.07%)]	 Loss: 0.000013
Training epoch 39 [41600/67107 (62.02%)]	 Loss: 0.000007
Training epoch 39 [42240/67107 (62.98%)]	 Loss: 0.000014
Training epoch 39 [42880/67107 (63.93%)]	 Loss: 0.000063
Training epoch 39 [43520/67107 (64.89%)]	 Loss: 0.000076
Training epoch 39 [44160/67107 (65.84%)]	 Loss: 0.000020
Training epoch 39 [44800/67107 (66.79%)]	 Loss: 0.000004
Training epoch 39 [45440/67107 (67.75%)]	 Loss: 0.000019
Training epoch 39 [46080/67107 (68.70%)]	 Loss: 0.000042
Training epoch 39 [46720/67107 (69.66%)]	 Loss: 0.000035
Training epoch 39 [47360/67107 (70.61%)]	 Loss: 0.000018
Training epoch 39 [48000/67107 (71.56%)]	 Loss: 0.000019
Training epoch 39 [48640/67107 (72.52%)]	 Loss: 0.000009
Training epoch 39 [49280/67107 (73.47%)]	 Loss: 0.000020
Training epoch 39 [49920/67107 (74.43%)]	 Loss: 0.000038
Training epoch 39 [50560/67107 (75.38%)]	 Loss: 0.000033
Training epoch 39 [51200/67107 (76.34%)]	 Loss: 0.000014
Training epoch 39 [51840/67107 (77.29%)]	 Loss: 0.000052
Training epoch 39 [52480/67107 (78.24%)]	 Loss: 0.000026
Training epoch 39 [53120/67107 (79.20%)]	 Loss: 0.000062
Training epoch 39 [53760/67107 (80.15%)]	 Loss: 0.000003
Training epoch 39 [54400/67107 (81.11%)]	 Loss: 0.000019
Training epoch 39 [55040/67107 (82.06%)]	 Loss: 0.000030
Training epoch 39 [55680/67107 (83.02%)]	 Loss: 0.000020
Training epoch 39 [56320/67107 (83.97%)]	 Loss: 0.000011
Training epoch 39 [56960/67107 (84.92%)]	 Loss: 0.000013
Training epoch 39 [57600/67107 (85.88%)]	 Loss: 0.000011
Training epoch 39 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 39 [58880/67107 (87.79%)]	 Loss: 0.000015
Training epoch 39 [59520/67107 (88.74%)]	 Loss: 0.000005
Training epoch 39 [60160/67107 (89.69%)]	 Loss: 0.000009
Training epoch 39 [60800/67107 (90.65%)]	 Loss: 0.000024
Training epoch 39 [61440/67107 (91.60%)]	 Loss: 0.000004
Training epoch 39 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 39 [62720/67107 (93.51%)]	 Loss: 0.000008
Training epoch 39 [63360/67107 (94.47%)]	 Loss: 0.000022
Training epoch 39 [64000/67107 (95.42%)]	 Loss: 0.000013
Training epoch 39 [64640/67107 (96.37%)]	 Loss: 0.000023
Training epoch 39 [65280/67107 (97.33%)]	 Loss: 0.000007
Training epoch 39 [65920/67107 (98.28%)]	 Loss: 0.000006
Training epoch 39 [66560/67107 (99.24%)]	 Loss: 0.000007
Test set: Average Loss: 0.000898
Training epoch 40 [0/67107 (0.00%)]	 Loss: 0.000002
Training epoch 40 [640/67107 (0.95%)]	 Loss: 0.000014
Training epoch 40 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 40 [1920/67107 (2.86%)]	 Loss: 0.000079
Training epoch 40 [2560/67107 (3.82%)]	 Loss: 0.000005
Training epoch 40 [3200/67107 (4.77%)]	 Loss: 0.000038
Training epoch 40 [3840/67107 (5.73%)]	 Loss: 0.000015
Training epoch 40 [4480/67107 (6.68%)]	 Loss: 0.000002
Training epoch 40 [5120/67107 (7.63%)]	 Loss: 0.000043
Training epoch 40 [5760/67107 (8.59%)]	 Loss: 0.000028
Training epoch 40 [6400/67107 (9.54%)]	 Loss: 0.000031
Training epoch 40 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 40 [7680/67107 (11.45%)]	 Loss: 0.000004
Training epoch 40 [8320/67107 (12.40%)]	 Loss: 0.000004
Training epoch 40 [8960/67107 (13.36%)]	 Loss: 0.000007
Training epoch 40 [9600/67107 (14.31%)]	 Loss: 0.000037
Training epoch 40 [10240/67107 (15.27%)]	 Loss: 0.000019
Training epoch 40 [10880/67107 (16.22%)]	 Loss: 0.000036
Training epoch 40 [11520/67107 (17.18%)]	 Loss: 0.000007
Training epoch 40 [12160/67107 (18.13%)]	 Loss: 0.000065
Training epoch 40 [12800/67107 (19.08%)]	 Loss: 0.000064
Training epoch 40 [13440/67107 (20.04%)]	 Loss: 0.000048
Training epoch 40 [14080/67107 (20.99%)]	 Loss: 0.000005
Training epoch 40 [14720/67107 (21.95%)]	 Loss: 0.000003
Training epoch 40 [15360/67107 (22.90%)]	 Loss: 0.000007
Training epoch 40 [16000/67107 (23.85%)]	 Loss: 0.000037
Training epoch 40 [16640/67107 (24.81%)]	 Loss: 0.000048
Training epoch 40 [17280/67107 (25.76%)]	 Loss: 0.000047
Training epoch 40 [17920/67107 (26.72%)]	 Loss: 0.000010
Training epoch 40 [18560/67107 (27.67%)]	 Loss: 0.000015
Training epoch 40 [19200/67107 (28.63%)]	 Loss: 0.000047
Training epoch 40 [19840/67107 (29.58%)]	 Loss: 0.000042
Training epoch 40 [20480/67107 (30.53%)]	 Loss: 0.000048
Training epoch 40 [21120/67107 (31.49%)]	 Loss: 0.000013
Training epoch 40 [21760/67107 (32.44%)]	 Loss: 0.000003
Training epoch 40 [22400/67107 (33.40%)]	 Loss: 0.000004
Training epoch 40 [23040/67107 (34.35%)]	 Loss: 0.000014
Training epoch 40 [23680/67107 (35.31%)]	 Loss: 0.000003
Training epoch 40 [24320/67107 (36.26%)]	 Loss: 0.000005
Training epoch 40 [24960/67107 (37.21%)]	 Loss: 0.000004
Training epoch 40 [25600/67107 (38.17%)]	 Loss: 0.000015
Training epoch 40 [26240/67107 (39.12%)]	 Loss: 0.000015
Training epoch 40 [26880/67107 (40.08%)]	 Loss: 0.000059
Training epoch 40 [27520/67107 (41.03%)]	 Loss: 0.000017
Training epoch 40 [28160/67107 (41.98%)]	 Loss: 0.000004
Training epoch 40 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 40 [29440/67107 (43.89%)]	 Loss: 0.000020
Training epoch 40 [30080/67107 (44.85%)]	 Loss: 0.000019
Training epoch 40 [30720/67107 (45.80%)]	 Loss: 0.000011
Training epoch 40 [31360/67107 (46.76%)]	 Loss: 0.000005
Training epoch 40 [32000/67107 (47.71%)]	 Loss: 0.000003
Training epoch 40 [32640/67107 (48.66%)]	 Loss: 0.000015
Training epoch 40 [33280/67107 (49.62%)]	 Loss: 0.000021
Training epoch 40 [33920/67107 (50.57%)]	 Loss: 0.000029
Training epoch 40 [34560/67107 (51.53%)]	 Loss: 0.000011
Training epoch 40 [35200/67107 (52.48%)]	 Loss: 0.000023
Training epoch 40 [35840/67107 (53.44%)]	 Loss: 0.000023
Training epoch 40 [36480/67107 (54.39%)]	 Loss: 0.000012
Training epoch 40 [37120/67107 (55.34%)]	 Loss: 0.000041
Training epoch 40 [37760/67107 (56.30%)]	 Loss: 0.000006
Training epoch 40 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 40 [39040/67107 (58.21%)]	 Loss: 0.000022
Training epoch 40 [39680/67107 (59.16%)]	 Loss: 0.000042
Training epoch 40 [40320/67107 (60.11%)]	 Loss: 0.000046
Training epoch 40 [40960/67107 (61.07%)]	 Loss: 0.000015
Training epoch 40 [41600/67107 (62.02%)]	 Loss: 0.000039
Training epoch 40 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 40 [42880/67107 (63.93%)]	 Loss: 0.000088
Training epoch 40 [43520/67107 (64.89%)]	 Loss: 0.000061
Training epoch 40 [44160/67107 (65.84%)]	 Loss: 0.000013
Training epoch 40 [44800/67107 (66.79%)]	 Loss: 0.000004
Training epoch 40 [45440/67107 (67.75%)]	 Loss: 0.000035
Training epoch 40 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 40 [46720/67107 (69.66%)]	 Loss: 0.000080
Training epoch 40 [47360/67107 (70.61%)]	 Loss: 0.000030
Training epoch 40 [48000/67107 (71.56%)]	 Loss: 0.000064
Training epoch 40 [48640/67107 (72.52%)]	 Loss: 0.000012
Training epoch 40 [49280/67107 (73.47%)]	 Loss: 0.000027
Training epoch 40 [49920/67107 (74.43%)]	 Loss: 0.000006
Training epoch 40 [50560/67107 (75.38%)]	 Loss: 0.000006
Training epoch 40 [51200/67107 (76.34%)]	 Loss: 0.000005
Training epoch 40 [51840/67107 (77.29%)]	 Loss: 0.000009
Training epoch 40 [52480/67107 (78.24%)]	 Loss: 0.000008
Training epoch 40 [53120/67107 (79.20%)]	 Loss: 0.000018
Training epoch 40 [53760/67107 (80.15%)]	 Loss: 0.000013
Training epoch 40 [54400/67107 (81.11%)]	 Loss: 0.000009
Training epoch 40 [55040/67107 (82.06%)]	 Loss: 0.000074
Training epoch 40 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 40 [56320/67107 (83.97%)]	 Loss: 0.000012
Training epoch 40 [56960/67107 (84.92%)]	 Loss: 0.000012
Training epoch 40 [57600/67107 (85.88%)]	 Loss: 0.000014
Training epoch 40 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 40 [58880/67107 (87.79%)]	 Loss: 0.000015
Training epoch 40 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 40 [60160/67107 (89.69%)]	 Loss: 0.000031
Training epoch 40 [60800/67107 (90.65%)]	 Loss: 0.000014
Training epoch 40 [61440/67107 (91.60%)]	 Loss: 0.000027
Training epoch 40 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 40 [62720/67107 (93.51%)]	 Loss: 0.000017
Training epoch 40 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 40 [64000/67107 (95.42%)]	 Loss: 0.000013
Training epoch 40 [64640/67107 (96.37%)]	 Loss: 0.000016
Training epoch 40 [65280/67107 (97.33%)]	 Loss: 0.000009
Training epoch 40 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 40 [66560/67107 (99.24%)]	 Loss: 0.000009
Test set: Average Loss: 0.000416
Training epoch 41 [0/67107 (0.00%)]	 Loss: 0.000031
Training epoch 41 [640/67107 (0.95%)]	 Loss: 0.000092
Training epoch 41 [1280/67107 (1.91%)]	 Loss: 0.000023
Training epoch 41 [1920/67107 (2.86%)]	 Loss: 0.000017
Training epoch 41 [2560/67107 (3.82%)]	 Loss: 0.000057
Training epoch 41 [3200/67107 (4.77%)]	 Loss: 0.000010
Training epoch 41 [3840/67107 (5.73%)]	 Loss: 0.000073
Training epoch 41 [4480/67107 (6.68%)]	 Loss: 0.000018
Training epoch 41 [5120/67107 (7.63%)]	 Loss: 0.000016
Training epoch 41 [5760/67107 (8.59%)]	 Loss: 0.000017
Training epoch 41 [6400/67107 (9.54%)]	 Loss: 0.000010
Training epoch 41 [7040/67107 (10.50%)]	 Loss: 0.000027
Training epoch 41 [7680/67107 (11.45%)]	 Loss: 0.000056
Training epoch 41 [8320/67107 (12.40%)]	 Loss: 0.000057
Training epoch 41 [8960/67107 (13.36%)]	 Loss: 0.000032
Training epoch 41 [9600/67107 (14.31%)]	 Loss: 0.000005
Training epoch 41 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 41 [10880/67107 (16.22%)]	 Loss: 0.000002
Training epoch 41 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 41 [12160/67107 (18.13%)]	 Loss: 0.000003
Training epoch 41 [12800/67107 (19.08%)]	 Loss: 0.000120
Training epoch 41 [13440/67107 (20.04%)]	 Loss: 0.000021
Training epoch 41 [14080/67107 (20.99%)]	 Loss: 0.000018
Training epoch 41 [14720/67107 (21.95%)]	 Loss: 0.000020
Training epoch 41 [15360/67107 (22.90%)]	 Loss: 0.000007
Training epoch 41 [16000/67107 (23.85%)]	 Loss: 0.000035
Training epoch 41 [16640/67107 (24.81%)]	 Loss: 0.000017
Training epoch 41 [17280/67107 (25.76%)]	 Loss: 0.000007
Training epoch 41 [17920/67107 (26.72%)]	 Loss: 0.000012
Training epoch 41 [18560/67107 (27.67%)]	 Loss: 0.000002
Training epoch 41 [19200/67107 (28.63%)]	 Loss: 0.000005
Training epoch 41 [19840/67107 (29.58%)]	 Loss: 0.000001
Training epoch 41 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 41 [21120/67107 (31.49%)]	 Loss: 0.000007
Training epoch 41 [21760/67107 (32.44%)]	 Loss: 0.000004
Training epoch 41 [22400/67107 (33.40%)]	 Loss: 0.000035
Training epoch 41 [23040/67107 (34.35%)]	 Loss: 0.000014
Training epoch 41 [23680/67107 (35.31%)]	 Loss: 0.000012
Training epoch 41 [24320/67107 (36.26%)]	 Loss: 0.000028
Training epoch 41 [24960/67107 (37.21%)]	 Loss: 0.000040
Training epoch 41 [25600/67107 (38.17%)]	 Loss: 0.000017
Training epoch 41 [26240/67107 (39.12%)]	 Loss: 0.000008
Training epoch 41 [26880/67107 (40.08%)]	 Loss: 0.000008
Training epoch 41 [27520/67107 (41.03%)]	 Loss: 0.000005
Training epoch 41 [28160/67107 (41.98%)]	 Loss: 0.000063
Training epoch 41 [28800/67107 (42.94%)]	 Loss: 0.000069
Training epoch 41 [29440/67107 (43.89%)]	 Loss: 0.000018
Training epoch 41 [30080/67107 (44.85%)]	 Loss: 0.000081
Training epoch 41 [30720/67107 (45.80%)]	 Loss: 0.000014
Training epoch 41 [31360/67107 (46.76%)]	 Loss: 0.000013
Training epoch 41 [32000/67107 (47.71%)]	 Loss: 0.000016
Training epoch 41 [32640/67107 (48.66%)]	 Loss: 0.000060
Training epoch 41 [33280/67107 (49.62%)]	 Loss: 0.000020
Training epoch 41 [33920/67107 (50.57%)]	 Loss: 0.000013
Training epoch 41 [34560/67107 (51.53%)]	 Loss: 0.000045
Training epoch 41 [35200/67107 (52.48%)]	 Loss: 0.000033
Training epoch 41 [35840/67107 (53.44%)]	 Loss: 0.000032
Training epoch 41 [36480/67107 (54.39%)]	 Loss: 0.000057
Training epoch 41 [37120/67107 (55.34%)]	 Loss: 0.000071
Training epoch 41 [37760/67107 (56.30%)]	 Loss: 0.000043
Training epoch 41 [38400/67107 (57.25%)]	 Loss: 0.000009
Training epoch 41 [39040/67107 (58.21%)]	 Loss: 0.000007
Training epoch 41 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 41 [40320/67107 (60.11%)]	 Loss: 0.000017
Training epoch 41 [40960/67107 (61.07%)]	 Loss: 0.000033
Training epoch 41 [41600/67107 (62.02%)]	 Loss: 0.000012
Training epoch 41 [42240/67107 (62.98%)]	 Loss: 0.000039
Training epoch 41 [42880/67107 (63.93%)]	 Loss: 0.000061
Training epoch 41 [43520/67107 (64.89%)]	 Loss: 0.000074
Training epoch 41 [44160/67107 (65.84%)]	 Loss: 0.000004
Training epoch 41 [44800/67107 (66.79%)]	 Loss: 0.000005
Training epoch 41 [45440/67107 (67.75%)]	 Loss: 0.000021
Training epoch 41 [46080/67107 (68.70%)]	 Loss: 0.000037
Training epoch 41 [46720/67107 (69.66%)]	 Loss: 0.000012
Training epoch 41 [47360/67107 (70.61%)]	 Loss: 0.000006
Training epoch 41 [48000/67107 (71.56%)]	 Loss: 0.000007
Training epoch 41 [48640/67107 (72.52%)]	 Loss: 0.000011
Training epoch 41 [49280/67107 (73.47%)]	 Loss: 0.000009
Training epoch 41 [49920/67107 (74.43%)]	 Loss: 0.000053
Training epoch 41 [50560/67107 (75.38%)]	 Loss: 0.000021
Training epoch 41 [51200/67107 (76.34%)]	 Loss: 0.000011
Training epoch 41 [51840/67107 (77.29%)]	 Loss: 0.000021
Training epoch 41 [52480/67107 (78.24%)]	 Loss: 0.000018
Training epoch 41 [53120/67107 (79.20%)]	 Loss: 0.000024
Training epoch 41 [53760/67107 (80.15%)]	 Loss: 0.000001
Training epoch 41 [54400/67107 (81.11%)]	 Loss: 0.000047
Training epoch 41 [55040/67107 (82.06%)]	 Loss: 0.000017
Training epoch 41 [55680/67107 (83.02%)]	 Loss: 0.000049
Training epoch 41 [56320/67107 (83.97%)]	 Loss: 0.000009
Training epoch 41 [56960/67107 (84.92%)]	 Loss: 0.000023
Training epoch 41 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 41 [58240/67107 (86.83%)]	 Loss: 0.000022
Training epoch 41 [58880/67107 (87.79%)]	 Loss: 0.000023
Training epoch 41 [59520/67107 (88.74%)]	 Loss: 0.000075
Training epoch 41 [60160/67107 (89.69%)]	 Loss: 0.000020
Training epoch 41 [60800/67107 (90.65%)]	 Loss: 0.000016
Training epoch 41 [61440/67107 (91.60%)]	 Loss: 0.000076
Training epoch 41 [62080/67107 (92.56%)]	 Loss: 0.000020
Training epoch 41 [62720/67107 (93.51%)]	 Loss: 0.000016
Training epoch 41 [63360/67107 (94.47%)]	 Loss: 0.000037
Training epoch 41 [64000/67107 (95.42%)]	 Loss: 0.000040
Training epoch 41 [64640/67107 (96.37%)]	 Loss: 0.000007
Training epoch 41 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 41 [65920/67107 (98.28%)]	 Loss: 0.000028
Training epoch 41 [66560/67107 (99.24%)]	 Loss: 0.000041
Test set: Average Loss: 0.000291
Training epoch 42 [0/67107 (0.00%)]	 Loss: 0.000055
Training epoch 42 [640/67107 (0.95%)]	 Loss: 0.000013
Training epoch 42 [1280/67107 (1.91%)]	 Loss: 0.000016
Training epoch 42 [1920/67107 (2.86%)]	 Loss: 0.000009
Training epoch 42 [2560/67107 (3.82%)]	 Loss: 0.000010
Training epoch 42 [3200/67107 (4.77%)]	 Loss: 0.000003
Training epoch 42 [3840/67107 (5.73%)]	 Loss: 0.000008
Training epoch 42 [4480/67107 (6.68%)]	 Loss: 0.000009
Training epoch 42 [5120/67107 (7.63%)]	 Loss: 0.000050
Training epoch 42 [5760/67107 (8.59%)]	 Loss: 0.000124
Training epoch 42 [6400/67107 (9.54%)]	 Loss: 0.000016
Training epoch 42 [7040/67107 (10.50%)]	 Loss: 0.000004
Training epoch 42 [7680/67107 (11.45%)]	 Loss: 0.000021
Training epoch 42 [8320/67107 (12.40%)]	 Loss: 0.000002
Training epoch 42 [8960/67107 (13.36%)]	 Loss: 0.000055
Training epoch 42 [9600/67107 (14.31%)]	 Loss: 0.000011
Training epoch 42 [10240/67107 (15.27%)]	 Loss: 0.000007
Training epoch 42 [10880/67107 (16.22%)]	 Loss: 0.000119
Training epoch 42 [11520/67107 (17.18%)]	 Loss: 0.000015
Training epoch 42 [12160/67107 (18.13%)]	 Loss: 0.000029
Training epoch 42 [12800/67107 (19.08%)]	 Loss: 0.000027
Training epoch 42 [13440/67107 (20.04%)]	 Loss: 0.000118
Training epoch 42 [14080/67107 (20.99%)]	 Loss: 0.000022
Training epoch 42 [14720/67107 (21.95%)]	 Loss: 0.000019
Training epoch 42 [15360/67107 (22.90%)]	 Loss: 0.000024
Training epoch 42 [16000/67107 (23.85%)]	 Loss: 0.000013
Training epoch 42 [16640/67107 (24.81%)]	 Loss: 0.000036
Training epoch 42 [17280/67107 (25.76%)]	 Loss: 0.000018
Training epoch 42 [17920/67107 (26.72%)]	 Loss: 0.000028
Training epoch 42 [18560/67107 (27.67%)]	 Loss: 0.000010
Training epoch 42 [19200/67107 (28.63%)]	 Loss: 0.000012
Training epoch 42 [19840/67107 (29.58%)]	 Loss: 0.000005
Training epoch 42 [20480/67107 (30.53%)]	 Loss: 0.000016
Training epoch 42 [21120/67107 (31.49%)]	 Loss: 0.000066
Training epoch 42 [21760/67107 (32.44%)]	 Loss: 0.000014
Training epoch 42 [22400/67107 (33.40%)]	 Loss: 0.000032
Training epoch 42 [23040/67107 (34.35%)]	 Loss: 0.000014
Training epoch 42 [23680/67107 (35.31%)]	 Loss: 0.000017
Training epoch 42 [24320/67107 (36.26%)]	 Loss: 0.000007
Training epoch 42 [24960/67107 (37.21%)]	 Loss: 0.000009
Training epoch 42 [25600/67107 (38.17%)]	 Loss: 0.000010
Training epoch 42 [26240/67107 (39.12%)]	 Loss: 0.000005
Training epoch 42 [26880/67107 (40.08%)]	 Loss: 0.000006
Training epoch 42 [27520/67107 (41.03%)]	 Loss: 0.000016
Training epoch 42 [28160/67107 (41.98%)]	 Loss: 0.000029
Training epoch 42 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 42 [29440/67107 (43.89%)]	 Loss: 0.000082
Training epoch 42 [30080/67107 (44.85%)]	 Loss: 0.000009
Training epoch 42 [30720/67107 (45.80%)]	 Loss: 0.000037
Training epoch 42 [31360/67107 (46.76%)]	 Loss: 0.000013
Training epoch 42 [32000/67107 (47.71%)]	 Loss: 0.000012
Training epoch 42 [32640/67107 (48.66%)]	 Loss: 0.000014
Training epoch 42 [33280/67107 (49.62%)]	 Loss: 0.000018
Training epoch 42 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 42 [34560/67107 (51.53%)]	 Loss: 0.000010
Training epoch 42 [35200/67107 (52.48%)]	 Loss: 0.000023
Training epoch 42 [35840/67107 (53.44%)]	 Loss: 0.000008
Training epoch 42 [36480/67107 (54.39%)]	 Loss: 0.000022
Training epoch 42 [37120/67107 (55.34%)]	 Loss: 0.000014
Training epoch 42 [37760/67107 (56.30%)]	 Loss: 0.000007
Training epoch 42 [38400/67107 (57.25%)]	 Loss: 0.000010
Training epoch 42 [39040/67107 (58.21%)]	 Loss: 0.000004
Training epoch 42 [39680/67107 (59.16%)]	 Loss: 0.000015
Training epoch 42 [40320/67107 (60.11%)]	 Loss: 0.000003
Training epoch 42 [40960/67107 (61.07%)]	 Loss: 0.000007
Training epoch 42 [41600/67107 (62.02%)]	 Loss: 0.000015
Training epoch 42 [42240/67107 (62.98%)]	 Loss: 0.000019
Training epoch 42 [42880/67107 (63.93%)]	 Loss: 0.000022
Training epoch 42 [43520/67107 (64.89%)]	 Loss: 0.000008
Training epoch 42 [44160/67107 (65.84%)]	 Loss: 0.000012
Training epoch 42 [44800/67107 (66.79%)]	 Loss: 0.000018
Training epoch 42 [45440/67107 (67.75%)]	 Loss: 0.000025
Training epoch 42 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 42 [46720/67107 (69.66%)]	 Loss: 0.000006
Training epoch 42 [47360/67107 (70.61%)]	 Loss: 0.000024
Training epoch 42 [48000/67107 (71.56%)]	 Loss: 0.000014
Training epoch 42 [48640/67107 (72.52%)]	 Loss: 0.000013
Training epoch 42 [49280/67107 (73.47%)]	 Loss: 0.000004
Training epoch 42 [49920/67107 (74.43%)]	 Loss: 0.000023
Training epoch 42 [50560/67107 (75.38%)]	 Loss: 0.000020
Training epoch 42 [51200/67107 (76.34%)]	 Loss: 0.000014
Training epoch 42 [51840/67107 (77.29%)]	 Loss: 0.000003
Training epoch 42 [52480/67107 (78.24%)]	 Loss: 0.000008
Training epoch 42 [53120/67107 (79.20%)]	 Loss: 0.000003
Training epoch 42 [53760/67107 (80.15%)]	 Loss: 0.000067
Training epoch 42 [54400/67107 (81.11%)]	 Loss: 0.000013
Training epoch 42 [55040/67107 (82.06%)]	 Loss: 0.000010
Training epoch 42 [55680/67107 (83.02%)]	 Loss: 0.000016
Training epoch 42 [56320/67107 (83.97%)]	 Loss: 0.000006
Training epoch 42 [56960/67107 (84.92%)]	 Loss: 0.000066
Training epoch 42 [57600/67107 (85.88%)]	 Loss: 0.000057
Training epoch 42 [58240/67107 (86.83%)]	 Loss: 0.000041
Training epoch 42 [58880/67107 (87.79%)]	 Loss: 0.000005
Training epoch 42 [59520/67107 (88.74%)]	 Loss: 0.000008
Training epoch 42 [60160/67107 (89.69%)]	 Loss: 0.000027
Training epoch 42 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 42 [61440/67107 (91.60%)]	 Loss: 0.000013
Training epoch 42 [62080/67107 (92.56%)]	 Loss: 0.000016
Training epoch 42 [62720/67107 (93.51%)]	 Loss: 0.000025
Training epoch 42 [63360/67107 (94.47%)]	 Loss: 0.000015
Training epoch 42 [64000/67107 (95.42%)]	 Loss: 0.000064
Training epoch 42 [64640/67107 (96.37%)]	 Loss: 0.000014
Training epoch 42 [65280/67107 (97.33%)]	 Loss: 0.000146
Training epoch 42 [65920/67107 (98.28%)]	 Loss: 0.000021
Training epoch 42 [66560/67107 (99.24%)]	 Loss: 0.000027
Test set: Average Loss: 0.028026
Training epoch 43 [0/67107 (0.00%)]	 Loss: 0.000007
Training epoch 43 [640/67107 (0.95%)]	 Loss: 0.000008
Training epoch 43 [1280/67107 (1.91%)]	 Loss: 0.000020
Training epoch 43 [1920/67107 (2.86%)]	 Loss: 0.000220
Training epoch 43 [2560/67107 (3.82%)]	 Loss: 0.000115
Training epoch 43 [3200/67107 (4.77%)]	 Loss: 0.000051
Training epoch 43 [3840/67107 (5.73%)]	 Loss: 0.000018
Training epoch 43 [4480/67107 (6.68%)]	 Loss: 0.000347
Training epoch 43 [5120/67107 (7.63%)]	 Loss: 0.000029
Training epoch 43 [5760/67107 (8.59%)]	 Loss: 0.000021
Training epoch 43 [6400/67107 (9.54%)]	 Loss: 0.000019
Training epoch 43 [7040/67107 (10.50%)]	 Loss: 0.000006
Training epoch 43 [7680/67107 (11.45%)]	 Loss: 0.000007
Training epoch 43 [8320/67107 (12.40%)]	 Loss: 0.000023
Training epoch 43 [8960/67107 (13.36%)]	 Loss: 0.000552
Training epoch 43 [9600/67107 (14.31%)]	 Loss: 0.000110
Training epoch 43 [10240/67107 (15.27%)]	 Loss: 0.000142
Training epoch 43 [10880/67107 (16.22%)]	 Loss: 0.000713
Training epoch 43 [11520/67107 (17.18%)]	 Loss: 0.000098
Training epoch 43 [12160/67107 (18.13%)]	 Loss: 0.000072
Training epoch 43 [12800/67107 (19.08%)]	 Loss: 0.000019
Training epoch 43 [13440/67107 (20.04%)]	 Loss: 0.000202
Training epoch 43 [14080/67107 (20.99%)]	 Loss: 0.000078
Training epoch 43 [14720/67107 (21.95%)]	 Loss: 0.000030
Training epoch 43 [15360/67107 (22.90%)]	 Loss: 0.000021
Training epoch 43 [16000/67107 (23.85%)]	 Loss: 0.000096
Training epoch 43 [16640/67107 (24.81%)]	 Loss: 0.000098
Training epoch 43 [17280/67107 (25.76%)]	 Loss: 0.000045
Training epoch 43 [17920/67107 (26.72%)]	 Loss: 0.000298
Training epoch 43 [18560/67107 (27.67%)]	 Loss: 0.000044
Training epoch 43 [19200/67107 (28.63%)]	 Loss: 0.000055
Training epoch 43 [19840/67107 (29.58%)]	 Loss: 0.000084
Training epoch 43 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 43 [21120/67107 (31.49%)]	 Loss: 0.000024
Training epoch 43 [21760/67107 (32.44%)]	 Loss: 0.000015
Training epoch 43 [22400/67107 (33.40%)]	 Loss: 0.000022
Training epoch 43 [23040/67107 (34.35%)]	 Loss: 0.000159
Training epoch 43 [23680/67107 (35.31%)]	 Loss: 0.000018
Training epoch 43 [24320/67107 (36.26%)]	 Loss: 0.000069
Training epoch 43 [24960/67107 (37.21%)]	 Loss: 0.000091
Training epoch 43 [25600/67107 (38.17%)]	 Loss: 0.000049
Training epoch 43 [26240/67107 (39.12%)]	 Loss: 0.000048
Training epoch 43 [26880/67107 (40.08%)]	 Loss: 0.000015
Training epoch 43 [27520/67107 (41.03%)]	 Loss: 0.000017
Training epoch 43 [28160/67107 (41.98%)]	 Loss: 0.000006
Training epoch 43 [28800/67107 (42.94%)]	 Loss: 0.000063
Training epoch 43 [29440/67107 (43.89%)]	 Loss: 0.000021
Training epoch 43 [30080/67107 (44.85%)]	 Loss: 0.000015
Training epoch 43 [30720/67107 (45.80%)]	 Loss: 0.000010
Training epoch 43 [31360/67107 (46.76%)]	 Loss: 0.000825
Training epoch 43 [32000/67107 (47.71%)]	 Loss: 0.000016
Training epoch 43 [32640/67107 (48.66%)]	 Loss: 0.000045
Training epoch 43 [33280/67107 (49.62%)]	 Loss: 0.000052
Training epoch 43 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 43 [34560/67107 (51.53%)]	 Loss: 0.000016
Training epoch 43 [35200/67107 (52.48%)]	 Loss: 0.000076
Training epoch 43 [35840/67107 (53.44%)]	 Loss: 0.000007
Training epoch 43 [36480/67107 (54.39%)]	 Loss: 0.000006
Training epoch 43 [37120/67107 (55.34%)]	 Loss: 0.000013
Training epoch 43 [37760/67107 (56.30%)]	 Loss: 0.000202
Training epoch 43 [38400/67107 (57.25%)]	 Loss: 0.000013
Training epoch 43 [39040/67107 (58.21%)]	 Loss: 0.000088
Training epoch 43 [39680/67107 (59.16%)]	 Loss: 0.000049
Training epoch 43 [40320/67107 (60.11%)]	 Loss: 0.000128
Training epoch 43 [40960/67107 (61.07%)]	 Loss: 0.000012
Training epoch 43 [41600/67107 (62.02%)]	 Loss: 0.000053
Training epoch 43 [42240/67107 (62.98%)]	 Loss: 0.000232
Training epoch 43 [42880/67107 (63.93%)]	 Loss: 0.000010
Training epoch 43 [43520/67107 (64.89%)]	 Loss: 0.000016
Training epoch 43 [44160/67107 (65.84%)]	 Loss: 0.000010
Training epoch 43 [44800/67107 (66.79%)]	 Loss: 0.000016
Training epoch 43 [45440/67107 (67.75%)]	 Loss: 0.000015
Training epoch 43 [46080/67107 (68.70%)]	 Loss: 0.000009
Training epoch 43 [46720/67107 (69.66%)]	 Loss: 0.000006
Training epoch 43 [47360/67107 (70.61%)]	 Loss: 0.000060
Training epoch 43 [48000/67107 (71.56%)]	 Loss: 0.000005
Training epoch 43 [48640/67107 (72.52%)]	 Loss: 0.000016
Training epoch 43 [49280/67107 (73.47%)]	 Loss: 0.000029
Training epoch 43 [49920/67107 (74.43%)]	 Loss: 0.000019
Training epoch 43 [50560/67107 (75.38%)]	 Loss: 0.000013
Training epoch 43 [51200/67107 (76.34%)]	 Loss: 0.000006
Training epoch 43 [51840/67107 (77.29%)]	 Loss: 0.000070
Training epoch 43 [52480/67107 (78.24%)]	 Loss: 0.000031
Training epoch 43 [53120/67107 (79.20%)]	 Loss: 0.000122
Training epoch 43 [53760/67107 (80.15%)]	 Loss: 0.000012
Training epoch 43 [54400/67107 (81.11%)]	 Loss: 0.000029
Training epoch 43 [55040/67107 (82.06%)]	 Loss: 0.000012
Training epoch 43 [55680/67107 (83.02%)]	 Loss: 0.000030
Training epoch 43 [56320/67107 (83.97%)]	 Loss: 0.000368
Training epoch 43 [56960/67107 (84.92%)]	 Loss: 0.000013
Training epoch 43 [57600/67107 (85.88%)]	 Loss: 0.000015
Training epoch 43 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 43 [58880/67107 (87.79%)]	 Loss: 0.000011
Training epoch 43 [59520/67107 (88.74%)]	 Loss: 0.000046
Training epoch 43 [60160/67107 (89.69%)]	 Loss: 0.000021
Training epoch 43 [60800/67107 (90.65%)]	 Loss: 0.000022
Training epoch 43 [61440/67107 (91.60%)]	 Loss: 0.000025
Training epoch 43 [62080/67107 (92.56%)]	 Loss: 0.000015
Training epoch 43 [62720/67107 (93.51%)]	 Loss: 0.000009
Training epoch 43 [63360/67107 (94.47%)]	 Loss: 0.000032
Training epoch 43 [64000/67107 (95.42%)]	 Loss: 0.000048
Training epoch 43 [64640/67107 (96.37%)]	 Loss: 0.000051
Training epoch 43 [65280/67107 (97.33%)]	 Loss: 0.000011
Training epoch 43 [65920/67107 (98.28%)]	 Loss: 0.000023
Training epoch 43 [66560/67107 (99.24%)]	 Loss: 0.000030
Test set: Average Loss: 0.018520
Training epoch 44 [0/67107 (0.00%)]	 Loss: 0.000103
Training epoch 44 [640/67107 (0.95%)]	 Loss: 0.000062
Training epoch 44 [1280/67107 (1.91%)]	 Loss: 0.000040
Training epoch 44 [1920/67107 (2.86%)]	 Loss: 0.000004
Training epoch 44 [2560/67107 (3.82%)]	 Loss: 0.000047
Training epoch 44 [3200/67107 (4.77%)]	 Loss: 0.000014
Training epoch 44 [3840/67107 (5.73%)]	 Loss: 0.000015
Training epoch 44 [4480/67107 (6.68%)]	 Loss: 0.000019
Training epoch 44 [5120/67107 (7.63%)]	 Loss: 0.000013
Training epoch 44 [5760/67107 (8.59%)]	 Loss: 0.000005
Training epoch 44 [6400/67107 (9.54%)]	 Loss: 0.000124
Training epoch 44 [7040/67107 (10.50%)]	 Loss: 0.000038
Training epoch 44 [7680/67107 (11.45%)]	 Loss: 0.000076
Training epoch 44 [8320/67107 (12.40%)]	 Loss: 0.000023
Training epoch 44 [8960/67107 (13.36%)]	 Loss: 0.000031
Training epoch 44 [9600/67107 (14.31%)]	 Loss: 0.000008
Training epoch 44 [10240/67107 (15.27%)]	 Loss: 0.000005
Training epoch 44 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 44 [11520/67107 (17.18%)]	 Loss: 0.000002
Training epoch 44 [12160/67107 (18.13%)]	 Loss: 0.000003
Training epoch 44 [12800/67107 (19.08%)]	 Loss: 0.000013
Training epoch 44 [13440/67107 (20.04%)]	 Loss: 0.000005
Training epoch 44 [14080/67107 (20.99%)]	 Loss: 0.000009
Training epoch 44 [14720/67107 (21.95%)]	 Loss: 0.000005
Training epoch 44 [15360/67107 (22.90%)]	 Loss: 0.000080
Training epoch 44 [16000/67107 (23.85%)]	 Loss: 0.000023
Training epoch 44 [16640/67107 (24.81%)]	 Loss: 0.000081
Training epoch 44 [17280/67107 (25.76%)]	 Loss: 0.000041
Training epoch 44 [17920/67107 (26.72%)]	 Loss: 0.000011
Training epoch 44 [18560/67107 (27.67%)]	 Loss: 0.000012
Training epoch 44 [19200/67107 (28.63%)]	 Loss: 0.000004
Training epoch 44 [19840/67107 (29.58%)]	 Loss: 0.000004
Training epoch 44 [20480/67107 (30.53%)]	 Loss: 0.000112
Training epoch 44 [21120/67107 (31.49%)]	 Loss: 0.000032
Training epoch 44 [21760/67107 (32.44%)]	 Loss: 0.000006
Training epoch 44 [22400/67107 (33.40%)]	 Loss: 0.000027
Training epoch 44 [23040/67107 (34.35%)]	 Loss: 0.000030
Training epoch 44 [23680/67107 (35.31%)]	 Loss: 0.000024
Training epoch 44 [24320/67107 (36.26%)]	 Loss: 0.000039
Training epoch 44 [24960/67107 (37.21%)]	 Loss: 0.000019
Training epoch 44 [25600/67107 (38.17%)]	 Loss: 0.000024
Training epoch 44 [26240/67107 (39.12%)]	 Loss: 0.000028
Training epoch 44 [26880/67107 (40.08%)]	 Loss: 0.000007
Training epoch 44 [27520/67107 (41.03%)]	 Loss: 0.000007
Training epoch 44 [28160/67107 (41.98%)]	 Loss: 0.000006
Training epoch 44 [28800/67107 (42.94%)]	 Loss: 0.000084
Training epoch 44 [29440/67107 (43.89%)]	 Loss: 0.000062
Training epoch 44 [30080/67107 (44.85%)]	 Loss: 0.000074
Training epoch 44 [30720/67107 (45.80%)]	 Loss: 0.000024
Training epoch 44 [31360/67107 (46.76%)]	 Loss: 0.000009
Training epoch 44 [32000/67107 (47.71%)]	 Loss: 0.000028
Training epoch 44 [32640/67107 (48.66%)]	 Loss: 0.000013
Training epoch 44 [33280/67107 (49.62%)]	 Loss: 0.000010
Training epoch 44 [33920/67107 (50.57%)]	 Loss: 0.000008
Training epoch 44 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 44 [35200/67107 (52.48%)]	 Loss: 0.000004
Training epoch 44 [35840/67107 (53.44%)]	 Loss: 0.000011
Training epoch 44 [36480/67107 (54.39%)]	 Loss: 0.000014
Training epoch 44 [37120/67107 (55.34%)]	 Loss: 0.000018
Training epoch 44 [37760/67107 (56.30%)]	 Loss: 0.000143
Training epoch 44 [38400/67107 (57.25%)]	 Loss: 0.000034
Training epoch 44 [39040/67107 (58.21%)]	 Loss: 0.000008
Training epoch 44 [39680/67107 (59.16%)]	 Loss: 0.000070
Training epoch 44 [40320/67107 (60.11%)]	 Loss: 0.000029
Training epoch 44 [40960/67107 (61.07%)]	 Loss: 0.000008
Training epoch 44 [41600/67107 (62.02%)]	 Loss: 0.000012
Training epoch 44 [42240/67107 (62.98%)]	 Loss: 0.000010
Training epoch 44 [42880/67107 (63.93%)]	 Loss: 0.000004
Training epoch 44 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 44 [44160/67107 (65.84%)]	 Loss: 0.000028
Training epoch 44 [44800/67107 (66.79%)]	 Loss: 0.000033
Training epoch 44 [45440/67107 (67.75%)]	 Loss: 0.000019
Training epoch 44 [46080/67107 (68.70%)]	 Loss: 0.000051
Training epoch 44 [46720/67107 (69.66%)]	 Loss: 0.000012
Training epoch 44 [47360/67107 (70.61%)]	 Loss: 0.000040
Training epoch 44 [48000/67107 (71.56%)]	 Loss: 0.000088
Training epoch 44 [48640/67107 (72.52%)]	 Loss: 0.000022
Training epoch 44 [49280/67107 (73.47%)]	 Loss: 0.000022
Training epoch 44 [49920/67107 (74.43%)]	 Loss: 0.000021
Training epoch 44 [50560/67107 (75.38%)]	 Loss: 0.000015
Training epoch 44 [51200/67107 (76.34%)]	 Loss: 0.000036
Training epoch 44 [51840/67107 (77.29%)]	 Loss: 0.000070
Training epoch 44 [52480/67107 (78.24%)]	 Loss: 0.000019
Training epoch 44 [53120/67107 (79.20%)]	 Loss: 0.000006
Training epoch 44 [53760/67107 (80.15%)]	 Loss: 0.000015
Training epoch 44 [54400/67107 (81.11%)]	 Loss: 0.000007
Training epoch 44 [55040/67107 (82.06%)]	 Loss: 0.000002
Training epoch 44 [55680/67107 (83.02%)]	 Loss: 0.000057
Training epoch 44 [56320/67107 (83.97%)]	 Loss: 0.000006
Training epoch 44 [56960/67107 (84.92%)]	 Loss: 0.000010
Training epoch 44 [57600/67107 (85.88%)]	 Loss: 0.000071
Training epoch 44 [58240/67107 (86.83%)]	 Loss: 0.000021
Training epoch 44 [58880/67107 (87.79%)]	 Loss: 0.000031
Training epoch 44 [59520/67107 (88.74%)]	 Loss: 0.000027
Training epoch 44 [60160/67107 (89.69%)]	 Loss: 0.000013
Training epoch 44 [60800/67107 (90.65%)]	 Loss: 0.000036
Training epoch 44 [61440/67107 (91.60%)]	 Loss: 0.000006
Training epoch 44 [62080/67107 (92.56%)]	 Loss: 0.000011
Training epoch 44 [62720/67107 (93.51%)]	 Loss: 0.000055
Training epoch 44 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 44 [64000/67107 (95.42%)]	 Loss: 0.000004
Training epoch 44 [64640/67107 (96.37%)]	 Loss: 0.000005
Training epoch 44 [65280/67107 (97.33%)]	 Loss: 0.000017
Training epoch 44 [65920/67107 (98.28%)]	 Loss: 0.000023
Training epoch 44 [66560/67107 (99.24%)]	 Loss: 0.000064
Test set: Average Loss: 0.002131
Training epoch 45 [0/67107 (0.00%)]	 Loss: 0.000026
Training epoch 45 [640/67107 (0.95%)]	 Loss: 0.000014
Training epoch 45 [1280/67107 (1.91%)]	 Loss: 0.000015
Training epoch 45 [1920/67107 (2.86%)]	 Loss: 0.000018
Training epoch 45 [2560/67107 (3.82%)]	 Loss: 0.000006
Training epoch 45 [3200/67107 (4.77%)]	 Loss: 0.000002
Training epoch 45 [3840/67107 (5.73%)]	 Loss: 0.000039
Training epoch 45 [4480/67107 (6.68%)]	 Loss: 0.000009
Training epoch 45 [5120/67107 (7.63%)]	 Loss: 0.000015
Training epoch 45 [5760/67107 (8.59%)]	 Loss: 0.000015
Training epoch 45 [6400/67107 (9.54%)]	 Loss: 0.000022
Training epoch 45 [7040/67107 (10.50%)]	 Loss: 0.000027
Training epoch 45 [7680/67107 (11.45%)]	 Loss: 0.000022
Training epoch 45 [8320/67107 (12.40%)]	 Loss: 0.000005
Training epoch 45 [8960/67107 (13.36%)]	 Loss: 0.000001
Training epoch 45 [9600/67107 (14.31%)]	 Loss: 0.000010
Training epoch 45 [10240/67107 (15.27%)]	 Loss: 0.000007
Training epoch 45 [10880/67107 (16.22%)]	 Loss: 0.000012
Training epoch 45 [11520/67107 (17.18%)]	 Loss: 0.000017
Training epoch 45 [12160/67107 (18.13%)]	 Loss: 0.000016
Training epoch 45 [12800/67107 (19.08%)]	 Loss: 0.000019
Training epoch 45 [13440/67107 (20.04%)]	 Loss: 0.000007
Training epoch 45 [14080/67107 (20.99%)]	 Loss: 0.000048
Training epoch 45 [14720/67107 (21.95%)]	 Loss: 0.000009
Training epoch 45 [15360/67107 (22.90%)]	 Loss: 0.000023
Training epoch 45 [16000/67107 (23.85%)]	 Loss: 0.000009
Training epoch 45 [16640/67107 (24.81%)]	 Loss: 0.000011
Training epoch 45 [17280/67107 (25.76%)]	 Loss: 0.000005
Training epoch 45 [17920/67107 (26.72%)]	 Loss: 0.000003
Training epoch 45 [18560/67107 (27.67%)]	 Loss: 0.000022
Training epoch 45 [19200/67107 (28.63%)]	 Loss: 0.000020
Training epoch 45 [19840/67107 (29.58%)]	 Loss: 0.000004
Training epoch 45 [20480/67107 (30.53%)]	 Loss: 0.000012
Training epoch 45 [21120/67107 (31.49%)]	 Loss: 0.000021
Training epoch 45 [21760/67107 (32.44%)]	 Loss: 0.000009
Training epoch 45 [22400/67107 (33.40%)]	 Loss: 0.000027
Training epoch 45 [23040/67107 (34.35%)]	 Loss: 0.000023
Training epoch 45 [23680/67107 (35.31%)]	 Loss: 0.000012
Training epoch 45 [24320/67107 (36.26%)]	 Loss: 0.000057
Training epoch 45 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 45 [25600/67107 (38.17%)]	 Loss: 0.000059
Training epoch 45 [26240/67107 (39.12%)]	 Loss: 0.000012
Training epoch 45 [26880/67107 (40.08%)]	 Loss: 0.000036
Training epoch 45 [27520/67107 (41.03%)]	 Loss: 0.000017
Training epoch 45 [28160/67107 (41.98%)]	 Loss: 0.000007
Training epoch 45 [28800/67107 (42.94%)]	 Loss: 0.000013
Training epoch 45 [29440/67107 (43.89%)]	 Loss: 0.000012
Training epoch 45 [30080/67107 (44.85%)]	 Loss: 0.000007
Training epoch 45 [30720/67107 (45.80%)]	 Loss: 0.000005
Training epoch 45 [31360/67107 (46.76%)]	 Loss: 0.000005
Training epoch 45 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 45 [32640/67107 (48.66%)]	 Loss: 0.000013
Training epoch 45 [33280/67107 (49.62%)]	 Loss: 0.000024
Training epoch 45 [33920/67107 (50.57%)]	 Loss: 0.000011
Training epoch 45 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 45 [35200/67107 (52.48%)]	 Loss: 0.000037
Training epoch 45 [35840/67107 (53.44%)]	 Loss: 0.000009
Training epoch 45 [36480/67107 (54.39%)]	 Loss: 0.000053
Training epoch 45 [37120/67107 (55.34%)]	 Loss: 0.000014
Training epoch 45 [37760/67107 (56.30%)]	 Loss: 0.000009
Training epoch 45 [38400/67107 (57.25%)]	 Loss: 0.000003
Training epoch 45 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 45 [39680/67107 (59.16%)]	 Loss: 0.000008
Training epoch 45 [40320/67107 (60.11%)]	 Loss: 0.000019
Training epoch 45 [40960/67107 (61.07%)]	 Loss: 0.000021
Training epoch 45 [41600/67107 (62.02%)]	 Loss: 0.000172
Training epoch 45 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 45 [42880/67107 (63.93%)]	 Loss: 0.000005
Training epoch 45 [43520/67107 (64.89%)]	 Loss: 0.000015
Training epoch 45 [44160/67107 (65.84%)]	 Loss: 0.000039
Training epoch 45 [44800/67107 (66.79%)]	 Loss: 0.000004
Training epoch 45 [45440/67107 (67.75%)]	 Loss: 0.000004
Training epoch 45 [46080/67107 (68.70%)]	 Loss: 0.000006
Training epoch 45 [46720/67107 (69.66%)]	 Loss: 0.000001
Training epoch 45 [47360/67107 (70.61%)]	 Loss: 0.000010
Training epoch 45 [48000/67107 (71.56%)]	 Loss: 0.000015
Training epoch 45 [48640/67107 (72.52%)]	 Loss: 0.000034
Training epoch 45 [49280/67107 (73.47%)]	 Loss: 0.000041
Training epoch 45 [49920/67107 (74.43%)]	 Loss: 0.000007
Training epoch 45 [50560/67107 (75.38%)]	 Loss: 0.000019
Training epoch 45 [51200/67107 (76.34%)]	 Loss: 0.000013
Training epoch 45 [51840/67107 (77.29%)]	 Loss: 0.000016
Training epoch 45 [52480/67107 (78.24%)]	 Loss: 0.000011
Training epoch 45 [53120/67107 (79.20%)]	 Loss: 0.000011
Training epoch 45 [53760/67107 (80.15%)]	 Loss: 0.000003
Training epoch 45 [54400/67107 (81.11%)]	 Loss: 0.000021
Training epoch 45 [55040/67107 (82.06%)]	 Loss: 0.000060
Training epoch 45 [55680/67107 (83.02%)]	 Loss: 0.000008
Training epoch 45 [56320/67107 (83.97%)]	 Loss: 0.000014
Training epoch 45 [56960/67107 (84.92%)]	 Loss: 0.000019
Training epoch 45 [57600/67107 (85.88%)]	 Loss: 0.000020
Training epoch 45 [58240/67107 (86.83%)]	 Loss: 0.000023
Training epoch 45 [58880/67107 (87.79%)]	 Loss: 0.000011
Training epoch 45 [59520/67107 (88.74%)]	 Loss: 0.000022
Training epoch 45 [60160/67107 (89.69%)]	 Loss: 0.000010
Training epoch 45 [60800/67107 (90.65%)]	 Loss: 0.000006
Training epoch 45 [61440/67107 (91.60%)]	 Loss: 0.000003
Training epoch 45 [62080/67107 (92.56%)]	 Loss: 0.000007
Training epoch 45 [62720/67107 (93.51%)]	 Loss: 0.000008
Training epoch 45 [63360/67107 (94.47%)]	 Loss: 0.000011
Training epoch 45 [64000/67107 (95.42%)]	 Loss: 0.000021
Training epoch 45 [64640/67107 (96.37%)]	 Loss: 0.000014
Training epoch 45 [65280/67107 (97.33%)]	 Loss: 0.000013
Training epoch 45 [65920/67107 (98.28%)]	 Loss: 0.000032
Training epoch 45 [66560/67107 (99.24%)]	 Loss: 0.000017
Test set: Average Loss: 0.000139
Training epoch 46 [0/67107 (0.00%)]	 Loss: 0.000024
Training epoch 46 [640/67107 (0.95%)]	 Loss: 0.000058
Training epoch 46 [1280/67107 (1.91%)]	 Loss: 0.000017
Training epoch 46 [1920/67107 (2.86%)]	 Loss: 0.000015
Training epoch 46 [2560/67107 (3.82%)]	 Loss: 0.000006
Training epoch 46 [3200/67107 (4.77%)]	 Loss: 0.000013
Training epoch 46 [3840/67107 (5.73%)]	 Loss: 0.000077
Training epoch 46 [4480/67107 (6.68%)]	 Loss: 0.000017
Training epoch 46 [5120/67107 (7.63%)]	 Loss: 0.000004
Training epoch 46 [5760/67107 (8.59%)]	 Loss: 0.000007
Training epoch 46 [6400/67107 (9.54%)]	 Loss: 0.000005
Training epoch 46 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 46 [7680/67107 (11.45%)]	 Loss: 0.000011
Training epoch 46 [8320/67107 (12.40%)]	 Loss: 0.000004
Training epoch 46 [8960/67107 (13.36%)]	 Loss: 0.000010
Training epoch 46 [9600/67107 (14.31%)]	 Loss: 0.000008
Training epoch 46 [10240/67107 (15.27%)]	 Loss: 0.000026
Training epoch 46 [10880/67107 (16.22%)]	 Loss: 0.000008
Training epoch 46 [11520/67107 (17.18%)]	 Loss: 0.000019
Training epoch 46 [12160/67107 (18.13%)]	 Loss: 0.000040
Training epoch 46 [12800/67107 (19.08%)]	 Loss: 0.000028
Training epoch 46 [13440/67107 (20.04%)]	 Loss: 0.000013
Training epoch 46 [14080/67107 (20.99%)]	 Loss: 0.000031
Training epoch 46 [14720/67107 (21.95%)]	 Loss: 0.000017
Training epoch 46 [15360/67107 (22.90%)]	 Loss: 0.000009
Training epoch 46 [16000/67107 (23.85%)]	 Loss: 0.000063
Training epoch 46 [16640/67107 (24.81%)]	 Loss: 0.000011
Training epoch 46 [17280/67107 (25.76%)]	 Loss: 0.000026
Training epoch 46 [17920/67107 (26.72%)]	 Loss: 0.000020
Training epoch 46 [18560/67107 (27.67%)]	 Loss: 0.000012
Training epoch 46 [19200/67107 (28.63%)]	 Loss: 0.000016
Training epoch 46 [19840/67107 (29.58%)]	 Loss: 0.000005
Training epoch 46 [20480/67107 (30.53%)]	 Loss: 0.000004
Training epoch 46 [21120/67107 (31.49%)]	 Loss: 0.000003
Training epoch 46 [21760/67107 (32.44%)]	 Loss: 0.000020
Training epoch 46 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 46 [23040/67107 (34.35%)]	 Loss: 0.000003
Training epoch 46 [23680/67107 (35.31%)]	 Loss: 0.000011
Training epoch 46 [24320/67107 (36.26%)]	 Loss: 0.000013
Training epoch 46 [24960/67107 (37.21%)]	 Loss: 0.000009
Training epoch 46 [25600/67107 (38.17%)]	 Loss: 0.000060
Training epoch 46 [26240/67107 (39.12%)]	 Loss: 0.000016
Training epoch 46 [26880/67107 (40.08%)]	 Loss: 0.000024
Training epoch 46 [27520/67107 (41.03%)]	 Loss: 0.000012
Training epoch 46 [28160/67107 (41.98%)]	 Loss: 0.000008
Training epoch 46 [28800/67107 (42.94%)]	 Loss: 0.000026
Training epoch 46 [29440/67107 (43.89%)]	 Loss: 0.000009
Training epoch 46 [30080/67107 (44.85%)]	 Loss: 0.000017
Training epoch 46 [30720/67107 (45.80%)]	 Loss: 0.000011
Training epoch 46 [31360/67107 (46.76%)]	 Loss: 0.000048
Training epoch 46 [32000/67107 (47.71%)]	 Loss: 0.000009
Training epoch 46 [32640/67107 (48.66%)]	 Loss: 0.000028
Training epoch 46 [33280/67107 (49.62%)]	 Loss: 0.000005
Training epoch 46 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 46 [34560/67107 (51.53%)]	 Loss: 0.000008
Training epoch 46 [35200/67107 (52.48%)]	 Loss: 0.000009
Training epoch 46 [35840/67107 (53.44%)]	 Loss: 0.000006
Training epoch 46 [36480/67107 (54.39%)]	 Loss: 0.000004
Training epoch 46 [37120/67107 (55.34%)]	 Loss: 0.000045
Training epoch 46 [37760/67107 (56.30%)]	 Loss: 0.000007
Training epoch 46 [38400/67107 (57.25%)]	 Loss: 0.000006
Training epoch 46 [39040/67107 (58.21%)]	 Loss: 0.000016
Training epoch 46 [39680/67107 (59.16%)]	 Loss: 0.000011
Training epoch 46 [40320/67107 (60.11%)]	 Loss: 0.000036
Training epoch 46 [40960/67107 (61.07%)]	 Loss: 0.000013
Training epoch 46 [41600/67107 (62.02%)]	 Loss: 0.000012
Training epoch 46 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 46 [42880/67107 (63.93%)]	 Loss: 0.000018
Training epoch 46 [43520/67107 (64.89%)]	 Loss: 0.000008
Training epoch 46 [44160/67107 (65.84%)]	 Loss: 0.000009
Training epoch 46 [44800/67107 (66.79%)]	 Loss: 0.000056
Training epoch 46 [45440/67107 (67.75%)]	 Loss: 0.000009
Training epoch 46 [46080/67107 (68.70%)]	 Loss: 0.000063
Training epoch 46 [46720/67107 (69.66%)]	 Loss: 0.000019
Training epoch 46 [47360/67107 (70.61%)]	 Loss: 0.000017
Training epoch 46 [48000/67107 (71.56%)]	 Loss: 0.000008
Training epoch 46 [48640/67107 (72.52%)]	 Loss: 0.000006
Training epoch 46 [49280/67107 (73.47%)]	 Loss: 0.000005
Training epoch 46 [49920/67107 (74.43%)]	 Loss: 0.000016
Training epoch 46 [50560/67107 (75.38%)]	 Loss: 0.000010
Training epoch 46 [51200/67107 (76.34%)]	 Loss: 0.000097
Training epoch 46 [51840/67107 (77.29%)]	 Loss: 0.000019
Training epoch 46 [52480/67107 (78.24%)]	 Loss: 0.000008
Training epoch 46 [53120/67107 (79.20%)]	 Loss: 0.000008
Training epoch 46 [53760/67107 (80.15%)]	 Loss: 0.000004
Training epoch 46 [54400/67107 (81.11%)]	 Loss: 0.000008
Training epoch 46 [55040/67107 (82.06%)]	 Loss: 0.000053
Training epoch 46 [55680/67107 (83.02%)]	 Loss: 0.000119
Training epoch 46 [56320/67107 (83.97%)]	 Loss: 0.000013
Training epoch 46 [56960/67107 (84.92%)]	 Loss: 0.000033
Training epoch 46 [57600/67107 (85.88%)]	 Loss: 0.000015
Training epoch 46 [58240/67107 (86.83%)]	 Loss: 0.000010
Training epoch 46 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 46 [59520/67107 (88.74%)]	 Loss: 0.000004
Training epoch 46 [60160/67107 (89.69%)]	 Loss: 0.000003
Training epoch 46 [60800/67107 (90.65%)]	 Loss: 0.000027
Training epoch 46 [61440/67107 (91.60%)]	 Loss: 0.000019
Training epoch 46 [62080/67107 (92.56%)]	 Loss: 0.000039
Training epoch 46 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 46 [63360/67107 (94.47%)]	 Loss: 0.000001
Training epoch 46 [64000/67107 (95.42%)]	 Loss: 0.000048
Training epoch 46 [64640/67107 (96.37%)]	 Loss: 0.000025
Training epoch 46 [65280/67107 (97.33%)]	 Loss: 0.000009
Training epoch 46 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 46 [66560/67107 (99.24%)]	 Loss: 0.000010
Test set: Average Loss: 0.001590
Training epoch 47 [0/67107 (0.00%)]	 Loss: 0.000005
Training epoch 47 [640/67107 (0.95%)]	 Loss: 0.000013
Training epoch 47 [1280/67107 (1.91%)]	 Loss: 0.000007
Training epoch 47 [1920/67107 (2.86%)]	 Loss: 0.000018
Training epoch 47 [2560/67107 (3.82%)]	 Loss: 0.000022
Training epoch 47 [3200/67107 (4.77%)]	 Loss: 0.000013
Training epoch 47 [3840/67107 (5.73%)]	 Loss: 0.000032
Training epoch 47 [4480/67107 (6.68%)]	 Loss: 0.000007
Training epoch 47 [5120/67107 (7.63%)]	 Loss: 0.000008
Training epoch 47 [5760/67107 (8.59%)]	 Loss: 0.000011
Training epoch 47 [6400/67107 (9.54%)]	 Loss: 0.000015
Training epoch 47 [7040/67107 (10.50%)]	 Loss: 0.000015
Training epoch 47 [7680/67107 (11.45%)]	 Loss: 0.000010
Training epoch 47 [8320/67107 (12.40%)]	 Loss: 0.000006
Training epoch 47 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 47 [9600/67107 (14.31%)]	 Loss: 0.000012
Training epoch 47 [10240/67107 (15.27%)]	 Loss: 0.000003
Training epoch 47 [10880/67107 (16.22%)]	 Loss: 0.000025
Training epoch 47 [11520/67107 (17.18%)]	 Loss: 0.000042
Training epoch 47 [12160/67107 (18.13%)]	 Loss: 0.000026
Training epoch 47 [12800/67107 (19.08%)]	 Loss: 0.000014
Training epoch 47 [13440/67107 (20.04%)]	 Loss: 0.000007
Training epoch 47 [14080/67107 (20.99%)]	 Loss: 0.000010
Training epoch 47 [14720/67107 (21.95%)]	 Loss: 0.000005
Training epoch 47 [15360/67107 (22.90%)]	 Loss: 0.000006
Training epoch 47 [16000/67107 (23.85%)]	 Loss: 0.000001
Training epoch 47 [16640/67107 (24.81%)]	 Loss: 0.000005
Training epoch 47 [17280/67107 (25.76%)]	 Loss: 0.000012
Training epoch 47 [17920/67107 (26.72%)]	 Loss: 0.000012
Training epoch 47 [18560/67107 (27.67%)]	 Loss: 0.000050
Training epoch 47 [19200/67107 (28.63%)]	 Loss: 0.000009
Training epoch 47 [19840/67107 (29.58%)]	 Loss: 0.000009
Training epoch 47 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 47 [21120/67107 (31.49%)]	 Loss: 0.000014
Training epoch 47 [21760/67107 (32.44%)]	 Loss: 0.000012
Training epoch 47 [22400/67107 (33.40%)]	 Loss: 0.000008
Training epoch 47 [23040/67107 (34.35%)]	 Loss: 0.000033
Training epoch 47 [23680/67107 (35.31%)]	 Loss: 0.000025
Training epoch 47 [24320/67107 (36.26%)]	 Loss: 0.000012
Training epoch 47 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 47 [25600/67107 (38.17%)]	 Loss: 0.000012
Training epoch 47 [26240/67107 (39.12%)]	 Loss: 0.000012
Training epoch 47 [26880/67107 (40.08%)]	 Loss: 0.000004
Training epoch 47 [27520/67107 (41.03%)]	 Loss: 0.000008
Training epoch 47 [28160/67107 (41.98%)]	 Loss: 0.000028
Training epoch 47 [28800/67107 (42.94%)]	 Loss: 0.000002
Training epoch 47 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 47 [30080/67107 (44.85%)]	 Loss: 0.000013
Training epoch 47 [30720/67107 (45.80%)]	 Loss: 0.000035
Training epoch 47 [31360/67107 (46.76%)]	 Loss: 0.000090
Training epoch 47 [32000/67107 (47.71%)]	 Loss: 0.000004
Training epoch 47 [32640/67107 (48.66%)]	 Loss: 0.000009
Training epoch 47 [33280/67107 (49.62%)]	 Loss: 0.000030
Training epoch 47 [33920/67107 (50.57%)]	 Loss: 0.000003
Training epoch 47 [34560/67107 (51.53%)]	 Loss: 0.000022
Training epoch 47 [35200/67107 (52.48%)]	 Loss: 0.000011
Training epoch 47 [35840/67107 (53.44%)]	 Loss: 0.000007
Training epoch 47 [36480/67107 (54.39%)]	 Loss: 0.000002
Training epoch 47 [37120/67107 (55.34%)]	 Loss: 0.000013
Training epoch 47 [37760/67107 (56.30%)]	 Loss: 0.000019
Training epoch 47 [38400/67107 (57.25%)]	 Loss: 0.000022
Training epoch 47 [39040/67107 (58.21%)]	 Loss: 0.000039
Training epoch 47 [39680/67107 (59.16%)]	 Loss: 0.000027
Training epoch 47 [40320/67107 (60.11%)]	 Loss: 0.000012
Training epoch 47 [40960/67107 (61.07%)]	 Loss: 0.000005
Training epoch 47 [41600/67107 (62.02%)]	 Loss: 0.000006
Training epoch 47 [42240/67107 (62.98%)]	 Loss: 0.000007
Training epoch 47 [42880/67107 (63.93%)]	 Loss: 0.000010
Training epoch 47 [43520/67107 (64.89%)]	 Loss: 0.000111
Training epoch 47 [44160/67107 (65.84%)]	 Loss: 0.000067
Training epoch 47 [44800/67107 (66.79%)]	 Loss: 0.000058
Training epoch 47 [45440/67107 (67.75%)]	 Loss: 0.000055
Training epoch 47 [46080/67107 (68.70%)]	 Loss: 0.000037
Training epoch 47 [46720/67107 (69.66%)]	 Loss: 0.000023
Training epoch 47 [47360/67107 (70.61%)]	 Loss: 0.000028
Training epoch 47 [48000/67107 (71.56%)]	 Loss: 0.000006
Training epoch 47 [48640/67107 (72.52%)]	 Loss: 0.000008
Training epoch 47 [49280/67107 (73.47%)]	 Loss: 0.000023
Training epoch 47 [49920/67107 (74.43%)]	 Loss: 0.000019
Training epoch 47 [50560/67107 (75.38%)]	 Loss: 0.000021
Training epoch 47 [51200/67107 (76.34%)]	 Loss: 0.000019
Training epoch 47 [51840/67107 (77.29%)]	 Loss: 0.000072
Training epoch 47 [52480/67107 (78.24%)]	 Loss: 0.000012
Training epoch 47 [53120/67107 (79.20%)]	 Loss: 0.000014
Training epoch 47 [53760/67107 (80.15%)]	 Loss: 0.000006
Training epoch 47 [54400/67107 (81.11%)]	 Loss: 0.000012
Training epoch 47 [55040/67107 (82.06%)]	 Loss: 0.000006
Training epoch 47 [55680/67107 (83.02%)]	 Loss: 0.000009
Training epoch 47 [56320/67107 (83.97%)]	 Loss: 0.000007
Training epoch 47 [56960/67107 (84.92%)]	 Loss: 0.000029
Training epoch 47 [57600/67107 (85.88%)]	 Loss: 0.000018
Training epoch 47 [58240/67107 (86.83%)]	 Loss: 0.000010
Training epoch 47 [58880/67107 (87.79%)]	 Loss: 0.000010
Training epoch 47 [59520/67107 (88.74%)]	 Loss: 0.000010
Training epoch 47 [60160/67107 (89.69%)]	 Loss: 0.000021
Training epoch 47 [60800/67107 (90.65%)]	 Loss: 0.000004
Training epoch 47 [61440/67107 (91.60%)]	 Loss: 0.000004
Training epoch 47 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 47 [62720/67107 (93.51%)]	 Loss: 0.000007
Training epoch 47 [63360/67107 (94.47%)]	 Loss: 0.000031
Training epoch 47 [64000/67107 (95.42%)]	 Loss: 0.000005
Training epoch 47 [64640/67107 (96.37%)]	 Loss: 0.000026
Training epoch 47 [65280/67107 (97.33%)]	 Loss: 0.000019
Training epoch 47 [65920/67107 (98.28%)]	 Loss: 0.000007
Training epoch 47 [66560/67107 (99.24%)]	 Loss: 0.000007
Test set: Average Loss: 0.000235
Training epoch 48 [0/67107 (0.00%)]	 Loss: 0.000008
Training epoch 48 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 48 [1280/67107 (1.91%)]	 Loss: 0.000014
Training epoch 48 [1920/67107 (2.86%)]	 Loss: 0.000012
Training epoch 48 [2560/67107 (3.82%)]	 Loss: 0.000018
Training epoch 48 [3200/67107 (4.77%)]	 Loss: 0.000055
Training epoch 48 [3840/67107 (5.73%)]	 Loss: 0.000029
Training epoch 48 [4480/67107 (6.68%)]	 Loss: 0.000017
Training epoch 48 [5120/67107 (7.63%)]	 Loss: 0.000025
Training epoch 48 [5760/67107 (8.59%)]	 Loss: 0.000011
Training epoch 48 [6400/67107 (9.54%)]	 Loss: 0.000006
Training epoch 48 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 48 [7680/67107 (11.45%)]	 Loss: 0.000003
Training epoch 48 [8320/67107 (12.40%)]	 Loss: 0.000004
Training epoch 48 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 48 [9600/67107 (14.31%)]	 Loss: 0.000001
Training epoch 48 [10240/67107 (15.27%)]	 Loss: 0.000031
Training epoch 48 [10880/67107 (16.22%)]	 Loss: 0.000010
Training epoch 48 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 48 [12160/67107 (18.13%)]	 Loss: 0.000079
Training epoch 48 [12800/67107 (19.08%)]	 Loss: 0.000025
Training epoch 48 [13440/67107 (20.04%)]	 Loss: 0.000009
Training epoch 48 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 48 [14720/67107 (21.95%)]	 Loss: 0.000021
Training epoch 48 [15360/67107 (22.90%)]	 Loss: 0.000016
Training epoch 48 [16000/67107 (23.85%)]	 Loss: 0.000014
Training epoch 48 [16640/67107 (24.81%)]	 Loss: 0.000004
Training epoch 48 [17280/67107 (25.76%)]	 Loss: 0.000010
Training epoch 48 [17920/67107 (26.72%)]	 Loss: 0.000022
Training epoch 48 [18560/67107 (27.67%)]	 Loss: 0.000010
Training epoch 48 [19200/67107 (28.63%)]	 Loss: 0.000029
Training epoch 48 [19840/67107 (29.58%)]	 Loss: 0.000008
Training epoch 48 [20480/67107 (30.53%)]	 Loss: 0.000043
Training epoch 48 [21120/67107 (31.49%)]	 Loss: 0.000062
Training epoch 48 [21760/67107 (32.44%)]	 Loss: 0.000027
Training epoch 48 [22400/67107 (33.40%)]	 Loss: 0.000015
Training epoch 48 [23040/67107 (34.35%)]	 Loss: 0.000004
Training epoch 48 [23680/67107 (35.31%)]	 Loss: 0.000004
Training epoch 48 [24320/67107 (36.26%)]	 Loss: 0.000002
Training epoch 48 [24960/67107 (37.21%)]	 Loss: 0.000024
Training epoch 48 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 48 [26240/67107 (39.12%)]	 Loss: 0.000013
Training epoch 48 [26880/67107 (40.08%)]	 Loss: 0.000033
Training epoch 48 [27520/67107 (41.03%)]	 Loss: 0.000042
Training epoch 48 [28160/67107 (41.98%)]	 Loss: 0.000030
Training epoch 48 [28800/67107 (42.94%)]	 Loss: 0.000094
Training epoch 48 [29440/67107 (43.89%)]	 Loss: 0.000003
Training epoch 48 [30080/67107 (44.85%)]	 Loss: 0.000018
Training epoch 48 [30720/67107 (45.80%)]	 Loss: 0.000139
Training epoch 48 [31360/67107 (46.76%)]	 Loss: 0.000035
Training epoch 48 [32000/67107 (47.71%)]	 Loss: 0.000019
Training epoch 48 [32640/67107 (48.66%)]	 Loss: 0.000004
Training epoch 48 [33280/67107 (49.62%)]	 Loss: 0.000006
Training epoch 48 [33920/67107 (50.57%)]	 Loss: 0.000007
Training epoch 48 [34560/67107 (51.53%)]	 Loss: 0.000042
Training epoch 48 [35200/67107 (52.48%)]	 Loss: 0.000022
Training epoch 48 [35840/67107 (53.44%)]	 Loss: 0.000013
Training epoch 48 [36480/67107 (54.39%)]	 Loss: 0.000009
Training epoch 48 [37120/67107 (55.34%)]	 Loss: 0.000040
Training epoch 48 [37760/67107 (56.30%)]	 Loss: 0.000033
Training epoch 48 [38400/67107 (57.25%)]	 Loss: 0.000149
Training epoch 48 [39040/67107 (58.21%)]	 Loss: 0.000024
Training epoch 48 [39680/67107 (59.16%)]	 Loss: 0.000032
Training epoch 48 [40320/67107 (60.11%)]	 Loss: 0.000088
Training epoch 48 [40960/67107 (61.07%)]	 Loss: 0.000053
Training epoch 48 [41600/67107 (62.02%)]	 Loss: 0.000022
Training epoch 48 [42240/67107 (62.98%)]	 Loss: 0.000012
Training epoch 48 [42880/67107 (63.93%)]	 Loss: 0.000035
Training epoch 48 [43520/67107 (64.89%)]	 Loss: 0.000068
Training epoch 48 [44160/67107 (65.84%)]	 Loss: 0.000058
Training epoch 48 [44800/67107 (66.79%)]	 Loss: 0.000010
Training epoch 48 [45440/67107 (67.75%)]	 Loss: 0.000011
Training epoch 48 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 48 [46720/67107 (69.66%)]	 Loss: 0.000014
Training epoch 48 [47360/67107 (70.61%)]	 Loss: 0.000006
Training epoch 48 [48000/67107 (71.56%)]	 Loss: 0.000006
Training epoch 48 [48640/67107 (72.52%)]	 Loss: 0.000004
Training epoch 48 [49280/67107 (73.47%)]	 Loss: 0.000022
Training epoch 48 [49920/67107 (74.43%)]	 Loss: 0.000016
Training epoch 48 [50560/67107 (75.38%)]	 Loss: 0.000025
Training epoch 48 [51200/67107 (76.34%)]	 Loss: 0.000012
Training epoch 48 [51840/67107 (77.29%)]	 Loss: 0.000011
Training epoch 48 [52480/67107 (78.24%)]	 Loss: 0.000008
Training epoch 48 [53120/67107 (79.20%)]	 Loss: 0.000043
Training epoch 48 [53760/67107 (80.15%)]	 Loss: 0.000056
Training epoch 48 [54400/67107 (81.11%)]	 Loss: 0.000017
Training epoch 48 [55040/67107 (82.06%)]	 Loss: 0.000017
Training epoch 48 [55680/67107 (83.02%)]	 Loss: 0.000028
Training epoch 48 [56320/67107 (83.97%)]	 Loss: 0.000004
Training epoch 48 [56960/67107 (84.92%)]	 Loss: 0.000017
Training epoch 48 [57600/67107 (85.88%)]	 Loss: 0.000009
Training epoch 48 [58240/67107 (86.83%)]	 Loss: 0.000042
Training epoch 48 [58880/67107 (87.79%)]	 Loss: 0.000005
Training epoch 48 [59520/67107 (88.74%)]	 Loss: 0.000013
Training epoch 48 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 48 [60800/67107 (90.65%)]	 Loss: 0.000006
Training epoch 48 [61440/67107 (91.60%)]	 Loss: 0.000005
Training epoch 48 [62080/67107 (92.56%)]	 Loss: 0.000007
Training epoch 48 [62720/67107 (93.51%)]	 Loss: 0.000008
Training epoch 48 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 48 [64000/67107 (95.42%)]	 Loss: 0.000006
Training epoch 48 [64640/67107 (96.37%)]	 Loss: 0.000018
Training epoch 48 [65280/67107 (97.33%)]	 Loss: 0.000008
Training epoch 48 [65920/67107 (98.28%)]	 Loss: 0.000010
Training epoch 48 [66560/67107 (99.24%)]	 Loss: 0.000020
Test set: Average Loss: 0.000125
Training epoch 49 [0/67107 (0.00%)]	 Loss: 0.000022
Training epoch 49 [640/67107 (0.95%)]	 Loss: 0.000022
Training epoch 49 [1280/67107 (1.91%)]	 Loss: 0.000013
Training epoch 49 [1920/67107 (2.86%)]	 Loss: 0.000018
Training epoch 49 [2560/67107 (3.82%)]	 Loss: 0.000010
Training epoch 49 [3200/67107 (4.77%)]	 Loss: 0.000015
Training epoch 49 [3840/67107 (5.73%)]	 Loss: 0.000018
Training epoch 49 [4480/67107 (6.68%)]	 Loss: 0.000011
Training epoch 49 [5120/67107 (7.63%)]	 Loss: 0.000038
Training epoch 49 [5760/67107 (8.59%)]	 Loss: 0.000004
Training epoch 49 [6400/67107 (9.54%)]	 Loss: 0.000005
Training epoch 49 [7040/67107 (10.50%)]	 Loss: 0.000006
Training epoch 49 [7680/67107 (11.45%)]	 Loss: 0.000004
Training epoch 49 [8320/67107 (12.40%)]	 Loss: 0.000004
Training epoch 49 [8960/67107 (13.36%)]	 Loss: 0.000009
Training epoch 49 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 49 [10240/67107 (15.27%)]	 Loss: 0.000034
Training epoch 49 [10880/67107 (16.22%)]	 Loss: 0.000005
Training epoch 49 [11520/67107 (17.18%)]	 Loss: 0.000022
Training epoch 49 [12160/67107 (18.13%)]	 Loss: 0.000011
Training epoch 49 [12800/67107 (19.08%)]	 Loss: 0.000037
Training epoch 49 [13440/67107 (20.04%)]	 Loss: 0.000035
Training epoch 49 [14080/67107 (20.99%)]	 Loss: 0.000010
Training epoch 49 [14720/67107 (21.95%)]	 Loss: 0.000011
Training epoch 49 [15360/67107 (22.90%)]	 Loss: 0.000016
Training epoch 49 [16000/67107 (23.85%)]	 Loss: 0.000016
Training epoch 49 [16640/67107 (24.81%)]	 Loss: 0.000019
Training epoch 49 [17280/67107 (25.76%)]	 Loss: 0.000006
Training epoch 49 [17920/67107 (26.72%)]	 Loss: 0.000023
Training epoch 49 [18560/67107 (27.67%)]	 Loss: 0.000034
Training epoch 49 [19200/67107 (28.63%)]	 Loss: 0.000019
Training epoch 49 [19840/67107 (29.58%)]	 Loss: 0.000005
Training epoch 49 [20480/67107 (30.53%)]	 Loss: 0.000007
Training epoch 49 [21120/67107 (31.49%)]	 Loss: 0.000007
Training epoch 49 [21760/67107 (32.44%)]	 Loss: 0.000017
Training epoch 49 [22400/67107 (33.40%)]	 Loss: 0.000020
Training epoch 49 [23040/67107 (34.35%)]	 Loss: 0.000009
Training epoch 49 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 49 [24320/67107 (36.26%)]	 Loss: 0.000005
Training epoch 49 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 49 [25600/67107 (38.17%)]	 Loss: 0.000020
Training epoch 49 [26240/67107 (39.12%)]	 Loss: 0.000010
Training epoch 49 [26880/67107 (40.08%)]	 Loss: 0.000025
Training epoch 49 [27520/67107 (41.03%)]	 Loss: 0.000014
Training epoch 49 [28160/67107 (41.98%)]	 Loss: 0.000051
Training epoch 49 [28800/67107 (42.94%)]	 Loss: 0.000013
Training epoch 49 [29440/67107 (43.89%)]	 Loss: 0.000009
Training epoch 49 [30080/67107 (44.85%)]	 Loss: 0.000009
Training epoch 49 [30720/67107 (45.80%)]	 Loss: 0.000009
Training epoch 49 [31360/67107 (46.76%)]	 Loss: 0.000011
Training epoch 49 [32000/67107 (47.71%)]	 Loss: 0.000031
Training epoch 49 [32640/67107 (48.66%)]	 Loss: 0.000027
Training epoch 49 [33280/67107 (49.62%)]	 Loss: 0.000021
Training epoch 49 [33920/67107 (50.57%)]	 Loss: 0.000013
Training epoch 49 [34560/67107 (51.53%)]	 Loss: 0.000034
Training epoch 49 [35200/67107 (52.48%)]	 Loss: 0.000023
Training epoch 49 [35840/67107 (53.44%)]	 Loss: 0.000009
Training epoch 49 [36480/67107 (54.39%)]	 Loss: 0.000052
Training epoch 49 [37120/67107 (55.34%)]	 Loss: 0.000005
Training epoch 49 [37760/67107 (56.30%)]	 Loss: 0.000006
Training epoch 49 [38400/67107 (57.25%)]	 Loss: 0.000028
Training epoch 49 [39040/67107 (58.21%)]	 Loss: 0.000017
Training epoch 49 [39680/67107 (59.16%)]	 Loss: 0.000022
Training epoch 49 [40320/67107 (60.11%)]	 Loss: 0.000056
Training epoch 49 [40960/67107 (61.07%)]	 Loss: 0.000001
Training epoch 49 [41600/67107 (62.02%)]	 Loss: 0.000038
Training epoch 49 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 49 [42880/67107 (63.93%)]	 Loss: 0.000042
Training epoch 49 [43520/67107 (64.89%)]	 Loss: 0.000019
Training epoch 49 [44160/67107 (65.84%)]	 Loss: 0.000001
Training epoch 49 [44800/67107 (66.79%)]	 Loss: 0.000006
Training epoch 49 [45440/67107 (67.75%)]	 Loss: 0.000005
Training epoch 49 [46080/67107 (68.70%)]	 Loss: 0.000006
Training epoch 49 [46720/67107 (69.66%)]	 Loss: 0.000023
Training epoch 49 [47360/67107 (70.61%)]	 Loss: 0.000013
Training epoch 49 [48000/67107 (71.56%)]	 Loss: 0.000015
Training epoch 49 [48640/67107 (72.52%)]	 Loss: 0.000012
Training epoch 49 [49280/67107 (73.47%)]	 Loss: 0.000012
Training epoch 49 [49920/67107 (74.43%)]	 Loss: 0.000009
Training epoch 49 [50560/67107 (75.38%)]	 Loss: 0.000056
Training epoch 49 [51200/67107 (76.34%)]	 Loss: 0.000001
Training epoch 49 [51840/67107 (77.29%)]	 Loss: 0.000021
Training epoch 49 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 49 [53120/67107 (79.20%)]	 Loss: 0.000027
Training epoch 49 [53760/67107 (80.15%)]	 Loss: 0.000016
Training epoch 49 [54400/67107 (81.11%)]	 Loss: 0.000010
Training epoch 49 [55040/67107 (82.06%)]	 Loss: 0.000013
Training epoch 49 [55680/67107 (83.02%)]	 Loss: 0.000021
Training epoch 49 [56320/67107 (83.97%)]	 Loss: 0.000022
Training epoch 49 [56960/67107 (84.92%)]	 Loss: 0.000005
Training epoch 49 [57600/67107 (85.88%)]	 Loss: 0.000012
Training epoch 49 [58240/67107 (86.83%)]	 Loss: 0.000004
Training epoch 49 [58880/67107 (87.79%)]	 Loss: 0.000005
Training epoch 49 [59520/67107 (88.74%)]	 Loss: 0.000017
Training epoch 49 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 49 [60800/67107 (90.65%)]	 Loss: 0.000020
Training epoch 49 [61440/67107 (91.60%)]	 Loss: 0.000014
Training epoch 49 [62080/67107 (92.56%)]	 Loss: 0.000076
Training epoch 49 [62720/67107 (93.51%)]	 Loss: 0.000016
Training epoch 49 [63360/67107 (94.47%)]	 Loss: 0.000112
Training epoch 49 [64000/67107 (95.42%)]	 Loss: 0.000021
Training epoch 49 [64640/67107 (96.37%)]	 Loss: 0.000012
Training epoch 49 [65280/67107 (97.33%)]	 Loss: 0.000013
Training epoch 49 [65920/67107 (98.28%)]	 Loss: 0.000003
Training epoch 49 [66560/67107 (99.24%)]	 Loss: 0.000002
Test set: Average Loss: 0.000932
Training epoch 50 [0/67107 (0.00%)]	 Loss: 0.000003
Training epoch 50 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 50 [1280/67107 (1.91%)]	 Loss: 0.000011
Training epoch 50 [1920/67107 (2.86%)]	 Loss: 0.000013
Training epoch 50 [2560/67107 (3.82%)]	 Loss: 0.000010
Training epoch 50 [3200/67107 (4.77%)]	 Loss: 0.000064
Training epoch 50 [3840/67107 (5.73%)]	 Loss: 0.000008
Training epoch 50 [4480/67107 (6.68%)]	 Loss: 0.000004
Training epoch 50 [5120/67107 (7.63%)]	 Loss: 0.000025
Training epoch 50 [5760/67107 (8.59%)]	 Loss: 0.000027
Training epoch 50 [6400/67107 (9.54%)]	 Loss: 0.000011
Training epoch 50 [7040/67107 (10.50%)]	 Loss: 0.000014
Training epoch 50 [7680/67107 (11.45%)]	 Loss: 0.000015
Training epoch 50 [8320/67107 (12.40%)]	 Loss: 0.000017
Training epoch 50 [8960/67107 (13.36%)]	 Loss: 0.000006
Training epoch 50 [9600/67107 (14.31%)]	 Loss: 0.000015
Training epoch 50 [10240/67107 (15.27%)]	 Loss: 0.000011
Training epoch 50 [10880/67107 (16.22%)]	 Loss: 0.000018
Training epoch 50 [11520/67107 (17.18%)]	 Loss: 0.000054
Training epoch 50 [12160/67107 (18.13%)]	 Loss: 0.000007
Training epoch 50 [12800/67107 (19.08%)]	 Loss: 0.000021
Training epoch 50 [13440/67107 (20.04%)]	 Loss: 0.000043
Training epoch 50 [14080/67107 (20.99%)]	 Loss: 0.000017
Training epoch 50 [14720/67107 (21.95%)]	 Loss: 0.000011
Training epoch 50 [15360/67107 (22.90%)]	 Loss: 0.000063
Training epoch 50 [16000/67107 (23.85%)]	 Loss: 0.000034
Training epoch 50 [16640/67107 (24.81%)]	 Loss: 0.000010
Training epoch 50 [17280/67107 (25.76%)]	 Loss: 0.000023
Training epoch 50 [17920/67107 (26.72%)]	 Loss: 0.000022
Training epoch 50 [18560/67107 (27.67%)]	 Loss: 0.000016
Training epoch 50 [19200/67107 (28.63%)]	 Loss: 0.000004
Training epoch 50 [19840/67107 (29.58%)]	 Loss: 0.000006
Training epoch 50 [20480/67107 (30.53%)]	 Loss: 0.000011
Training epoch 50 [21120/67107 (31.49%)]	 Loss: 0.000040
Training epoch 50 [21760/67107 (32.44%)]	 Loss: 0.000046
Training epoch 50 [22400/67107 (33.40%)]	 Loss: 0.000037
Training epoch 50 [23040/67107 (34.35%)]	 Loss: 0.000059
Training epoch 50 [23680/67107 (35.31%)]	 Loss: 0.000011
Training epoch 50 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 50 [24960/67107 (37.21%)]	 Loss: 0.000004
Training epoch 50 [25600/67107 (38.17%)]	 Loss: 0.000003
Training epoch 50 [26240/67107 (39.12%)]	 Loss: 0.000009
Training epoch 50 [26880/67107 (40.08%)]	 Loss: 0.000002
Training epoch 50 [27520/67107 (41.03%)]	 Loss: 0.000032
Training epoch 50 [28160/67107 (41.98%)]	 Loss: 0.000018
Training epoch 50 [28800/67107 (42.94%)]	 Loss: 0.000033
Training epoch 50 [29440/67107 (43.89%)]	 Loss: 0.000020
Training epoch 50 [30080/67107 (44.85%)]	 Loss: 0.000009
Training epoch 50 [30720/67107 (45.80%)]	 Loss: 0.000009
Training epoch 50 [31360/67107 (46.76%)]	 Loss: 0.000002
Training epoch 50 [32000/67107 (47.71%)]	 Loss: 0.000028
Training epoch 50 [32640/67107 (48.66%)]	 Loss: 0.000009
Training epoch 50 [33280/67107 (49.62%)]	 Loss: 0.000009
Training epoch 50 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 50 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 50 [35200/67107 (52.48%)]	 Loss: 0.000009
Training epoch 50 [35840/67107 (53.44%)]	 Loss: 0.000084
Training epoch 50 [36480/67107 (54.39%)]	 Loss: 0.000010
Training epoch 50 [37120/67107 (55.34%)]	 Loss: 0.000027
Training epoch 50 [37760/67107 (56.30%)]	 Loss: 0.000024
Training epoch 50 [38400/67107 (57.25%)]	 Loss: 0.000015
Training epoch 50 [39040/67107 (58.21%)]	 Loss: 0.000011
Training epoch 50 [39680/67107 (59.16%)]	 Loss: 0.000008
Training epoch 50 [40320/67107 (60.11%)]	 Loss: 0.000025
Training epoch 50 [40960/67107 (61.07%)]	 Loss: 0.000019
Training epoch 50 [41600/67107 (62.02%)]	 Loss: 0.000008
Training epoch 50 [42240/67107 (62.98%)]	 Loss: 0.000029
Training epoch 50 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 50 [43520/67107 (64.89%)]	 Loss: 0.000039
Training epoch 50 [44160/67107 (65.84%)]	 Loss: 0.000010
Training epoch 50 [44800/67107 (66.79%)]	 Loss: 0.000017
Training epoch 50 [45440/67107 (67.75%)]	 Loss: 0.000006
Training epoch 50 [46080/67107 (68.70%)]	 Loss: 0.000005
Training epoch 50 [46720/67107 (69.66%)]	 Loss: 0.000013
Training epoch 50 [47360/67107 (70.61%)]	 Loss: 0.000003
Training epoch 50 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 50 [48640/67107 (72.52%)]	 Loss: 0.000006
Training epoch 50 [49280/67107 (73.47%)]	 Loss: 0.000024
Training epoch 50 [49920/67107 (74.43%)]	 Loss: 0.000007
Training epoch 50 [50560/67107 (75.38%)]	 Loss: 0.000012
Training epoch 50 [51200/67107 (76.34%)]	 Loss: 0.000019
Training epoch 50 [51840/67107 (77.29%)]	 Loss: 0.000002
Training epoch 50 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 50 [53120/67107 (79.20%)]	 Loss: 0.000009
Training epoch 50 [53760/67107 (80.15%)]	 Loss: 0.000058
Training epoch 50 [54400/67107 (81.11%)]	 Loss: 0.000005
Training epoch 50 [55040/67107 (82.06%)]	 Loss: 0.000012
Training epoch 50 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 50 [56320/67107 (83.97%)]	 Loss: 0.000003
Training epoch 50 [56960/67107 (84.92%)]	 Loss: 0.000003
Training epoch 50 [57600/67107 (85.88%)]	 Loss: 0.000014
Training epoch 50 [58240/67107 (86.83%)]	 Loss: 0.000016
Training epoch 50 [58880/67107 (87.79%)]	 Loss: 0.000028
Training epoch 50 [59520/67107 (88.74%)]	 Loss: 0.000011
Training epoch 50 [60160/67107 (89.69%)]	 Loss: 0.000038
Training epoch 50 [60800/67107 (90.65%)]	 Loss: 0.000027
Training epoch 50 [61440/67107 (91.60%)]	 Loss: 0.000012
Training epoch 50 [62080/67107 (92.56%)]	 Loss: 0.000023
Training epoch 50 [62720/67107 (93.51%)]	 Loss: 0.000041
Training epoch 50 [63360/67107 (94.47%)]	 Loss: 0.000058
Training epoch 50 [64000/67107 (95.42%)]	 Loss: 0.000022
Training epoch 50 [64640/67107 (96.37%)]	 Loss: 0.000019
Training epoch 50 [65280/67107 (97.33%)]	 Loss: 0.000043
Training epoch 50 [65920/67107 (98.28%)]	 Loss: 0.000059
Training epoch 50 [66560/67107 (99.24%)]	 Loss: 0.000018
Test set: Average Loss: 0.006535
Training epoch 51 [0/67107 (0.00%)]	 Loss: 0.000006
Training epoch 51 [640/67107 (0.95%)]	 Loss: 0.000010
Training epoch 51 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 51 [1920/67107 (2.86%)]	 Loss: 0.000007
Training epoch 51 [2560/67107 (3.82%)]	 Loss: 0.000002
Training epoch 51 [3200/67107 (4.77%)]	 Loss: 0.000005
Training epoch 51 [3840/67107 (5.73%)]	 Loss: 0.000004
Training epoch 51 [4480/67107 (6.68%)]	 Loss: 0.000055
Training epoch 51 [5120/67107 (7.63%)]	 Loss: 0.000017
Training epoch 51 [5760/67107 (8.59%)]	 Loss: 0.000023
Training epoch 51 [6400/67107 (9.54%)]	 Loss: 0.000009
Training epoch 51 [7040/67107 (10.50%)]	 Loss: 0.000003
Training epoch 51 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 51 [8320/67107 (12.40%)]	 Loss: 0.000028
Training epoch 51 [8960/67107 (13.36%)]	 Loss: 0.000004
Training epoch 51 [9600/67107 (14.31%)]	 Loss: 0.000005
Training epoch 51 [10240/67107 (15.27%)]	 Loss: 0.000006
Training epoch 51 [10880/67107 (16.22%)]	 Loss: 0.000006
Training epoch 51 [11520/67107 (17.18%)]	 Loss: 0.000072
Training epoch 51 [12160/67107 (18.13%)]	 Loss: 0.000037
Training epoch 51 [12800/67107 (19.08%)]	 Loss: 0.000082
Training epoch 51 [13440/67107 (20.04%)]	 Loss: 0.000024
Training epoch 51 [14080/67107 (20.99%)]	 Loss: 0.000009
Training epoch 51 [14720/67107 (21.95%)]	 Loss: 0.000018
Training epoch 51 [15360/67107 (22.90%)]	 Loss: 0.000016
Training epoch 51 [16000/67107 (23.85%)]	 Loss: 0.000004
Training epoch 51 [16640/67107 (24.81%)]	 Loss: 0.000029
Training epoch 51 [17280/67107 (25.76%)]	 Loss: 0.000020
Training epoch 51 [17920/67107 (26.72%)]	 Loss: 0.000011
Training epoch 51 [18560/67107 (27.67%)]	 Loss: 0.000017
Training epoch 51 [19200/67107 (28.63%)]	 Loss: 0.000019
Training epoch 51 [19840/67107 (29.58%)]	 Loss: 0.000010
Training epoch 51 [20480/67107 (30.53%)]	 Loss: 0.000049
Training epoch 51 [21120/67107 (31.49%)]	 Loss: 0.000003
Training epoch 51 [21760/67107 (32.44%)]	 Loss: 0.000008
Training epoch 51 [22400/67107 (33.40%)]	 Loss: 0.000003
Training epoch 51 [23040/67107 (34.35%)]	 Loss: 0.000010
Training epoch 51 [23680/67107 (35.31%)]	 Loss: 0.000010
Training epoch 51 [24320/67107 (36.26%)]	 Loss: 0.000015
Training epoch 51 [24960/67107 (37.21%)]	 Loss: 0.000013
Training epoch 51 [25600/67107 (38.17%)]	 Loss: 0.000013
Training epoch 51 [26240/67107 (39.12%)]	 Loss: 0.000054
Training epoch 51 [26880/67107 (40.08%)]	 Loss: 0.000025
Training epoch 51 [27520/67107 (41.03%)]	 Loss: 0.000007
Training epoch 51 [28160/67107 (41.98%)]	 Loss: 0.000004
Training epoch 51 [28800/67107 (42.94%)]	 Loss: 0.000003
Training epoch 51 [29440/67107 (43.89%)]	 Loss: 0.000007
Training epoch 51 [30080/67107 (44.85%)]	 Loss: 0.000003
Training epoch 51 [30720/67107 (45.80%)]	 Loss: 0.000006
Training epoch 51 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 51 [32000/67107 (47.71%)]	 Loss: 0.000006
Training epoch 51 [32640/67107 (48.66%)]	 Loss: 0.000018
Training epoch 51 [33280/67107 (49.62%)]	 Loss: 0.000039
Training epoch 51 [33920/67107 (50.57%)]	 Loss: 0.000027
Training epoch 51 [34560/67107 (51.53%)]	 Loss: 0.000023
Training epoch 51 [35200/67107 (52.48%)]	 Loss: 0.000007
Training epoch 51 [35840/67107 (53.44%)]	 Loss: 0.000020
Training epoch 51 [36480/67107 (54.39%)]	 Loss: 0.000006
Training epoch 51 [37120/67107 (55.34%)]	 Loss: 0.000018
Training epoch 51 [37760/67107 (56.30%)]	 Loss: 0.000004
Training epoch 51 [38400/67107 (57.25%)]	 Loss: 0.000010
Training epoch 51 [39040/67107 (58.21%)]	 Loss: 0.000007
Training epoch 51 [39680/67107 (59.16%)]	 Loss: 0.000005
Training epoch 51 [40320/67107 (60.11%)]	 Loss: 0.000005
Training epoch 51 [40960/67107 (61.07%)]	 Loss: 0.000105
Training epoch 51 [41600/67107 (62.02%)]	 Loss: 0.000015
Training epoch 51 [42240/67107 (62.98%)]	 Loss: 0.000028
Training epoch 51 [42880/67107 (63.93%)]	 Loss: 0.000008
Training epoch 51 [43520/67107 (64.89%)]	 Loss: 0.000012
Training epoch 51 [44160/67107 (65.84%)]	 Loss: 0.000007
Training epoch 51 [44800/67107 (66.79%)]	 Loss: 0.000008
Training epoch 51 [45440/67107 (67.75%)]	 Loss: 0.000061
Training epoch 51 [46080/67107 (68.70%)]	 Loss: 0.000011
Training epoch 51 [46720/67107 (69.66%)]	 Loss: 0.000009
Training epoch 51 [47360/67107 (70.61%)]	 Loss: 0.000014
Training epoch 51 [48000/67107 (71.56%)]	 Loss: 0.000014
Training epoch 51 [48640/67107 (72.52%)]	 Loss: 0.000008
Training epoch 51 [49280/67107 (73.47%)]	 Loss: 0.000026
Training epoch 51 [49920/67107 (74.43%)]	 Loss: 0.000013
Training epoch 51 [50560/67107 (75.38%)]	 Loss: 0.000011
Training epoch 51 [51200/67107 (76.34%)]	 Loss: 0.000006
Training epoch 51 [51840/67107 (77.29%)]	 Loss: 0.000023
Training epoch 51 [52480/67107 (78.24%)]	 Loss: 0.000014
Training epoch 51 [53120/67107 (79.20%)]	 Loss: 0.000016
Training epoch 51 [53760/67107 (80.15%)]	 Loss: 0.000026
Training epoch 51 [54400/67107 (81.11%)]	 Loss: 0.000021
Training epoch 51 [55040/67107 (82.06%)]	 Loss: 0.000023
Training epoch 51 [55680/67107 (83.02%)]	 Loss: 0.000027
Training epoch 51 [56320/67107 (83.97%)]	 Loss: 0.000081
Training epoch 51 [56960/67107 (84.92%)]	 Loss: 0.000013
Training epoch 51 [57600/67107 (85.88%)]	 Loss: 0.000017
Training epoch 51 [58240/67107 (86.83%)]	 Loss: 0.000016
Training epoch 51 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 51 [59520/67107 (88.74%)]	 Loss: 0.000017
Training epoch 51 [60160/67107 (89.69%)]	 Loss: 0.000016
Training epoch 51 [60800/67107 (90.65%)]	 Loss: 0.000010
Training epoch 51 [61440/67107 (91.60%)]	 Loss: 0.000024
Training epoch 51 [62080/67107 (92.56%)]	 Loss: 0.000020
Training epoch 51 [62720/67107 (93.51%)]	 Loss: 0.000023
Training epoch 51 [63360/67107 (94.47%)]	 Loss: 0.000006
Training epoch 51 [64000/67107 (95.42%)]	 Loss: 0.000015
Training epoch 51 [64640/67107 (96.37%)]	 Loss: 0.000002
Training epoch 51 [65280/67107 (97.33%)]	 Loss: 0.000007
Training epoch 51 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 51 [66560/67107 (99.24%)]	 Loss: 0.000008
Test set: Average Loss: 0.004592
Training epoch 52 [0/67107 (0.00%)]	 Loss: 0.000011
Training epoch 52 [640/67107 (0.95%)]	 Loss: 0.000043
Training epoch 52 [1280/67107 (1.91%)]	 Loss: 0.000015
Training epoch 52 [1920/67107 (2.86%)]	 Loss: 0.000007
Training epoch 52 [2560/67107 (3.82%)]	 Loss: 0.000027
Training epoch 52 [3200/67107 (4.77%)]	 Loss: 0.000007
Training epoch 52 [3840/67107 (5.73%)]	 Loss: 0.000004
Training epoch 52 [4480/67107 (6.68%)]	 Loss: 0.000002
Training epoch 52 [5120/67107 (7.63%)]	 Loss: 0.000005
Training epoch 52 [5760/67107 (8.59%)]	 Loss: 0.000014
Training epoch 52 [6400/67107 (9.54%)]	 Loss: 0.000024
Training epoch 52 [7040/67107 (10.50%)]	 Loss: 0.000004
Training epoch 52 [7680/67107 (11.45%)]	 Loss: 0.000009
Training epoch 52 [8320/67107 (12.40%)]	 Loss: 0.000025
Training epoch 52 [8960/67107 (13.36%)]	 Loss: 0.000009
Training epoch 52 [9600/67107 (14.31%)]	 Loss: 0.000009
Training epoch 52 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 52 [10880/67107 (16.22%)]	 Loss: 0.000033
Training epoch 52 [11520/67107 (17.18%)]	 Loss: 0.000011
Training epoch 52 [12160/67107 (18.13%)]	 Loss: 0.000006
Training epoch 52 [12800/67107 (19.08%)]	 Loss: 0.000006
Training epoch 52 [13440/67107 (20.04%)]	 Loss: 0.000006
Training epoch 52 [14080/67107 (20.99%)]	 Loss: 0.000002
Training epoch 52 [14720/67107 (21.95%)]	 Loss: 0.000020
Training epoch 52 [15360/67107 (22.90%)]	 Loss: 0.000010
Training epoch 52 [16000/67107 (23.85%)]	 Loss: 0.000025
Training epoch 52 [16640/67107 (24.81%)]	 Loss: 0.000002
Training epoch 52 [17280/67107 (25.76%)]	 Loss: 0.000037
Training epoch 52 [17920/67107 (26.72%)]	 Loss: 0.000009
Training epoch 52 [18560/67107 (27.67%)]	 Loss: 0.000013
Training epoch 52 [19200/67107 (28.63%)]	 Loss: 0.000151
Training epoch 52 [19840/67107 (29.58%)]	 Loss: 0.000076
Training epoch 52 [20480/67107 (30.53%)]	 Loss: 0.000097
Training epoch 52 [21120/67107 (31.49%)]	 Loss: 0.000029
Training epoch 52 [21760/67107 (32.44%)]	 Loss: 0.000094
Training epoch 52 [22400/67107 (33.40%)]	 Loss: 0.000045
Training epoch 52 [23040/67107 (34.35%)]	 Loss: 0.000009
Training epoch 52 [23680/67107 (35.31%)]	 Loss: 0.000022
Training epoch 52 [24320/67107 (36.26%)]	 Loss: 0.000009
Training epoch 52 [24960/67107 (37.21%)]	 Loss: 0.000024
Training epoch 52 [25600/67107 (38.17%)]	 Loss: 0.000011
Training epoch 52 [26240/67107 (39.12%)]	 Loss: 0.000005
Training epoch 52 [26880/67107 (40.08%)]	 Loss: 0.000007
Training epoch 52 [27520/67107 (41.03%)]	 Loss: 0.000006
Training epoch 52 [28160/67107 (41.98%)]	 Loss: 0.000001
Training epoch 52 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 52 [29440/67107 (43.89%)]	 Loss: 0.000009
Training epoch 52 [30080/67107 (44.85%)]	 Loss: 0.000076
Training epoch 52 [30720/67107 (45.80%)]	 Loss: 0.000002
Training epoch 52 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 52 [32000/67107 (47.71%)]	 Loss: 0.000046
Training epoch 52 [32640/67107 (48.66%)]	 Loss: 0.000029
Training epoch 52 [33280/67107 (49.62%)]	 Loss: 0.000007
Training epoch 52 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 52 [34560/67107 (51.53%)]	 Loss: 0.000025
Training epoch 52 [35200/67107 (52.48%)]	 Loss: 0.000016
Training epoch 52 [35840/67107 (53.44%)]	 Loss: 0.000006
Training epoch 52 [36480/67107 (54.39%)]	 Loss: 0.000002
Training epoch 52 [37120/67107 (55.34%)]	 Loss: 0.000010
Training epoch 52 [37760/67107 (56.30%)]	 Loss: 0.000012
Training epoch 52 [38400/67107 (57.25%)]	 Loss: 0.000005
Training epoch 52 [39040/67107 (58.21%)]	 Loss: 0.000013
Training epoch 52 [39680/67107 (59.16%)]	 Loss: 0.000021
Training epoch 52 [40320/67107 (60.11%)]	 Loss: 0.000001
Training epoch 52 [40960/67107 (61.07%)]	 Loss: 0.000005
Training epoch 52 [41600/67107 (62.02%)]	 Loss: 0.000008
Training epoch 52 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 52 [42880/67107 (63.93%)]	 Loss: 0.000020
Training epoch 52 [43520/67107 (64.89%)]	 Loss: 0.000015
Training epoch 52 [44160/67107 (65.84%)]	 Loss: 0.000013
Training epoch 52 [44800/67107 (66.79%)]	 Loss: 0.000001
Training epoch 52 [45440/67107 (67.75%)]	 Loss: 0.000003
Training epoch 52 [46080/67107 (68.70%)]	 Loss: 0.000019
Training epoch 52 [46720/67107 (69.66%)]	 Loss: 0.000023
Training epoch 52 [47360/67107 (70.61%)]	 Loss: 0.000046
Training epoch 52 [48000/67107 (71.56%)]	 Loss: 0.000019
Training epoch 52 [48640/67107 (72.52%)]	 Loss: 0.000038
Training epoch 52 [49280/67107 (73.47%)]	 Loss: 0.000007
Training epoch 52 [49920/67107 (74.43%)]	 Loss: 0.000019
Training epoch 52 [50560/67107 (75.38%)]	 Loss: 0.000016
Training epoch 52 [51200/67107 (76.34%)]	 Loss: 0.000013
Training epoch 52 [51840/67107 (77.29%)]	 Loss: 0.000007
Training epoch 52 [52480/67107 (78.24%)]	 Loss: 0.000034
Training epoch 52 [53120/67107 (79.20%)]	 Loss: 0.000017
Training epoch 52 [53760/67107 (80.15%)]	 Loss: 0.000008
Training epoch 52 [54400/67107 (81.11%)]	 Loss: 0.000016
Training epoch 52 [55040/67107 (82.06%)]	 Loss: 0.000014
Training epoch 52 [55680/67107 (83.02%)]	 Loss: 0.000008
Training epoch 52 [56320/67107 (83.97%)]	 Loss: 0.000008
Training epoch 52 [56960/67107 (84.92%)]	 Loss: 0.000071
Training epoch 52 [57600/67107 (85.88%)]	 Loss: 0.000009
Training epoch 52 [58240/67107 (86.83%)]	 Loss: 0.000004
Training epoch 52 [58880/67107 (87.79%)]	 Loss: 0.000003
Training epoch 52 [59520/67107 (88.74%)]	 Loss: 0.000002
Training epoch 52 [60160/67107 (89.69%)]	 Loss: 0.000004
Training epoch 52 [60800/67107 (90.65%)]	 Loss: 0.000029
Training epoch 52 [61440/67107 (91.60%)]	 Loss: 0.000027
Training epoch 52 [62080/67107 (92.56%)]	 Loss: 0.000003
Training epoch 52 [62720/67107 (93.51%)]	 Loss: 0.000031
Training epoch 52 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 52 [64000/67107 (95.42%)]	 Loss: 0.000034
Training epoch 52 [64640/67107 (96.37%)]	 Loss: 0.000013
Training epoch 52 [65280/67107 (97.33%)]	 Loss: 0.000005
Training epoch 52 [65920/67107 (98.28%)]	 Loss: 0.000018
Training epoch 52 [66560/67107 (99.24%)]	 Loss: 0.000008
Test set: Average Loss: 0.000180
Training epoch 53 [0/67107 (0.00%)]	 Loss: 0.000006
Training epoch 53 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 53 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 53 [1920/67107 (2.86%)]	 Loss: 0.000010
Training epoch 53 [2560/67107 (3.82%)]	 Loss: 0.000024
Training epoch 53 [3200/67107 (4.77%)]	 Loss: 0.000020
Training epoch 53 [3840/67107 (5.73%)]	 Loss: 0.000012
Training epoch 53 [4480/67107 (6.68%)]	 Loss: 0.000024
Training epoch 53 [5120/67107 (7.63%)]	 Loss: 0.000007
Training epoch 53 [5760/67107 (8.59%)]	 Loss: 0.000019
Training epoch 53 [6400/67107 (9.54%)]	 Loss: 0.000038
Training epoch 53 [7040/67107 (10.50%)]	 Loss: 0.000014
Training epoch 53 [7680/67107 (11.45%)]	 Loss: 0.000010
Training epoch 53 [8320/67107 (12.40%)]	 Loss: 0.000023
Training epoch 53 [8960/67107 (13.36%)]	 Loss: 0.000022
Training epoch 53 [9600/67107 (14.31%)]	 Loss: 0.000051
Training epoch 53 [10240/67107 (15.27%)]	 Loss: 0.000007
Training epoch 53 [10880/67107 (16.22%)]	 Loss: 0.000007
Training epoch 53 [11520/67107 (17.18%)]	 Loss: 0.000004
Training epoch 53 [12160/67107 (18.13%)]	 Loss: 0.000010
Training epoch 53 [12800/67107 (19.08%)]	 Loss: 0.000002
Training epoch 53 [13440/67107 (20.04%)]	 Loss: 0.000048
Training epoch 53 [14080/67107 (20.99%)]	 Loss: 0.000006
Training epoch 53 [14720/67107 (21.95%)]	 Loss: 0.000015
Training epoch 53 [15360/67107 (22.90%)]	 Loss: 0.000025
Training epoch 53 [16000/67107 (23.85%)]	 Loss: 0.000056
Training epoch 53 [16640/67107 (24.81%)]	 Loss: 0.000022
Training epoch 53 [17280/67107 (25.76%)]	 Loss: 0.000020
Training epoch 53 [17920/67107 (26.72%)]	 Loss: 0.000010
Training epoch 53 [18560/67107 (27.67%)]	 Loss: 0.000009
Training epoch 53 [19200/67107 (28.63%)]	 Loss: 0.000003
Training epoch 53 [19840/67107 (29.58%)]	 Loss: 0.000004
Training epoch 53 [20480/67107 (30.53%)]	 Loss: 0.000018
Training epoch 53 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 53 [21760/67107 (32.44%)]	 Loss: 0.000007
Training epoch 53 [22400/67107 (33.40%)]	 Loss: 0.000008
Training epoch 53 [23040/67107 (34.35%)]	 Loss: 0.000007
Training epoch 53 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 53 [24320/67107 (36.26%)]	 Loss: 0.000014
Training epoch 53 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 53 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 53 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 53 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 53 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 53 [28160/67107 (41.98%)]	 Loss: 0.000002
Training epoch 53 [28800/67107 (42.94%)]	 Loss: 0.000004
Training epoch 53 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 53 [30080/67107 (44.85%)]	 Loss: 0.000011
Training epoch 53 [30720/67107 (45.80%)]	 Loss: 0.000027
Training epoch 53 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 53 [32000/67107 (47.71%)]	 Loss: 0.000003
Training epoch 53 [32640/67107 (48.66%)]	 Loss: 0.000021
Training epoch 53 [33280/67107 (49.62%)]	 Loss: 0.000035
Training epoch 53 [33920/67107 (50.57%)]	 Loss: 0.000131
Training epoch 53 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 53 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 53 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 53 [36480/67107 (54.39%)]	 Loss: 0.000009
Training epoch 53 [37120/67107 (55.34%)]	 Loss: 0.000009
Training epoch 53 [37760/67107 (56.30%)]	 Loss: 0.000008
Training epoch 53 [38400/67107 (57.25%)]	 Loss: 0.000018
Training epoch 53 [39040/67107 (58.21%)]	 Loss: 0.000012
Training epoch 53 [39680/67107 (59.16%)]	 Loss: 0.000012
Training epoch 53 [40320/67107 (60.11%)]	 Loss: 0.000012
Training epoch 53 [40960/67107 (61.07%)]	 Loss: 0.000025
Training epoch 53 [41600/67107 (62.02%)]	 Loss: 0.000021
Training epoch 53 [42240/67107 (62.98%)]	 Loss: 0.000006
Training epoch 53 [42880/67107 (63.93%)]	 Loss: 0.000002
Training epoch 53 [43520/67107 (64.89%)]	 Loss: 0.000009
Training epoch 53 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 53 [44800/67107 (66.79%)]	 Loss: 0.000025
Training epoch 53 [45440/67107 (67.75%)]	 Loss: 0.000011
Training epoch 53 [46080/67107 (68.70%)]	 Loss: 0.000085
Training epoch 53 [46720/67107 (69.66%)]	 Loss: 0.000008
Training epoch 53 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 53 [48000/67107 (71.56%)]	 Loss: 0.000060
Training epoch 53 [48640/67107 (72.52%)]	 Loss: 0.000020
Training epoch 53 [49280/67107 (73.47%)]	 Loss: 0.000017
Training epoch 53 [49920/67107 (74.43%)]	 Loss: 0.000014
Training epoch 53 [50560/67107 (75.38%)]	 Loss: 0.000016
Training epoch 53 [51200/67107 (76.34%)]	 Loss: 0.000013
Training epoch 53 [51840/67107 (77.29%)]	 Loss: 0.000046
Training epoch 53 [52480/67107 (78.24%)]	 Loss: 0.000036
Training epoch 53 [53120/67107 (79.20%)]	 Loss: 0.000046
Training epoch 53 [53760/67107 (80.15%)]	 Loss: 0.000004
Training epoch 53 [54400/67107 (81.11%)]	 Loss: 0.000019
Training epoch 53 [55040/67107 (82.06%)]	 Loss: 0.000005
Training epoch 53 [55680/67107 (83.02%)]	 Loss: 0.000028
Training epoch 53 [56320/67107 (83.97%)]	 Loss: 0.000012
Training epoch 53 [56960/67107 (84.92%)]	 Loss: 0.000019
Training epoch 53 [57600/67107 (85.88%)]	 Loss: 0.000026
Training epoch 53 [58240/67107 (86.83%)]	 Loss: 0.000030
Training epoch 53 [58880/67107 (87.79%)]	 Loss: 0.000161
Training epoch 53 [59520/67107 (88.74%)]	 Loss: 0.000014
Training epoch 53 [60160/67107 (89.69%)]	 Loss: 0.000086
Training epoch 53 [60800/67107 (90.65%)]	 Loss: 0.000009
Training epoch 53 [61440/67107 (91.60%)]	 Loss: 0.000018
Training epoch 53 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 53 [62720/67107 (93.51%)]	 Loss: 0.000006
Training epoch 53 [63360/67107 (94.47%)]	 Loss: 0.000010
Training epoch 53 [64000/67107 (95.42%)]	 Loss: 0.000007
Training epoch 53 [64640/67107 (96.37%)]	 Loss: 0.000045
Training epoch 53 [65280/67107 (97.33%)]	 Loss: 0.000014
Training epoch 53 [65920/67107 (98.28%)]	 Loss: 0.000008
Training epoch 53 [66560/67107 (99.24%)]	 Loss: 0.000014
Test set: Average Loss: 0.000405
Training epoch 54 [0/67107 (0.00%)]	 Loss: 0.000059
Training epoch 54 [640/67107 (0.95%)]	 Loss: 0.000013
Training epoch 54 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 54 [1920/67107 (2.86%)]	 Loss: 0.000002
Training epoch 54 [2560/67107 (3.82%)]	 Loss: 0.000024
Training epoch 54 [3200/67107 (4.77%)]	 Loss: 0.000010
Training epoch 54 [3840/67107 (5.73%)]	 Loss: 0.000006
Training epoch 54 [4480/67107 (6.68%)]	 Loss: 0.000035
Training epoch 54 [5120/67107 (7.63%)]	 Loss: 0.000014
Training epoch 54 [5760/67107 (8.59%)]	 Loss: 0.000008
Training epoch 54 [6400/67107 (9.54%)]	 Loss: 0.000031
Training epoch 54 [7040/67107 (10.50%)]	 Loss: 0.000010
Training epoch 54 [7680/67107 (11.45%)]	 Loss: 0.000023
Training epoch 54 [8320/67107 (12.40%)]	 Loss: 0.000002
Training epoch 54 [8960/67107 (13.36%)]	 Loss: 0.000005
Training epoch 54 [9600/67107 (14.31%)]	 Loss: 0.000007
Training epoch 54 [10240/67107 (15.27%)]	 Loss: 0.000002
Training epoch 54 [10880/67107 (16.22%)]	 Loss: 0.000014
Training epoch 54 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 54 [12160/67107 (18.13%)]	 Loss: 0.000023
Training epoch 54 [12800/67107 (19.08%)]	 Loss: 0.000018
Training epoch 54 [13440/67107 (20.04%)]	 Loss: 0.000021
Training epoch 54 [14080/67107 (20.99%)]	 Loss: 0.000018
Training epoch 54 [14720/67107 (21.95%)]	 Loss: 0.000006
Training epoch 54 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 54 [16000/67107 (23.85%)]	 Loss: 0.000002
Training epoch 54 [16640/67107 (24.81%)]	 Loss: 0.000002
Training epoch 54 [17280/67107 (25.76%)]	 Loss: 0.000008
Training epoch 54 [17920/67107 (26.72%)]	 Loss: 0.000006
Training epoch 54 [18560/67107 (27.67%)]	 Loss: 0.000014
Training epoch 54 [19200/67107 (28.63%)]	 Loss: 0.000003
Training epoch 54 [19840/67107 (29.58%)]	 Loss: 0.000095
Training epoch 54 [20480/67107 (30.53%)]	 Loss: 0.000012
Training epoch 54 [21120/67107 (31.49%)]	 Loss: 0.000061
Training epoch 54 [21760/67107 (32.44%)]	 Loss: 0.000015
Training epoch 54 [22400/67107 (33.40%)]	 Loss: 0.000013
Training epoch 54 [23040/67107 (34.35%)]	 Loss: 0.000004
Training epoch 54 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 54 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 54 [24960/67107 (37.21%)]	 Loss: 0.000002
Training epoch 54 [25600/67107 (38.17%)]	 Loss: 0.000004
Training epoch 54 [26240/67107 (39.12%)]	 Loss: 0.000052
Training epoch 54 [26880/67107 (40.08%)]	 Loss: 0.000016
Training epoch 54 [27520/67107 (41.03%)]	 Loss: 0.000006
Training epoch 54 [28160/67107 (41.98%)]	 Loss: 0.000001
Training epoch 54 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 54 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 54 [30080/67107 (44.85%)]	 Loss: 0.000017
Training epoch 54 [30720/67107 (45.80%)]	 Loss: 0.000008
Training epoch 54 [31360/67107 (46.76%)]	 Loss: 0.000012
Training epoch 54 [32000/67107 (47.71%)]	 Loss: 0.000037
Training epoch 54 [32640/67107 (48.66%)]	 Loss: 0.000006
Training epoch 54 [33280/67107 (49.62%)]	 Loss: 0.000035
Training epoch 54 [33920/67107 (50.57%)]	 Loss: 0.000027
Training epoch 54 [34560/67107 (51.53%)]	 Loss: 0.000007
Training epoch 54 [35200/67107 (52.48%)]	 Loss: 0.000017
Training epoch 54 [35840/67107 (53.44%)]	 Loss: 0.000013
Training epoch 54 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 54 [37120/67107 (55.34%)]	 Loss: 0.000004
Training epoch 54 [37760/67107 (56.30%)]	 Loss: 0.000003
Training epoch 54 [38400/67107 (57.25%)]	 Loss: 0.000004
Training epoch 54 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 54 [39680/67107 (59.16%)]	 Loss: 0.000007
Training epoch 54 [40320/67107 (60.11%)]	 Loss: 0.000006
Training epoch 54 [40960/67107 (61.07%)]	 Loss: 0.000022
Training epoch 54 [41600/67107 (62.02%)]	 Loss: 0.000019
Training epoch 54 [42240/67107 (62.98%)]	 Loss: 0.000046
Training epoch 54 [42880/67107 (63.93%)]	 Loss: 0.000011
Training epoch 54 [43520/67107 (64.89%)]	 Loss: 0.000015
Training epoch 54 [44160/67107 (65.84%)]	 Loss: 0.000048
Training epoch 54 [44800/67107 (66.79%)]	 Loss: 0.000030
Training epoch 54 [45440/67107 (67.75%)]	 Loss: 0.000008
Training epoch 54 [46080/67107 (68.70%)]	 Loss: 0.000017
Training epoch 54 [46720/67107 (69.66%)]	 Loss: 0.000009
Training epoch 54 [47360/67107 (70.61%)]	 Loss: 0.000011
Training epoch 54 [48000/67107 (71.56%)]	 Loss: 0.000009
Training epoch 54 [48640/67107 (72.52%)]	 Loss: 0.000046
Training epoch 54 [49280/67107 (73.47%)]	 Loss: 0.000011
Training epoch 54 [49920/67107 (74.43%)]	 Loss: 0.000015
Training epoch 54 [50560/67107 (75.38%)]	 Loss: 0.000006
Training epoch 54 [51200/67107 (76.34%)]	 Loss: 0.000003
Training epoch 54 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 54 [52480/67107 (78.24%)]	 Loss: 0.000011
Training epoch 54 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 54 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 54 [54400/67107 (81.11%)]	 Loss: 0.000003
Training epoch 54 [55040/67107 (82.06%)]	 Loss: 0.000021
Training epoch 54 [55680/67107 (83.02%)]	 Loss: 0.000042
Training epoch 54 [56320/67107 (83.97%)]	 Loss: 0.000030
Training epoch 54 [56960/67107 (84.92%)]	 Loss: 0.000006
Training epoch 54 [57600/67107 (85.88%)]	 Loss: 0.000012
Training epoch 54 [58240/67107 (86.83%)]	 Loss: 0.000005
Training epoch 54 [58880/67107 (87.79%)]	 Loss: 0.000029
Training epoch 54 [59520/67107 (88.74%)]	 Loss: 0.000003
Training epoch 54 [60160/67107 (89.69%)]	 Loss: 0.000002
Training epoch 54 [60800/67107 (90.65%)]	 Loss: 0.000004
Training epoch 54 [61440/67107 (91.60%)]	 Loss: 0.000003
Training epoch 54 [62080/67107 (92.56%)]	 Loss: 0.000015
Training epoch 54 [62720/67107 (93.51%)]	 Loss: 0.000028
Training epoch 54 [63360/67107 (94.47%)]	 Loss: 0.000018
Training epoch 54 [64000/67107 (95.42%)]	 Loss: 0.000007
Training epoch 54 [64640/67107 (96.37%)]	 Loss: 0.000009
Training epoch 54 [65280/67107 (97.33%)]	 Loss: 0.000041
Training epoch 54 [65920/67107 (98.28%)]	 Loss: 0.000015
Training epoch 54 [66560/67107 (99.24%)]	 Loss: 0.000019
Test set: Average Loss: 0.000156
Training epoch 55 [0/67107 (0.00%)]	 Loss: 0.000008
Training epoch 55 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 55 [1280/67107 (1.91%)]	 Loss: 0.000004
Training epoch 55 [1920/67107 (2.86%)]	 Loss: 0.000005
Training epoch 55 [2560/67107 (3.82%)]	 Loss: 0.000020
Training epoch 55 [3200/67107 (4.77%)]	 Loss: 0.000036
Training epoch 55 [3840/67107 (5.73%)]	 Loss: 0.000001
Training epoch 55 [4480/67107 (6.68%)]	 Loss: 0.000011
Training epoch 55 [5120/67107 (7.63%)]	 Loss: 0.000023
Training epoch 55 [5760/67107 (8.59%)]	 Loss: 0.000013
Training epoch 55 [6400/67107 (9.54%)]	 Loss: 0.000010
Training epoch 55 [7040/67107 (10.50%)]	 Loss: 0.000013
Training epoch 55 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 55 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 55 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 55 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 55 [10240/67107 (15.27%)]	 Loss: 0.000003
Training epoch 55 [10880/67107 (16.22%)]	 Loss: 0.000010
Training epoch 55 [11520/67107 (17.18%)]	 Loss: 0.000015
Training epoch 55 [12160/67107 (18.13%)]	 Loss: 0.000039
Training epoch 55 [12800/67107 (19.08%)]	 Loss: 0.000012
Training epoch 55 [13440/67107 (20.04%)]	 Loss: 0.000007
Training epoch 55 [14080/67107 (20.99%)]	 Loss: 0.000026
Training epoch 55 [14720/67107 (21.95%)]	 Loss: 0.000029
Training epoch 55 [15360/67107 (22.90%)]	 Loss: 0.000018
Training epoch 55 [16000/67107 (23.85%)]	 Loss: 0.000005
Training epoch 55 [16640/67107 (24.81%)]	 Loss: 0.000003
Training epoch 55 [17280/67107 (25.76%)]	 Loss: 0.000015
Training epoch 55 [17920/67107 (26.72%)]	 Loss: 0.000020
Training epoch 55 [18560/67107 (27.67%)]	 Loss: 0.000007
Training epoch 55 [19200/67107 (28.63%)]	 Loss: 0.000026
Training epoch 55 [19840/67107 (29.58%)]	 Loss: 0.000012
Training epoch 55 [20480/67107 (30.53%)]	 Loss: 0.000034
Training epoch 55 [21120/67107 (31.49%)]	 Loss: 0.000015
Training epoch 55 [21760/67107 (32.44%)]	 Loss: 0.000018
Training epoch 55 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 55 [23040/67107 (34.35%)]	 Loss: 0.000008
Training epoch 55 [23680/67107 (35.31%)]	 Loss: 0.000005
Training epoch 55 [24320/67107 (36.26%)]	 Loss: 0.000008
Training epoch 55 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 55 [25600/67107 (38.17%)]	 Loss: 0.000028
Training epoch 55 [26240/67107 (39.12%)]	 Loss: 0.000016
Training epoch 55 [26880/67107 (40.08%)]	 Loss: 0.000009
Training epoch 55 [27520/67107 (41.03%)]	 Loss: 0.000008
Training epoch 55 [28160/67107 (41.98%)]	 Loss: 0.000031
Training epoch 55 [28800/67107 (42.94%)]	 Loss: 0.000029
Training epoch 55 [29440/67107 (43.89%)]	 Loss: 0.000010
Training epoch 55 [30080/67107 (44.85%)]	 Loss: 0.000032
Training epoch 55 [30720/67107 (45.80%)]	 Loss: 0.000016
Training epoch 55 [31360/67107 (46.76%)]	 Loss: 0.000024
Training epoch 55 [32000/67107 (47.71%)]	 Loss: 0.000040
Training epoch 55 [32640/67107 (48.66%)]	 Loss: 0.000061
Training epoch 55 [33280/67107 (49.62%)]	 Loss: 0.000004
Training epoch 55 [33920/67107 (50.57%)]	 Loss: 0.000034
Training epoch 55 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 55 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 55 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 55 [36480/67107 (54.39%)]	 Loss: 0.000002
Training epoch 55 [37120/67107 (55.34%)]	 Loss: 0.000008
Training epoch 55 [37760/67107 (56.30%)]	 Loss: 0.000076
Training epoch 55 [38400/67107 (57.25%)]	 Loss: 0.000004
Training epoch 55 [39040/67107 (58.21%)]	 Loss: 0.000008
Training epoch 55 [39680/67107 (59.16%)]	 Loss: 0.000017
Training epoch 55 [40320/67107 (60.11%)]	 Loss: 0.000062
Training epoch 55 [40960/67107 (61.07%)]	 Loss: 0.000019
Training epoch 55 [41600/67107 (62.02%)]	 Loss: 0.000008
Training epoch 55 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 55 [42880/67107 (63.93%)]	 Loss: 0.000011
Training epoch 55 [43520/67107 (64.89%)]	 Loss: 0.000007
Training epoch 55 [44160/67107 (65.84%)]	 Loss: 0.000004
Training epoch 55 [44800/67107 (66.79%)]	 Loss: 0.000020
Training epoch 55 [45440/67107 (67.75%)]	 Loss: 0.000013
Training epoch 55 [46080/67107 (68.70%)]	 Loss: 0.000047
Training epoch 55 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 55 [47360/67107 (70.61%)]	 Loss: 0.000010
Training epoch 55 [48000/67107 (71.56%)]	 Loss: 0.000011
Training epoch 55 [48640/67107 (72.52%)]	 Loss: 0.000046
Training epoch 55 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 55 [49920/67107 (74.43%)]	 Loss: 0.000013
Training epoch 55 [50560/67107 (75.38%)]	 Loss: 0.000011
Training epoch 55 [51200/67107 (76.34%)]	 Loss: 0.000012
Training epoch 55 [51840/67107 (77.29%)]	 Loss: 0.000003
Training epoch 55 [52480/67107 (78.24%)]	 Loss: 0.000029
Training epoch 55 [53120/67107 (79.20%)]	 Loss: 0.000021
Training epoch 55 [53760/67107 (80.15%)]	 Loss: 0.000013
Training epoch 55 [54400/67107 (81.11%)]	 Loss: 0.000005
Training epoch 55 [55040/67107 (82.06%)]	 Loss: 0.000017
Training epoch 55 [55680/67107 (83.02%)]	 Loss: 0.000013
Training epoch 55 [56320/67107 (83.97%)]	 Loss: 0.000077
Training epoch 55 [56960/67107 (84.92%)]	 Loss: 0.000005
Training epoch 55 [57600/67107 (85.88%)]	 Loss: 0.000014
Training epoch 55 [58240/67107 (86.83%)]	 Loss: 0.000015
Training epoch 55 [58880/67107 (87.79%)]	 Loss: 0.000012
Training epoch 55 [59520/67107 (88.74%)]	 Loss: 0.000020
Training epoch 55 [60160/67107 (89.69%)]	 Loss: 0.000007
Training epoch 55 [60800/67107 (90.65%)]	 Loss: 0.000031
Training epoch 55 [61440/67107 (91.60%)]	 Loss: 0.000005
Training epoch 55 [62080/67107 (92.56%)]	 Loss: 0.000002
Training epoch 55 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 55 [63360/67107 (94.47%)]	 Loss: 0.000004
Training epoch 55 [64000/67107 (95.42%)]	 Loss: 0.000002
Training epoch 55 [64640/67107 (96.37%)]	 Loss: 0.000010
Training epoch 55 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 55 [65920/67107 (98.28%)]	 Loss: 0.000011
Training epoch 55 [66560/67107 (99.24%)]	 Loss: 0.000014
Test set: Average Loss: 0.000177
Training epoch 56 [0/67107 (0.00%)]	 Loss: 0.000048
Training epoch 56 [640/67107 (0.95%)]	 Loss: 0.000009
Training epoch 56 [1280/67107 (1.91%)]	 Loss: 0.000007
Training epoch 56 [1920/67107 (2.86%)]	 Loss: 0.000005
Training epoch 56 [2560/67107 (3.82%)]	 Loss: 0.000013
Training epoch 56 [3200/67107 (4.77%)]	 Loss: 0.000074
Training epoch 56 [3840/67107 (5.73%)]	 Loss: 0.000009
Training epoch 56 [4480/67107 (6.68%)]	 Loss: 0.000004
Training epoch 56 [5120/67107 (7.63%)]	 Loss: 0.000032
Training epoch 56 [5760/67107 (8.59%)]	 Loss: 0.000019
Training epoch 56 [6400/67107 (9.54%)]	 Loss: 0.000044
Training epoch 56 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 56 [7680/67107 (11.45%)]	 Loss: 0.000008
Training epoch 56 [8320/67107 (12.40%)]	 Loss: 0.000006
Training epoch 56 [8960/67107 (13.36%)]	 Loss: 0.000012
Training epoch 56 [9600/67107 (14.31%)]	 Loss: 0.000016
Training epoch 56 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 56 [10880/67107 (16.22%)]	 Loss: 0.000004
Training epoch 56 [11520/67107 (17.18%)]	 Loss: 0.000007
Training epoch 56 [12160/67107 (18.13%)]	 Loss: 0.000089
Training epoch 56 [12800/67107 (19.08%)]	 Loss: 0.000018
Training epoch 56 [13440/67107 (20.04%)]	 Loss: 0.000028
Training epoch 56 [14080/67107 (20.99%)]	 Loss: 0.000011
Training epoch 56 [14720/67107 (21.95%)]	 Loss: 0.000048
Training epoch 56 [15360/67107 (22.90%)]	 Loss: 0.000045
Training epoch 56 [16000/67107 (23.85%)]	 Loss: 0.000006
Training epoch 56 [16640/67107 (24.81%)]	 Loss: 0.000023
Training epoch 56 [17280/67107 (25.76%)]	 Loss: 0.000006
Training epoch 56 [17920/67107 (26.72%)]	 Loss: 0.000006
Training epoch 56 [18560/67107 (27.67%)]	 Loss: 0.000002
Training epoch 56 [19200/67107 (28.63%)]	 Loss: 0.000005
Training epoch 56 [19840/67107 (29.58%)]	 Loss: 0.000061
Training epoch 56 [20480/67107 (30.53%)]	 Loss: 0.000025
Training epoch 56 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 56 [21760/67107 (32.44%)]	 Loss: 0.000022
Training epoch 56 [22400/67107 (33.40%)]	 Loss: 0.000025
Training epoch 56 [23040/67107 (34.35%)]	 Loss: 0.000004
Training epoch 56 [23680/67107 (35.31%)]	 Loss: 0.000005
Training epoch 56 [24320/67107 (36.26%)]	 Loss: 0.000014
Training epoch 56 [24960/67107 (37.21%)]	 Loss: 0.000011
Training epoch 56 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 56 [26240/67107 (39.12%)]	 Loss: 0.000007
Training epoch 56 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 56 [27520/67107 (41.03%)]	 Loss: 0.000034
Training epoch 56 [28160/67107 (41.98%)]	 Loss: 0.000007
Training epoch 56 [28800/67107 (42.94%)]	 Loss: 0.000019
Training epoch 56 [29440/67107 (43.89%)]	 Loss: 0.000034
Training epoch 56 [30080/67107 (44.85%)]	 Loss: 0.000015
Training epoch 56 [30720/67107 (45.80%)]	 Loss: 0.000010
Training epoch 56 [31360/67107 (46.76%)]	 Loss: 0.000048
Training epoch 56 [32000/67107 (47.71%)]	 Loss: 0.000009
Training epoch 56 [32640/67107 (48.66%)]	 Loss: 0.000040
Training epoch 56 [33280/67107 (49.62%)]	 Loss: 0.000029
Training epoch 56 [33920/67107 (50.57%)]	 Loss: 0.000018
Training epoch 56 [34560/67107 (51.53%)]	 Loss: 0.000036
Training epoch 56 [35200/67107 (52.48%)]	 Loss: 0.000008
Training epoch 56 [35840/67107 (53.44%)]	 Loss: 0.000051
Training epoch 56 [36480/67107 (54.39%)]	 Loss: 0.000003
Training epoch 56 [37120/67107 (55.34%)]	 Loss: 0.000007
Training epoch 56 [37760/67107 (56.30%)]	 Loss: 0.000008
Training epoch 56 [38400/67107 (57.25%)]	 Loss: 0.000006
Training epoch 56 [39040/67107 (58.21%)]	 Loss: 0.000002
Training epoch 56 [39680/67107 (59.16%)]	 Loss: 0.000003
Training epoch 56 [40320/67107 (60.11%)]	 Loss: 0.000005
Training epoch 56 [40960/67107 (61.07%)]	 Loss: 0.000051
Training epoch 56 [41600/67107 (62.02%)]	 Loss: 0.000016
Training epoch 56 [42240/67107 (62.98%)]	 Loss: 0.000027
Training epoch 56 [42880/67107 (63.93%)]	 Loss: 0.000020
Training epoch 56 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 56 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 56 [44800/67107 (66.79%)]	 Loss: 0.000013
Training epoch 56 [45440/67107 (67.75%)]	 Loss: 0.000042
Training epoch 56 [46080/67107 (68.70%)]	 Loss: 0.000011
Training epoch 56 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 56 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 56 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 56 [48640/67107 (72.52%)]	 Loss: 0.000026
Training epoch 56 [49280/67107 (73.47%)]	 Loss: 0.000019
Training epoch 56 [49920/67107 (74.43%)]	 Loss: 0.000013
Training epoch 56 [50560/67107 (75.38%)]	 Loss: 0.000023
Training epoch 56 [51200/67107 (76.34%)]	 Loss: 0.000009
Training epoch 56 [51840/67107 (77.29%)]	 Loss: 0.000018
Training epoch 56 [52480/67107 (78.24%)]	 Loss: 0.000004
Training epoch 56 [53120/67107 (79.20%)]	 Loss: 0.000018
Training epoch 56 [53760/67107 (80.15%)]	 Loss: 0.000034
Training epoch 56 [54400/67107 (81.11%)]	 Loss: 0.000024
Training epoch 56 [55040/67107 (82.06%)]	 Loss: 0.000038
Training epoch 56 [55680/67107 (83.02%)]	 Loss: 0.000044
Training epoch 56 [56320/67107 (83.97%)]	 Loss: 0.000070
Training epoch 56 [56960/67107 (84.92%)]	 Loss: 0.000007
Training epoch 56 [57600/67107 (85.88%)]	 Loss: 0.000006
Training epoch 56 [58240/67107 (86.83%)]	 Loss: 0.000008
Training epoch 56 [58880/67107 (87.79%)]	 Loss: 0.000003
Training epoch 56 [59520/67107 (88.74%)]	 Loss: 0.000028
Training epoch 56 [60160/67107 (89.69%)]	 Loss: 0.000019
Training epoch 56 [60800/67107 (90.65%)]	 Loss: 0.000016
Training epoch 56 [61440/67107 (91.60%)]	 Loss: 0.000013
Training epoch 56 [62080/67107 (92.56%)]	 Loss: 0.000064
Training epoch 56 [62720/67107 (93.51%)]	 Loss: 0.000005
Training epoch 56 [63360/67107 (94.47%)]	 Loss: 0.000010
Training epoch 56 [64000/67107 (95.42%)]	 Loss: 0.000008
Training epoch 56 [64640/67107 (96.37%)]	 Loss: 0.000003
Training epoch 56 [65280/67107 (97.33%)]	 Loss: 0.000001
Training epoch 56 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 56 [66560/67107 (99.24%)]	 Loss: 0.000023
Test set: Average Loss: 0.000189
Training epoch 57 [0/67107 (0.00%)]	 Loss: 0.000024
Training epoch 57 [640/67107 (0.95%)]	 Loss: 0.000028
Training epoch 57 [1280/67107 (1.91%)]	 Loss: 0.000011
Training epoch 57 [1920/67107 (2.86%)]	 Loss: 0.000049
Training epoch 57 [2560/67107 (3.82%)]	 Loss: 0.000001
Training epoch 57 [3200/67107 (4.77%)]	 Loss: 0.000019
Training epoch 57 [3840/67107 (5.73%)]	 Loss: 0.000022
Training epoch 57 [4480/67107 (6.68%)]	 Loss: 0.000004
Training epoch 57 [5120/67107 (7.63%)]	 Loss: 0.000010
Training epoch 57 [5760/67107 (8.59%)]	 Loss: 0.000028
Training epoch 57 [6400/67107 (9.54%)]	 Loss: 0.000014
Training epoch 57 [7040/67107 (10.50%)]	 Loss: 0.000016
Training epoch 57 [7680/67107 (11.45%)]	 Loss: 0.000017
Training epoch 57 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 57 [8960/67107 (13.36%)]	 Loss: 0.000008
Training epoch 57 [9600/67107 (14.31%)]	 Loss: 0.000002
Training epoch 57 [10240/67107 (15.27%)]	 Loss: 0.000010
Training epoch 57 [10880/67107 (16.22%)]	 Loss: 0.000023
Training epoch 57 [11520/67107 (17.18%)]	 Loss: 0.000008
Training epoch 57 [12160/67107 (18.13%)]	 Loss: 0.000043
Training epoch 57 [12800/67107 (19.08%)]	 Loss: 0.000088
Training epoch 57 [13440/67107 (20.04%)]	 Loss: 0.000038
Training epoch 57 [14080/67107 (20.99%)]	 Loss: 0.000033
Training epoch 57 [14720/67107 (21.95%)]	 Loss: 0.000064
Training epoch 57 [15360/67107 (22.90%)]	 Loss: 0.000005
Training epoch 57 [16000/67107 (23.85%)]	 Loss: 0.000006
Training epoch 57 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 57 [17280/67107 (25.76%)]	 Loss: 0.000013
Training epoch 57 [17920/67107 (26.72%)]	 Loss: 0.000170
Training epoch 57 [18560/67107 (27.67%)]	 Loss: 0.000112
Training epoch 57 [19200/67107 (28.63%)]	 Loss: 0.000007
Training epoch 57 [19840/67107 (29.58%)]	 Loss: 0.000041
Training epoch 57 [20480/67107 (30.53%)]	 Loss: 0.000024
Training epoch 57 [21120/67107 (31.49%)]	 Loss: 0.000040
Training epoch 57 [21760/67107 (32.44%)]	 Loss: 0.000039
Training epoch 57 [22400/67107 (33.40%)]	 Loss: 0.000020
Training epoch 57 [23040/67107 (34.35%)]	 Loss: 0.000036
Training epoch 57 [23680/67107 (35.31%)]	 Loss: 0.000032
Training epoch 57 [24320/67107 (36.26%)]	 Loss: 0.000005
Training epoch 57 [24960/67107 (37.21%)]	 Loss: 0.000004
Training epoch 57 [25600/67107 (38.17%)]	 Loss: 0.000009
Training epoch 57 [26240/67107 (39.12%)]	 Loss: 0.000006
Training epoch 57 [26880/67107 (40.08%)]	 Loss: 0.000005
Training epoch 57 [27520/67107 (41.03%)]	 Loss: 0.000006
Training epoch 57 [28160/67107 (41.98%)]	 Loss: 0.000017
Training epoch 57 [28800/67107 (42.94%)]	 Loss: 0.000018
Training epoch 57 [29440/67107 (43.89%)]	 Loss: 0.000007
Training epoch 57 [30080/67107 (44.85%)]	 Loss: 0.000011
Training epoch 57 [30720/67107 (45.80%)]	 Loss: 0.000012
Training epoch 57 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 57 [32000/67107 (47.71%)]	 Loss: 0.000020
Training epoch 57 [32640/67107 (48.66%)]	 Loss: 0.000008
Training epoch 57 [33280/67107 (49.62%)]	 Loss: 0.000020
Training epoch 57 [33920/67107 (50.57%)]	 Loss: 0.000003
Training epoch 57 [34560/67107 (51.53%)]	 Loss: 0.000041
Training epoch 57 [35200/67107 (52.48%)]	 Loss: 0.000011
Training epoch 57 [35840/67107 (53.44%)]	 Loss: 0.000006
Training epoch 57 [36480/67107 (54.39%)]	 Loss: 0.000032
Training epoch 57 [37120/67107 (55.34%)]	 Loss: 0.000020
Training epoch 57 [37760/67107 (56.30%)]	 Loss: 0.000002
Training epoch 57 [38400/67107 (57.25%)]	 Loss: 0.000014
Training epoch 57 [39040/67107 (58.21%)]	 Loss: 0.000024
Training epoch 57 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 57 [40320/67107 (60.11%)]	 Loss: 0.000005
Training epoch 57 [40960/67107 (61.07%)]	 Loss: 0.000014
Training epoch 57 [41600/67107 (62.02%)]	 Loss: 0.000034
Training epoch 57 [42240/67107 (62.98%)]	 Loss: 0.000006
Training epoch 57 [42880/67107 (63.93%)]	 Loss: 0.000001
Training epoch 57 [43520/67107 (64.89%)]	 Loss: 0.000008
Training epoch 57 [44160/67107 (65.84%)]	 Loss: 0.000010
Training epoch 57 [44800/67107 (66.79%)]	 Loss: 0.000006
Training epoch 57 [45440/67107 (67.75%)]	 Loss: 0.000016
Training epoch 57 [46080/67107 (68.70%)]	 Loss: 0.000008
Training epoch 57 [46720/67107 (69.66%)]	 Loss: 0.000006
Training epoch 57 [47360/67107 (70.61%)]	 Loss: 0.000003
Training epoch 57 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 57 [48640/67107 (72.52%)]	 Loss: 0.000021
Training epoch 57 [49280/67107 (73.47%)]	 Loss: 0.000010
Training epoch 57 [49920/67107 (74.43%)]	 Loss: 0.000009
Training epoch 57 [50560/67107 (75.38%)]	 Loss: 0.000006
Training epoch 57 [51200/67107 (76.34%)]	 Loss: 0.000017
Training epoch 57 [51840/67107 (77.29%)]	 Loss: 0.000014
Training epoch 57 [52480/67107 (78.24%)]	 Loss: 0.000021
Training epoch 57 [53120/67107 (79.20%)]	 Loss: 0.000005
Training epoch 57 [53760/67107 (80.15%)]	 Loss: 0.000004
Training epoch 57 [54400/67107 (81.11%)]	 Loss: 0.000026
Training epoch 57 [55040/67107 (82.06%)]	 Loss: 0.000010
Training epoch 57 [55680/67107 (83.02%)]	 Loss: 0.000006
Training epoch 57 [56320/67107 (83.97%)]	 Loss: 0.000004
Training epoch 57 [56960/67107 (84.92%)]	 Loss: 0.000008
Training epoch 57 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 57 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 57 [58880/67107 (87.79%)]	 Loss: 0.000015
Training epoch 57 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 57 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 57 [60800/67107 (90.65%)]	 Loss: 0.000001
Training epoch 57 [61440/67107 (91.60%)]	 Loss: 0.000003
Training epoch 57 [62080/67107 (92.56%)]	 Loss: 0.000011
Training epoch 57 [62720/67107 (93.51%)]	 Loss: 0.000012
Training epoch 57 [63360/67107 (94.47%)]	 Loss: 0.000012
Training epoch 57 [64000/67107 (95.42%)]	 Loss: 0.000019
Training epoch 57 [64640/67107 (96.37%)]	 Loss: 0.000027
Training epoch 57 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 57 [65920/67107 (98.28%)]	 Loss: 0.000014
Training epoch 57 [66560/67107 (99.24%)]	 Loss: 0.000048
Test set: Average Loss: 0.000381
Training epoch 58 [0/67107 (0.00%)]	 Loss: 0.000009
Training epoch 58 [640/67107 (0.95%)]	 Loss: 0.000009
Training epoch 58 [1280/67107 (1.91%)]	 Loss: 0.000008
Training epoch 58 [1920/67107 (2.86%)]	 Loss: 0.000003
Training epoch 58 [2560/67107 (3.82%)]	 Loss: 0.000044
Training epoch 58 [3200/67107 (4.77%)]	 Loss: 0.000004
Training epoch 58 [3840/67107 (5.73%)]	 Loss: 0.000005
Training epoch 58 [4480/67107 (6.68%)]	 Loss: 0.000006
Training epoch 58 [5120/67107 (7.63%)]	 Loss: 0.000017
Training epoch 58 [5760/67107 (8.59%)]	 Loss: 0.000013
Training epoch 58 [6400/67107 (9.54%)]	 Loss: 0.000013
Training epoch 58 [7040/67107 (10.50%)]	 Loss: 0.000008
Training epoch 58 [7680/67107 (11.45%)]	 Loss: 0.000012
Training epoch 58 [8320/67107 (12.40%)]	 Loss: 0.000007
Training epoch 58 [8960/67107 (13.36%)]	 Loss: 0.000012
Training epoch 58 [9600/67107 (14.31%)]	 Loss: 0.000012
Training epoch 58 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 58 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 58 [11520/67107 (17.18%)]	 Loss: 0.000003
Training epoch 58 [12160/67107 (18.13%)]	 Loss: 0.000009
Training epoch 58 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 58 [13440/67107 (20.04%)]	 Loss: 0.000019
Training epoch 58 [14080/67107 (20.99%)]	 Loss: 0.000042
Training epoch 58 [14720/67107 (21.95%)]	 Loss: 0.000006
Training epoch 58 [15360/67107 (22.90%)]	 Loss: 0.000013
Training epoch 58 [16000/67107 (23.85%)]	 Loss: 0.000028
Training epoch 58 [16640/67107 (24.81%)]	 Loss: 0.000010
Training epoch 58 [17280/67107 (25.76%)]	 Loss: 0.000093
Training epoch 58 [17920/67107 (26.72%)]	 Loss: 0.000120
Training epoch 58 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 58 [19200/67107 (28.63%)]	 Loss: 0.000003
Training epoch 58 [19840/67107 (29.58%)]	 Loss: 0.000015
Training epoch 58 [20480/67107 (30.53%)]	 Loss: 0.000015
Training epoch 58 [21120/67107 (31.49%)]	 Loss: 0.000036
Training epoch 58 [21760/67107 (32.44%)]	 Loss: 0.000009
Training epoch 58 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 58 [23040/67107 (34.35%)]	 Loss: 0.000008
Training epoch 58 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 58 [24320/67107 (36.26%)]	 Loss: 0.000006
Training epoch 58 [24960/67107 (37.21%)]	 Loss: 0.000009
Training epoch 58 [25600/67107 (38.17%)]	 Loss: 0.000005
Training epoch 58 [26240/67107 (39.12%)]	 Loss: 0.000006
Training epoch 58 [26880/67107 (40.08%)]	 Loss: 0.000005
Training epoch 58 [27520/67107 (41.03%)]	 Loss: 0.000062
Training epoch 58 [28160/67107 (41.98%)]	 Loss: 0.000010
Training epoch 58 [28800/67107 (42.94%)]	 Loss: 0.000006
Training epoch 58 [29440/67107 (43.89%)]	 Loss: 0.000015
Training epoch 58 [30080/67107 (44.85%)]	 Loss: 0.000039
Training epoch 58 [30720/67107 (45.80%)]	 Loss: 0.000087
Training epoch 58 [31360/67107 (46.76%)]	 Loss: 0.000028
Training epoch 58 [32000/67107 (47.71%)]	 Loss: 0.000011
Training epoch 58 [32640/67107 (48.66%)]	 Loss: 0.000015
Training epoch 58 [33280/67107 (49.62%)]	 Loss: 0.000018
Training epoch 58 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 58 [34560/67107 (51.53%)]	 Loss: 0.000003
Training epoch 58 [35200/67107 (52.48%)]	 Loss: 0.000017
Training epoch 58 [35840/67107 (53.44%)]	 Loss: 0.000013
Training epoch 58 [36480/67107 (54.39%)]	 Loss: 0.000011
Training epoch 58 [37120/67107 (55.34%)]	 Loss: 0.000038
Training epoch 58 [37760/67107 (56.30%)]	 Loss: 0.000005
Training epoch 58 [38400/67107 (57.25%)]	 Loss: 0.000017
Training epoch 58 [39040/67107 (58.21%)]	 Loss: 0.000011
Training epoch 58 [39680/67107 (59.16%)]	 Loss: 0.000008
Training epoch 58 [40320/67107 (60.11%)]	 Loss: 0.000048
Training epoch 58 [40960/67107 (61.07%)]	 Loss: 0.000004
Training epoch 58 [41600/67107 (62.02%)]	 Loss: 0.000005
Training epoch 58 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 58 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 58 [43520/67107 (64.89%)]	 Loss: 0.000013
Training epoch 58 [44160/67107 (65.84%)]	 Loss: 0.000040
Training epoch 58 [44800/67107 (66.79%)]	 Loss: 0.000014
Training epoch 58 [45440/67107 (67.75%)]	 Loss: 0.000019
Training epoch 58 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 58 [46720/67107 (69.66%)]	 Loss: 0.000007
Training epoch 58 [47360/67107 (70.61%)]	 Loss: 0.000056
Training epoch 58 [48000/67107 (71.56%)]	 Loss: 0.000006
Training epoch 58 [48640/67107 (72.52%)]	 Loss: 0.000009
Training epoch 58 [49280/67107 (73.47%)]	 Loss: 0.000017
Training epoch 58 [49920/67107 (74.43%)]	 Loss: 0.000005
Training epoch 58 [50560/67107 (75.38%)]	 Loss: 0.000002
Training epoch 58 [51200/67107 (76.34%)]	 Loss: 0.000010
Training epoch 58 [51840/67107 (77.29%)]	 Loss: 0.000006
Training epoch 58 [52480/67107 (78.24%)]	 Loss: 0.000015
Training epoch 58 [53120/67107 (79.20%)]	 Loss: 0.000002
Training epoch 58 [53760/67107 (80.15%)]	 Loss: 0.000008
Training epoch 58 [54400/67107 (81.11%)]	 Loss: 0.000009
Training epoch 58 [55040/67107 (82.06%)]	 Loss: 0.000002
Training epoch 58 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 58 [56320/67107 (83.97%)]	 Loss: 0.000003
Training epoch 58 [56960/67107 (84.92%)]	 Loss: 0.000014
Training epoch 58 [57600/67107 (85.88%)]	 Loss: 0.000027
Training epoch 58 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 58 [58880/67107 (87.79%)]	 Loss: 0.000009
Training epoch 58 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 58 [60160/67107 (89.69%)]	 Loss: 0.000009
Training epoch 58 [60800/67107 (90.65%)]	 Loss: 0.000014
Training epoch 58 [61440/67107 (91.60%)]	 Loss: 0.000011
Training epoch 58 [62080/67107 (92.56%)]	 Loss: 0.000003
Training epoch 58 [62720/67107 (93.51%)]	 Loss: 0.000024
Training epoch 58 [63360/67107 (94.47%)]	 Loss: 0.000006
Training epoch 58 [64000/67107 (95.42%)]	 Loss: 0.000012
Training epoch 58 [64640/67107 (96.37%)]	 Loss: 0.000016
Training epoch 58 [65280/67107 (97.33%)]	 Loss: 0.000017
Training epoch 58 [65920/67107 (98.28%)]	 Loss: 0.000033
Training epoch 58 [66560/67107 (99.24%)]	 Loss: 0.000016
Test set: Average Loss: 0.006045
Training epoch 59 [0/67107 (0.00%)]	 Loss: 0.000064
Training epoch 59 [640/67107 (0.95%)]	 Loss: 0.000015
Training epoch 59 [1280/67107 (1.91%)]	 Loss: 0.000010
Training epoch 59 [1920/67107 (2.86%)]	 Loss: 0.000004
Training epoch 59 [2560/67107 (3.82%)]	 Loss: 0.000006
Training epoch 59 [3200/67107 (4.77%)]	 Loss: 0.000002
Training epoch 59 [3840/67107 (5.73%)]	 Loss: 0.000002
Training epoch 59 [4480/67107 (6.68%)]	 Loss: 0.000003
Training epoch 59 [5120/67107 (7.63%)]	 Loss: 0.000018
Training epoch 59 [5760/67107 (8.59%)]	 Loss: 0.000014
Training epoch 59 [6400/67107 (9.54%)]	 Loss: 0.000001
Training epoch 59 [7040/67107 (10.50%)]	 Loss: 0.000016
Training epoch 59 [7680/67107 (11.45%)]	 Loss: 0.000013
Training epoch 59 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 59 [8960/67107 (13.36%)]	 Loss: 0.000014
Training epoch 59 [9600/67107 (14.31%)]	 Loss: 0.000005
Training epoch 59 [10240/67107 (15.27%)]	 Loss: 0.000010
Training epoch 59 [10880/67107 (16.22%)]	 Loss: 0.000008
Training epoch 59 [11520/67107 (17.18%)]	 Loss: 0.000003
Training epoch 59 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 59 [12800/67107 (19.08%)]	 Loss: 0.000005
Training epoch 59 [13440/67107 (20.04%)]	 Loss: 0.000014
Training epoch 59 [14080/67107 (20.99%)]	 Loss: 0.000021
Training epoch 59 [14720/67107 (21.95%)]	 Loss: 0.000005
Training epoch 59 [15360/67107 (22.90%)]	 Loss: 0.000028
Training epoch 59 [16000/67107 (23.85%)]	 Loss: 0.000015
Training epoch 59 [16640/67107 (24.81%)]	 Loss: 0.000010
Training epoch 59 [17280/67107 (25.76%)]	 Loss: 0.000004
Training epoch 59 [17920/67107 (26.72%)]	 Loss: 0.000082
Training epoch 59 [18560/67107 (27.67%)]	 Loss: 0.000016
Training epoch 59 [19200/67107 (28.63%)]	 Loss: 0.000017
Training epoch 59 [19840/67107 (29.58%)]	 Loss: 0.000001
Training epoch 59 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 59 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 59 [21760/67107 (32.44%)]	 Loss: 0.000017
Training epoch 59 [22400/67107 (33.40%)]	 Loss: 0.000008
Training epoch 59 [23040/67107 (34.35%)]	 Loss: 0.000043
Training epoch 59 [23680/67107 (35.31%)]	 Loss: 0.000009
Training epoch 59 [24320/67107 (36.26%)]	 Loss: 0.000006
Training epoch 59 [24960/67107 (37.21%)]	 Loss: 0.000058
Training epoch 59 [25600/67107 (38.17%)]	 Loss: 0.000011
Training epoch 59 [26240/67107 (39.12%)]	 Loss: 0.000018
Training epoch 59 [26880/67107 (40.08%)]	 Loss: 0.000007
Training epoch 59 [27520/67107 (41.03%)]	 Loss: 0.000011
Training epoch 59 [28160/67107 (41.98%)]	 Loss: 0.000017
Training epoch 59 [28800/67107 (42.94%)]	 Loss: 0.000072
Training epoch 59 [29440/67107 (43.89%)]	 Loss: 0.000033
Training epoch 59 [30080/67107 (44.85%)]	 Loss: 0.000029
Training epoch 59 [30720/67107 (45.80%)]	 Loss: 0.000009
Training epoch 59 [31360/67107 (46.76%)]	 Loss: 0.000019
Training epoch 59 [32000/67107 (47.71%)]	 Loss: 0.000004
Training epoch 59 [32640/67107 (48.66%)]	 Loss: 0.000006
Training epoch 59 [33280/67107 (49.62%)]	 Loss: 0.000005
Training epoch 59 [33920/67107 (50.57%)]	 Loss: 0.000032
Training epoch 59 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 59 [35200/67107 (52.48%)]	 Loss: 0.000007
Training epoch 59 [35840/67107 (53.44%)]	 Loss: 0.000020
Training epoch 59 [36480/67107 (54.39%)]	 Loss: 0.000008
Training epoch 59 [37120/67107 (55.34%)]	 Loss: 0.000014
Training epoch 59 [37760/67107 (56.30%)]	 Loss: 0.000007
Training epoch 59 [38400/67107 (57.25%)]	 Loss: 0.000003
Training epoch 59 [39040/67107 (58.21%)]	 Loss: 0.000004
Training epoch 59 [39680/67107 (59.16%)]	 Loss: 0.000015
Training epoch 59 [40320/67107 (60.11%)]	 Loss: 0.000004
Training epoch 59 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 59 [41600/67107 (62.02%)]	 Loss: 0.000004
Training epoch 59 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 59 [42880/67107 (63.93%)]	 Loss: 0.000025
Training epoch 59 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 59 [44160/67107 (65.84%)]	 Loss: 0.000026
Training epoch 59 [44800/67107 (66.79%)]	 Loss: 0.000007
Training epoch 59 [45440/67107 (67.75%)]	 Loss: 0.000008
Training epoch 59 [46080/67107 (68.70%)]	 Loss: 0.000002
Training epoch 59 [46720/67107 (69.66%)]	 Loss: 0.000020
Training epoch 59 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 59 [48000/67107 (71.56%)]	 Loss: 0.000011
Training epoch 59 [48640/67107 (72.52%)]	 Loss: 0.000004
Training epoch 59 [49280/67107 (73.47%)]	 Loss: 0.000004
Training epoch 59 [49920/67107 (74.43%)]	 Loss: 0.000003
Training epoch 59 [50560/67107 (75.38%)]	 Loss: 0.000049
Training epoch 59 [51200/67107 (76.34%)]	 Loss: 0.000011
Training epoch 59 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 59 [52480/67107 (78.24%)]	 Loss: 0.000050
Training epoch 59 [53120/67107 (79.20%)]	 Loss: 0.000005
Training epoch 59 [53760/67107 (80.15%)]	 Loss: 0.000017
Training epoch 59 [54400/67107 (81.11%)]	 Loss: 0.000004
Training epoch 59 [55040/67107 (82.06%)]	 Loss: 0.000039
Training epoch 59 [55680/67107 (83.02%)]	 Loss: 0.000036
Training epoch 59 [56320/67107 (83.97%)]	 Loss: 0.000015
Training epoch 59 [56960/67107 (84.92%)]	 Loss: 0.000067
Training epoch 59 [57600/67107 (85.88%)]	 Loss: 0.000028
Training epoch 59 [58240/67107 (86.83%)]	 Loss: 0.000012
Training epoch 59 [58880/67107 (87.79%)]	 Loss: 0.000036
Training epoch 59 [59520/67107 (88.74%)]	 Loss: 0.000037
Training epoch 59 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 59 [60800/67107 (90.65%)]	 Loss: 0.000016
Training epoch 59 [61440/67107 (91.60%)]	 Loss: 0.000007
Training epoch 59 [62080/67107 (92.56%)]	 Loss: 0.000006
Training epoch 59 [62720/67107 (93.51%)]	 Loss: 0.000014
Training epoch 59 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 59 [64000/67107 (95.42%)]	 Loss: 0.000011
Training epoch 59 [64640/67107 (96.37%)]	 Loss: 0.000005
Training epoch 59 [65280/67107 (97.33%)]	 Loss: 0.000075
Training epoch 59 [65920/67107 (98.28%)]	 Loss: 0.000008
Training epoch 59 [66560/67107 (99.24%)]	 Loss: 0.000018
Test set: Average Loss: 0.000186
Training epoch 60 [0/67107 (0.00%)]	 Loss: 0.000043
Training epoch 60 [640/67107 (0.95%)]	 Loss: 0.000023
Training epoch 60 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 60 [1920/67107 (2.86%)]	 Loss: 0.000040
Training epoch 60 [2560/67107 (3.82%)]	 Loss: 0.000009
Training epoch 60 [3200/67107 (4.77%)]	 Loss: 0.000110
Training epoch 60 [3840/67107 (5.73%)]	 Loss: 0.000053
Training epoch 60 [4480/67107 (6.68%)]	 Loss: 0.000040
Training epoch 60 [5120/67107 (7.63%)]	 Loss: 0.000033
Training epoch 60 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 60 [6400/67107 (9.54%)]	 Loss: 0.000006
Training epoch 60 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 60 [7680/67107 (11.45%)]	 Loss: 0.000002
Training epoch 60 [8320/67107 (12.40%)]	 Loss: 0.000007
Training epoch 60 [8960/67107 (13.36%)]	 Loss: 0.000030
Training epoch 60 [9600/67107 (14.31%)]	 Loss: 0.000012
Training epoch 60 [10240/67107 (15.27%)]	 Loss: 0.000010
Training epoch 60 [10880/67107 (16.22%)]	 Loss: 0.000028
Training epoch 60 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 60 [12160/67107 (18.13%)]	 Loss: 0.000027
Training epoch 60 [12800/67107 (19.08%)]	 Loss: 0.000019
Training epoch 60 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 60 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 60 [14720/67107 (21.95%)]	 Loss: 0.000005
Training epoch 60 [15360/67107 (22.90%)]	 Loss: 0.000003
Training epoch 60 [16000/67107 (23.85%)]	 Loss: 0.000002
Training epoch 60 [16640/67107 (24.81%)]	 Loss: 0.000019
Training epoch 60 [17280/67107 (25.76%)]	 Loss: 0.000020
Training epoch 60 [17920/67107 (26.72%)]	 Loss: 0.000007
Training epoch 60 [18560/67107 (27.67%)]	 Loss: 0.000020
Training epoch 60 [19200/67107 (28.63%)]	 Loss: 0.000022
Training epoch 60 [19840/67107 (29.58%)]	 Loss: 0.000013
Training epoch 60 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 60 [21120/67107 (31.49%)]	 Loss: 0.000026
Training epoch 60 [21760/67107 (32.44%)]	 Loss: 0.000013
Training epoch 60 [22400/67107 (33.40%)]	 Loss: 0.000003
Training epoch 60 [23040/67107 (34.35%)]	 Loss: 0.000002
Training epoch 60 [23680/67107 (35.31%)]	 Loss: 0.000007
Training epoch 60 [24320/67107 (36.26%)]	 Loss: 0.000005
Training epoch 60 [24960/67107 (37.21%)]	 Loss: 0.000013
Training epoch 60 [25600/67107 (38.17%)]	 Loss: 0.000011
Training epoch 60 [26240/67107 (39.12%)]	 Loss: 0.000002
Training epoch 60 [26880/67107 (40.08%)]	 Loss: 0.000004
Training epoch 60 [27520/67107 (41.03%)]	 Loss: 0.000010
Training epoch 60 [28160/67107 (41.98%)]	 Loss: 0.000005
Training epoch 60 [28800/67107 (42.94%)]	 Loss: 0.000001
Training epoch 60 [29440/67107 (43.89%)]	 Loss: 0.000008
Training epoch 60 [30080/67107 (44.85%)]	 Loss: 0.000002
Training epoch 60 [30720/67107 (45.80%)]	 Loss: 0.000005
Training epoch 60 [31360/67107 (46.76%)]	 Loss: 0.000023
Training epoch 60 [32000/67107 (47.71%)]	 Loss: 0.000026
Training epoch 60 [32640/67107 (48.66%)]	 Loss: 0.000034
Training epoch 60 [33280/67107 (49.62%)]	 Loss: 0.000065
Training epoch 60 [33920/67107 (50.57%)]	 Loss: 0.000009
Training epoch 60 [34560/67107 (51.53%)]	 Loss: 0.000015
Training epoch 60 [35200/67107 (52.48%)]	 Loss: 0.000012
Training epoch 60 [35840/67107 (53.44%)]	 Loss: 0.000002
Training epoch 60 [36480/67107 (54.39%)]	 Loss: 0.000006
Training epoch 60 [37120/67107 (55.34%)]	 Loss: 0.000010
Training epoch 60 [37760/67107 (56.30%)]	 Loss: 0.000003
Training epoch 60 [38400/67107 (57.25%)]	 Loss: 0.000020
Training epoch 60 [39040/67107 (58.21%)]	 Loss: 0.000006
Training epoch 60 [39680/67107 (59.16%)]	 Loss: 0.000033
Training epoch 60 [40320/67107 (60.11%)]	 Loss: 0.000009
Training epoch 60 [40960/67107 (61.07%)]	 Loss: 0.000012
Training epoch 60 [41600/67107 (62.02%)]	 Loss: 0.000008
Training epoch 60 [42240/67107 (62.98%)]	 Loss: 0.000024
Training epoch 60 [42880/67107 (63.93%)]	 Loss: 0.000005
Training epoch 60 [43520/67107 (64.89%)]	 Loss: 0.000015
Training epoch 60 [44160/67107 (65.84%)]	 Loss: 0.000009
Training epoch 60 [44800/67107 (66.79%)]	 Loss: 0.000001
Training epoch 60 [45440/67107 (67.75%)]	 Loss: 0.000004
Training epoch 60 [46080/67107 (68.70%)]	 Loss: 0.000003
Training epoch 60 [46720/67107 (69.66%)]	 Loss: 0.000015
Training epoch 60 [47360/67107 (70.61%)]	 Loss: 0.000003
Training epoch 60 [48000/67107 (71.56%)]	 Loss: 0.000022
Training epoch 60 [48640/67107 (72.52%)]	 Loss: 0.000012
Training epoch 60 [49280/67107 (73.47%)]	 Loss: 0.000026
Training epoch 60 [49920/67107 (74.43%)]	 Loss: 0.000006
Training epoch 60 [50560/67107 (75.38%)]	 Loss: 0.000002
Training epoch 60 [51200/67107 (76.34%)]	 Loss: 0.000002
Training epoch 60 [51840/67107 (77.29%)]	 Loss: 0.000001
Training epoch 60 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 60 [53120/67107 (79.20%)]	 Loss: 0.000009
Training epoch 60 [53760/67107 (80.15%)]	 Loss: 0.000022
Training epoch 60 [54400/67107 (81.11%)]	 Loss: 0.000012
Training epoch 60 [55040/67107 (82.06%)]	 Loss: 0.000006
Training epoch 60 [55680/67107 (83.02%)]	 Loss: 0.000005
Training epoch 60 [56320/67107 (83.97%)]	 Loss: 0.000017
Training epoch 60 [56960/67107 (84.92%)]	 Loss: 0.000028
Training epoch 60 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 60 [58240/67107 (86.83%)]	 Loss: 0.000001
Training epoch 60 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 60 [59520/67107 (88.74%)]	 Loss: 0.000004
Training epoch 60 [60160/67107 (89.69%)]	 Loss: 0.000010
Training epoch 60 [60800/67107 (90.65%)]	 Loss: 0.000008
Training epoch 60 [61440/67107 (91.60%)]	 Loss: 0.000019
Training epoch 60 [62080/67107 (92.56%)]	 Loss: 0.000028
Training epoch 60 [62720/67107 (93.51%)]	 Loss: 0.000009
Training epoch 60 [63360/67107 (94.47%)]	 Loss: 0.000010
Training epoch 60 [64000/67107 (95.42%)]	 Loss: 0.000051
Training epoch 60 [64640/67107 (96.37%)]	 Loss: 0.000013
Training epoch 60 [65280/67107 (97.33%)]	 Loss: 0.000005
Training epoch 60 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 60 [66560/67107 (99.24%)]	 Loss: 0.000014
Test set: Average Loss: 0.000141
Training epoch 61 [0/67107 (0.00%)]	 Loss: 0.000027
Training epoch 61 [640/67107 (0.95%)]	 Loss: 0.000011
Training epoch 61 [1280/67107 (1.91%)]	 Loss: 0.000024
Training epoch 61 [1920/67107 (2.86%)]	 Loss: 0.000017
Training epoch 61 [2560/67107 (3.82%)]	 Loss: 0.000012
Training epoch 61 [3200/67107 (4.77%)]	 Loss: 0.000005
Training epoch 61 [3840/67107 (5.73%)]	 Loss: 0.000014
Training epoch 61 [4480/67107 (6.68%)]	 Loss: 0.000002
Training epoch 61 [5120/67107 (7.63%)]	 Loss: 0.000030
Training epoch 61 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 61 [6400/67107 (9.54%)]	 Loss: 0.000029
Training epoch 61 [7040/67107 (10.50%)]	 Loss: 0.000050
Training epoch 61 [7680/67107 (11.45%)]	 Loss: 0.000073
Training epoch 61 [8320/67107 (12.40%)]	 Loss: 0.000015
Training epoch 61 [8960/67107 (13.36%)]	 Loss: 0.000009
Training epoch 61 [9600/67107 (14.31%)]	 Loss: 0.000003
Training epoch 61 [10240/67107 (15.27%)]	 Loss: 0.000006
Training epoch 61 [10880/67107 (16.22%)]	 Loss: 0.000014
Training epoch 61 [11520/67107 (17.18%)]	 Loss: 0.000009
Training epoch 61 [12160/67107 (18.13%)]	 Loss: 0.000014
Training epoch 61 [12800/67107 (19.08%)]	 Loss: 0.000021
Training epoch 61 [13440/67107 (20.04%)]	 Loss: 0.000009
Training epoch 61 [14080/67107 (20.99%)]	 Loss: 0.000026
Training epoch 61 [14720/67107 (21.95%)]	 Loss: 0.000007
Training epoch 61 [15360/67107 (22.90%)]	 Loss: 0.000043
Training epoch 61 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 61 [16640/67107 (24.81%)]	 Loss: 0.000010
Training epoch 61 [17280/67107 (25.76%)]	 Loss: 0.000008
Training epoch 61 [17920/67107 (26.72%)]	 Loss: 0.000009
Training epoch 61 [18560/67107 (27.67%)]	 Loss: 0.000061
Training epoch 61 [19200/67107 (28.63%)]	 Loss: 0.000012
Training epoch 61 [19840/67107 (29.58%)]	 Loss: 0.000008
Training epoch 61 [20480/67107 (30.53%)]	 Loss: 0.000013
Training epoch 61 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 61 [21760/67107 (32.44%)]	 Loss: 0.000027
Training epoch 61 [22400/67107 (33.40%)]	 Loss: 0.000011
Training epoch 61 [23040/67107 (34.35%)]	 Loss: 0.000006
Training epoch 61 [23680/67107 (35.31%)]	 Loss: 0.000007
Training epoch 61 [24320/67107 (36.26%)]	 Loss: 0.000007
Training epoch 61 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 61 [25600/67107 (38.17%)]	 Loss: 0.000005
Training epoch 61 [26240/67107 (39.12%)]	 Loss: 0.000007
Training epoch 61 [26880/67107 (40.08%)]	 Loss: 0.000023
Training epoch 61 [27520/67107 (41.03%)]	 Loss: 0.000030
Training epoch 61 [28160/67107 (41.98%)]	 Loss: 0.000004
Training epoch 61 [28800/67107 (42.94%)]	 Loss: 0.000019
Training epoch 61 [29440/67107 (43.89%)]	 Loss: 0.000023
Training epoch 61 [30080/67107 (44.85%)]	 Loss: 0.000019
Training epoch 61 [30720/67107 (45.80%)]	 Loss: 0.000018
Training epoch 61 [31360/67107 (46.76%)]	 Loss: 0.000034
Training epoch 61 [32000/67107 (47.71%)]	 Loss: 0.000004
Training epoch 61 [32640/67107 (48.66%)]	 Loss: 0.000003
Training epoch 61 [33280/67107 (49.62%)]	 Loss: 0.000004
Training epoch 61 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 61 [34560/67107 (51.53%)]	 Loss: 0.000003
Training epoch 61 [35200/67107 (52.48%)]	 Loss: 0.000025
Training epoch 61 [35840/67107 (53.44%)]	 Loss: 0.000011
Training epoch 61 [36480/67107 (54.39%)]	 Loss: 0.000022
Training epoch 61 [37120/67107 (55.34%)]	 Loss: 0.000001
Training epoch 61 [37760/67107 (56.30%)]	 Loss: 0.000011
Training epoch 61 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 61 [39040/67107 (58.21%)]	 Loss: 0.000015
Training epoch 61 [39680/67107 (59.16%)]	 Loss: 0.000029
Training epoch 61 [40320/67107 (60.11%)]	 Loss: 0.000048
Training epoch 61 [40960/67107 (61.07%)]	 Loss: 0.000172
Training epoch 61 [41600/67107 (62.02%)]	 Loss: 0.000006
Training epoch 61 [42240/67107 (62.98%)]	 Loss: 0.000008
Training epoch 61 [42880/67107 (63.93%)]	 Loss: 0.000035
Training epoch 61 [43520/67107 (64.89%)]	 Loss: 0.000060
Training epoch 61 [44160/67107 (65.84%)]	 Loss: 0.000038
Training epoch 61 [44800/67107 (66.79%)]	 Loss: 0.000023
Training epoch 61 [45440/67107 (67.75%)]	 Loss: 0.000002
Training epoch 61 [46080/67107 (68.70%)]	 Loss: 0.000013
Training epoch 61 [46720/67107 (69.66%)]	 Loss: 0.000009
Training epoch 61 [47360/67107 (70.61%)]	 Loss: 0.000035
Training epoch 61 [48000/67107 (71.56%)]	 Loss: 0.000028
Training epoch 61 [48640/67107 (72.52%)]	 Loss: 0.000026
Training epoch 61 [49280/67107 (73.47%)]	 Loss: 0.000042
Training epoch 61 [49920/67107 (74.43%)]	 Loss: 0.000022
Training epoch 61 [50560/67107 (75.38%)]	 Loss: 0.000013
Training epoch 61 [51200/67107 (76.34%)]	 Loss: 0.000015
Training epoch 61 [51840/67107 (77.29%)]	 Loss: 0.000040
Training epoch 61 [52480/67107 (78.24%)]	 Loss: 0.000012
Training epoch 61 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 61 [53760/67107 (80.15%)]	 Loss: 0.000011
Training epoch 61 [54400/67107 (81.11%)]	 Loss: 0.000007
Training epoch 61 [55040/67107 (82.06%)]	 Loss: 0.000006
Training epoch 61 [55680/67107 (83.02%)]	 Loss: 0.000007
Training epoch 61 [56320/67107 (83.97%)]	 Loss: 0.000009
Training epoch 61 [56960/67107 (84.92%)]	 Loss: 0.000004
Training epoch 61 [57600/67107 (85.88%)]	 Loss: 0.000066
Training epoch 61 [58240/67107 (86.83%)]	 Loss: 0.000009
Training epoch 61 [58880/67107 (87.79%)]	 Loss: 0.000003
Training epoch 61 [59520/67107 (88.74%)]	 Loss: 0.000009
Training epoch 61 [60160/67107 (89.69%)]	 Loss: 0.000006
Training epoch 61 [60800/67107 (90.65%)]	 Loss: 0.000008
Training epoch 61 [61440/67107 (91.60%)]	 Loss: 0.000003
Training epoch 61 [62080/67107 (92.56%)]	 Loss: 0.000020
Training epoch 61 [62720/67107 (93.51%)]	 Loss: 0.000009
Training epoch 61 [63360/67107 (94.47%)]	 Loss: 0.000005
Training epoch 61 [64000/67107 (95.42%)]	 Loss: 0.000003
Training epoch 61 [64640/67107 (96.37%)]	 Loss: 0.000002
Training epoch 61 [65280/67107 (97.33%)]	 Loss: 0.000003
Training epoch 61 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 61 [66560/67107 (99.24%)]	 Loss: 0.000012
Test set: Average Loss: 0.000902
Training epoch 62 [0/67107 (0.00%)]	 Loss: 0.000005
Training epoch 62 [640/67107 (0.95%)]	 Loss: 0.000025
Training epoch 62 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 62 [1920/67107 (2.86%)]	 Loss: 0.000005
Training epoch 62 [2560/67107 (3.82%)]	 Loss: 0.000003
Training epoch 62 [3200/67107 (4.77%)]	 Loss: 0.000019
Training epoch 62 [3840/67107 (5.73%)]	 Loss: 0.000011
Training epoch 62 [4480/67107 (6.68%)]	 Loss: 0.000004
Training epoch 62 [5120/67107 (7.63%)]	 Loss: 0.000021
Training epoch 62 [5760/67107 (8.59%)]	 Loss: 0.000009
Training epoch 62 [6400/67107 (9.54%)]	 Loss: 0.000003
Training epoch 62 [7040/67107 (10.50%)]	 Loss: 0.000028
Training epoch 62 [7680/67107 (11.45%)]	 Loss: 0.000009
Training epoch 62 [8320/67107 (12.40%)]	 Loss: 0.000013
Training epoch 62 [8960/67107 (13.36%)]	 Loss: 0.000004
Training epoch 62 [9600/67107 (14.31%)]	 Loss: 0.000010
Training epoch 62 [10240/67107 (15.27%)]	 Loss: 0.000011
Training epoch 62 [10880/67107 (16.22%)]	 Loss: 0.000006
Training epoch 62 [11520/67107 (17.18%)]	 Loss: 0.000012
Training epoch 62 [12160/67107 (18.13%)]	 Loss: 0.000006
Training epoch 62 [12800/67107 (19.08%)]	 Loss: 0.000005
Training epoch 62 [13440/67107 (20.04%)]	 Loss: 0.000039
Training epoch 62 [14080/67107 (20.99%)]	 Loss: 0.000019
Training epoch 62 [14720/67107 (21.95%)]	 Loss: 0.000023
Training epoch 62 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 62 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 62 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 62 [17280/67107 (25.76%)]	 Loss: 0.000032
Training epoch 62 [17920/67107 (26.72%)]	 Loss: 0.000007
Training epoch 62 [18560/67107 (27.67%)]	 Loss: 0.000029
Training epoch 62 [19200/67107 (28.63%)]	 Loss: 0.000026
Training epoch 62 [19840/67107 (29.58%)]	 Loss: 0.000033
Training epoch 62 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 62 [21120/67107 (31.49%)]	 Loss: 0.000024
Training epoch 62 [21760/67107 (32.44%)]	 Loss: 0.000003
Training epoch 62 [22400/67107 (33.40%)]	 Loss: 0.000022
Training epoch 62 [23040/67107 (34.35%)]	 Loss: 0.000060
Training epoch 62 [23680/67107 (35.31%)]	 Loss: 0.000036
Training epoch 62 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 62 [24960/67107 (37.21%)]	 Loss: 0.000004
Training epoch 62 [25600/67107 (38.17%)]	 Loss: 0.000005
Training epoch 62 [26240/67107 (39.12%)]	 Loss: 0.000009
Training epoch 62 [26880/67107 (40.08%)]	 Loss: 0.000020
Training epoch 62 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 62 [28160/67107 (41.98%)]	 Loss: 0.000030
Training epoch 62 [28800/67107 (42.94%)]	 Loss: 0.000007
Training epoch 62 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 62 [30080/67107 (44.85%)]	 Loss: 0.000027
Training epoch 62 [30720/67107 (45.80%)]	 Loss: 0.000009
Training epoch 62 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 62 [32000/67107 (47.71%)]	 Loss: 0.000001
Training epoch 62 [32640/67107 (48.66%)]	 Loss: 0.000008
Training epoch 62 [33280/67107 (49.62%)]	 Loss: 0.000008
Training epoch 62 [33920/67107 (50.57%)]	 Loss: 0.000008
Training epoch 62 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 62 [35200/67107 (52.48%)]	 Loss: 0.000002
Training epoch 62 [35840/67107 (53.44%)]	 Loss: 0.000025
Training epoch 62 [36480/67107 (54.39%)]	 Loss: 0.000020
Training epoch 62 [37120/67107 (55.34%)]	 Loss: 0.000009
Training epoch 62 [37760/67107 (56.30%)]	 Loss: 0.000013
Training epoch 62 [38400/67107 (57.25%)]	 Loss: 0.000003
Training epoch 62 [39040/67107 (58.21%)]	 Loss: 0.000001
Training epoch 62 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 62 [40320/67107 (60.11%)]	 Loss: 0.000002
Training epoch 62 [40960/67107 (61.07%)]	 Loss: 0.000009
Training epoch 62 [41600/67107 (62.02%)]	 Loss: 0.000002
Training epoch 62 [42240/67107 (62.98%)]	 Loss: 0.000025
Training epoch 62 [42880/67107 (63.93%)]	 Loss: 0.000007
Training epoch 62 [43520/67107 (64.89%)]	 Loss: 0.000009
Training epoch 62 [44160/67107 (65.84%)]	 Loss: 0.000002
Training epoch 62 [44800/67107 (66.79%)]	 Loss: 0.000008
Training epoch 62 [45440/67107 (67.75%)]	 Loss: 0.000007
Training epoch 62 [46080/67107 (68.70%)]	 Loss: 0.000006
Training epoch 62 [46720/67107 (69.66%)]	 Loss: 0.000007
Training epoch 62 [47360/67107 (70.61%)]	 Loss: 0.000003
Training epoch 62 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 62 [48640/67107 (72.52%)]	 Loss: 0.000002
Training epoch 62 [49280/67107 (73.47%)]	 Loss: 0.000004
Training epoch 62 [49920/67107 (74.43%)]	 Loss: 0.000009
Training epoch 62 [50560/67107 (75.38%)]	 Loss: 0.000004
Training epoch 62 [51200/67107 (76.34%)]	 Loss: 0.000028
Training epoch 62 [51840/67107 (77.29%)]	 Loss: 0.000025
Training epoch 62 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 62 [53120/67107 (79.20%)]	 Loss: 0.000109
Training epoch 62 [53760/67107 (80.15%)]	 Loss: 0.000005
Training epoch 62 [54400/67107 (81.11%)]	 Loss: 0.000005
Training epoch 62 [55040/67107 (82.06%)]	 Loss: 0.000010
Training epoch 62 [55680/67107 (83.02%)]	 Loss: 0.000007
Training epoch 62 [56320/67107 (83.97%)]	 Loss: 0.000005
Training epoch 62 [56960/67107 (84.92%)]	 Loss: 0.000003
Training epoch 62 [57600/67107 (85.88%)]	 Loss: 0.000037
Training epoch 62 [58240/67107 (86.83%)]	 Loss: 0.000008
Training epoch 62 [58880/67107 (87.79%)]	 Loss: 0.000025
Training epoch 62 [59520/67107 (88.74%)]	 Loss: 0.000016
Training epoch 62 [60160/67107 (89.69%)]	 Loss: 0.000020
Training epoch 62 [60800/67107 (90.65%)]	 Loss: 0.000006
Training epoch 62 [61440/67107 (91.60%)]	 Loss: 0.000005
Training epoch 62 [62080/67107 (92.56%)]	 Loss: 0.000007
Training epoch 62 [62720/67107 (93.51%)]	 Loss: 0.000011
Training epoch 62 [63360/67107 (94.47%)]	 Loss: 0.000034
Training epoch 62 [64000/67107 (95.42%)]	 Loss: 0.000012
Training epoch 62 [64640/67107 (96.37%)]	 Loss: 0.000009
Training epoch 62 [65280/67107 (97.33%)]	 Loss: 0.000013
Training epoch 62 [65920/67107 (98.28%)]	 Loss: 0.000006
Training epoch 62 [66560/67107 (99.24%)]	 Loss: 0.000010
Test set: Average Loss: 0.000226
Training epoch 63 [0/67107 (0.00%)]	 Loss: 0.000002
Training epoch 63 [640/67107 (0.95%)]	 Loss: 0.000002
Training epoch 63 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 63 [1920/67107 (2.86%)]	 Loss: 0.000002
Training epoch 63 [2560/67107 (3.82%)]	 Loss: 0.000002
Training epoch 63 [3200/67107 (4.77%)]	 Loss: 0.000002
Training epoch 63 [3840/67107 (5.73%)]	 Loss: 0.000007
Training epoch 63 [4480/67107 (6.68%)]	 Loss: 0.000006
Training epoch 63 [5120/67107 (7.63%)]	 Loss: 0.000016
Training epoch 63 [5760/67107 (8.59%)]	 Loss: 0.000014
Training epoch 63 [6400/67107 (9.54%)]	 Loss: 0.000008
Training epoch 63 [7040/67107 (10.50%)]	 Loss: 0.000011
Training epoch 63 [7680/67107 (11.45%)]	 Loss: 0.000014
Training epoch 63 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 63 [8960/67107 (13.36%)]	 Loss: 0.000005
Training epoch 63 [9600/67107 (14.31%)]	 Loss: 0.000010
Training epoch 63 [10240/67107 (15.27%)]	 Loss: 0.000015
Training epoch 63 [10880/67107 (16.22%)]	 Loss: 0.000019
Training epoch 63 [11520/67107 (17.18%)]	 Loss: 0.000068
Training epoch 63 [12160/67107 (18.13%)]	 Loss: 0.000015
Training epoch 63 [12800/67107 (19.08%)]	 Loss: 0.000014
Training epoch 63 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 63 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 63 [14720/67107 (21.95%)]	 Loss: 0.000005
Training epoch 63 [15360/67107 (22.90%)]	 Loss: 0.000089
Training epoch 63 [16000/67107 (23.85%)]	 Loss: 0.000053
Training epoch 63 [16640/67107 (24.81%)]	 Loss: 0.000043
Training epoch 63 [17280/67107 (25.76%)]	 Loss: 0.000013
Training epoch 63 [17920/67107 (26.72%)]	 Loss: 0.000010
Training epoch 63 [18560/67107 (27.67%)]	 Loss: 0.000015
Training epoch 63 [19200/67107 (28.63%)]	 Loss: 0.000017
Training epoch 63 [19840/67107 (29.58%)]	 Loss: 0.000045
Training epoch 63 [20480/67107 (30.53%)]	 Loss: 0.000021
Training epoch 63 [21120/67107 (31.49%)]	 Loss: 0.000019
Training epoch 63 [21760/67107 (32.44%)]	 Loss: 0.000008
Training epoch 63 [22400/67107 (33.40%)]	 Loss: 0.000024
Training epoch 63 [23040/67107 (34.35%)]	 Loss: 0.000002
Training epoch 63 [23680/67107 (35.31%)]	 Loss: 0.000003
Training epoch 63 [24320/67107 (36.26%)]	 Loss: 0.000003
Training epoch 63 [24960/67107 (37.21%)]	 Loss: 0.000031
Training epoch 63 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 63 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 63 [26880/67107 (40.08%)]	 Loss: 0.000013
Training epoch 63 [27520/67107 (41.03%)]	 Loss: 0.000003
Training epoch 63 [28160/67107 (41.98%)]	 Loss: 0.000006
Training epoch 63 [28800/67107 (42.94%)]	 Loss: 0.000003
Training epoch 63 [29440/67107 (43.89%)]	 Loss: 0.000019
Training epoch 63 [30080/67107 (44.85%)]	 Loss: 0.000007
Training epoch 63 [30720/67107 (45.80%)]	 Loss: 0.000001
Training epoch 63 [31360/67107 (46.76%)]	 Loss: 0.000001
Training epoch 63 [32000/67107 (47.71%)]	 Loss: 0.000004
Training epoch 63 [32640/67107 (48.66%)]	 Loss: 0.000003
Training epoch 63 [33280/67107 (49.62%)]	 Loss: 0.000002
Training epoch 63 [33920/67107 (50.57%)]	 Loss: 0.000003
Training epoch 63 [34560/67107 (51.53%)]	 Loss: 0.000012
Training epoch 63 [35200/67107 (52.48%)]	 Loss: 0.000020
Training epoch 63 [35840/67107 (53.44%)]	 Loss: 0.000009
Training epoch 63 [36480/67107 (54.39%)]	 Loss: 0.000006
Training epoch 63 [37120/67107 (55.34%)]	 Loss: 0.000021
Training epoch 63 [37760/67107 (56.30%)]	 Loss: 0.000044
Training epoch 63 [38400/67107 (57.25%)]	 Loss: 0.000011
Training epoch 63 [39040/67107 (58.21%)]	 Loss: 0.000001
Training epoch 63 [39680/67107 (59.16%)]	 Loss: 0.000010
Training epoch 63 [40320/67107 (60.11%)]	 Loss: 0.000021
Training epoch 63 [40960/67107 (61.07%)]	 Loss: 0.000020
Training epoch 63 [41600/67107 (62.02%)]	 Loss: 0.000013
Training epoch 63 [42240/67107 (62.98%)]	 Loss: 0.000030
Training epoch 63 [42880/67107 (63.93%)]	 Loss: 0.000010
Training epoch 63 [43520/67107 (64.89%)]	 Loss: 0.000011
Training epoch 63 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 63 [44800/67107 (66.79%)]	 Loss: 0.000036
Training epoch 63 [45440/67107 (67.75%)]	 Loss: 0.000026
Training epoch 63 [46080/67107 (68.70%)]	 Loss: 0.000002
Training epoch 63 [46720/67107 (69.66%)]	 Loss: 0.000027
Training epoch 63 [47360/67107 (70.61%)]	 Loss: 0.000015
Training epoch 63 [48000/67107 (71.56%)]	 Loss: 0.000019
Training epoch 63 [48640/67107 (72.52%)]	 Loss: 0.000004
Training epoch 63 [49280/67107 (73.47%)]	 Loss: 0.000010
Training epoch 63 [49920/67107 (74.43%)]	 Loss: 0.000017
Training epoch 63 [50560/67107 (75.38%)]	 Loss: 0.000040
Training epoch 63 [51200/67107 (76.34%)]	 Loss: 0.000039
Training epoch 63 [51840/67107 (77.29%)]	 Loss: 0.000012
Training epoch 63 [52480/67107 (78.24%)]	 Loss: 0.000004
Training epoch 63 [53120/67107 (79.20%)]	 Loss: 0.000020
Training epoch 63 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 63 [54400/67107 (81.11%)]	 Loss: 0.000002
Training epoch 63 [55040/67107 (82.06%)]	 Loss: 0.000005
Training epoch 63 [55680/67107 (83.02%)]	 Loss: 0.000030
Training epoch 63 [56320/67107 (83.97%)]	 Loss: 0.000003
Training epoch 63 [56960/67107 (84.92%)]	 Loss: 0.000008
Training epoch 63 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 63 [58240/67107 (86.83%)]	 Loss: 0.000021
Training epoch 63 [58880/67107 (87.79%)]	 Loss: 0.000004
Training epoch 63 [59520/67107 (88.74%)]	 Loss: 0.000002
Training epoch 63 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 63 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 63 [61440/67107 (91.60%)]	 Loss: 0.000004
Training epoch 63 [62080/67107 (92.56%)]	 Loss: 0.000005
Training epoch 63 [62720/67107 (93.51%)]	 Loss: 0.000006
Training epoch 63 [63360/67107 (94.47%)]	 Loss: 0.000008
Training epoch 63 [64000/67107 (95.42%)]	 Loss: 0.000012
Training epoch 63 [64640/67107 (96.37%)]	 Loss: 0.000013
Training epoch 63 [65280/67107 (97.33%)]	 Loss: 0.000008
Training epoch 63 [65920/67107 (98.28%)]	 Loss: 0.000017
Training epoch 63 [66560/67107 (99.24%)]	 Loss: 0.000007
Test set: Average Loss: 0.028711
Training epoch 64 [0/67107 (0.00%)]	 Loss: 0.000052
Training epoch 64 [640/67107 (0.95%)]	 Loss: 0.000008
Training epoch 64 [1280/67107 (1.91%)]	 Loss: 0.000034
Training epoch 64 [1920/67107 (2.86%)]	 Loss: 0.000002
Training epoch 64 [2560/67107 (3.82%)]	 Loss: 0.000003
Training epoch 64 [3200/67107 (4.77%)]	 Loss: 0.000004
Training epoch 64 [3840/67107 (5.73%)]	 Loss: 0.000023
Training epoch 64 [4480/67107 (6.68%)]	 Loss: 0.000009
Training epoch 64 [5120/67107 (7.63%)]	 Loss: 0.000010
Training epoch 64 [5760/67107 (8.59%)]	 Loss: 0.000004
Training epoch 64 [6400/67107 (9.54%)]	 Loss: 0.000010
Training epoch 64 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 64 [7680/67107 (11.45%)]	 Loss: 0.000016
Training epoch 64 [8320/67107 (12.40%)]	 Loss: 0.000016
Training epoch 64 [8960/67107 (13.36%)]	 Loss: 0.000005
Training epoch 64 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 64 [10240/67107 (15.27%)]	 Loss: 0.000006
Training epoch 64 [10880/67107 (16.22%)]	 Loss: 0.000006
Training epoch 64 [11520/67107 (17.18%)]	 Loss: 0.000029
Training epoch 64 [12160/67107 (18.13%)]	 Loss: 0.000007
Training epoch 64 [12800/67107 (19.08%)]	 Loss: 0.000015
Training epoch 64 [13440/67107 (20.04%)]	 Loss: 0.000011
Training epoch 64 [14080/67107 (20.99%)]	 Loss: 0.000008
Training epoch 64 [14720/67107 (21.95%)]	 Loss: 0.000011
Training epoch 64 [15360/67107 (22.90%)]	 Loss: 0.000052
Training epoch 64 [16000/67107 (23.85%)]	 Loss: 0.000004
Training epoch 64 [16640/67107 (24.81%)]	 Loss: 0.000008
Training epoch 64 [17280/67107 (25.76%)]	 Loss: 0.000008
Training epoch 64 [17920/67107 (26.72%)]	 Loss: 0.000012
Training epoch 64 [18560/67107 (27.67%)]	 Loss: 0.000002
Training epoch 64 [19200/67107 (28.63%)]	 Loss: 0.000015
Training epoch 64 [19840/67107 (29.58%)]	 Loss: 0.000016
Training epoch 64 [20480/67107 (30.53%)]	 Loss: 0.000013
Training epoch 64 [21120/67107 (31.49%)]	 Loss: 0.000021
Training epoch 64 [21760/67107 (32.44%)]	 Loss: 0.000014
Training epoch 64 [22400/67107 (33.40%)]	 Loss: 0.000018
Training epoch 64 [23040/67107 (34.35%)]	 Loss: 0.000009
Training epoch 64 [23680/67107 (35.31%)]	 Loss: 0.000024
Training epoch 64 [24320/67107 (36.26%)]	 Loss: 0.000009
Training epoch 64 [24960/67107 (37.21%)]	 Loss: 0.000002
Training epoch 64 [25600/67107 (38.17%)]	 Loss: 0.000002
Training epoch 64 [26240/67107 (39.12%)]	 Loss: 0.000024
Training epoch 64 [26880/67107 (40.08%)]	 Loss: 0.000022
Training epoch 64 [27520/67107 (41.03%)]	 Loss: 0.000018
Training epoch 64 [28160/67107 (41.98%)]	 Loss: 0.000032
Training epoch 64 [28800/67107 (42.94%)]	 Loss: 0.000009
Training epoch 64 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 64 [30080/67107 (44.85%)]	 Loss: 0.000025
Training epoch 64 [30720/67107 (45.80%)]	 Loss: 0.000018
Training epoch 64 [31360/67107 (46.76%)]	 Loss: 0.000036
Training epoch 64 [32000/67107 (47.71%)]	 Loss: 0.000055
Training epoch 64 [32640/67107 (48.66%)]	 Loss: 0.000029
Training epoch 64 [33280/67107 (49.62%)]	 Loss: 0.000034
Training epoch 64 [33920/67107 (50.57%)]	 Loss: 0.000020
Training epoch 64 [34560/67107 (51.53%)]	 Loss: 0.000029
Training epoch 64 [35200/67107 (52.48%)]	 Loss: 0.000011
Training epoch 64 [35840/67107 (53.44%)]	 Loss: 0.000007
Training epoch 64 [36480/67107 (54.39%)]	 Loss: 0.000013
Training epoch 64 [37120/67107 (55.34%)]	 Loss: 0.000054
Training epoch 64 [37760/67107 (56.30%)]	 Loss: 0.000007
Training epoch 64 [38400/67107 (57.25%)]	 Loss: 0.000004
Training epoch 64 [39040/67107 (58.21%)]	 Loss: 0.000078
Training epoch 64 [39680/67107 (59.16%)]	 Loss: 0.000007
Training epoch 64 [40320/67107 (60.11%)]	 Loss: 0.000009
Training epoch 64 [40960/67107 (61.07%)]	 Loss: 0.000012
Training epoch 64 [41600/67107 (62.02%)]	 Loss: 0.000036
Training epoch 64 [42240/67107 (62.98%)]	 Loss: 0.000031
Training epoch 64 [42880/67107 (63.93%)]	 Loss: 0.000037
Training epoch 64 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 64 [44160/67107 (65.84%)]	 Loss: 0.000016
Training epoch 64 [44800/67107 (66.79%)]	 Loss: 0.000007
Training epoch 64 [45440/67107 (67.75%)]	 Loss: 0.000009
Training epoch 64 [46080/67107 (68.70%)]	 Loss: 0.000003
Training epoch 64 [46720/67107 (69.66%)]	 Loss: 0.000005
Training epoch 64 [47360/67107 (70.61%)]	 Loss: 0.000035
Training epoch 64 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 64 [48640/67107 (72.52%)]	 Loss: 0.000021
Training epoch 64 [49280/67107 (73.47%)]	 Loss: 0.000032
Training epoch 64 [49920/67107 (74.43%)]	 Loss: 0.000013
Training epoch 64 [50560/67107 (75.38%)]	 Loss: 0.000001
Training epoch 64 [51200/67107 (76.34%)]	 Loss: 0.000027
Training epoch 64 [51840/67107 (77.29%)]	 Loss: 0.000018
Training epoch 64 [52480/67107 (78.24%)]	 Loss: 0.000101
Training epoch 64 [53120/67107 (79.20%)]	 Loss: 0.000014
Training epoch 64 [53760/67107 (80.15%)]	 Loss: 0.000075
Training epoch 64 [54400/67107 (81.11%)]	 Loss: 0.000030
Training epoch 64 [55040/67107 (82.06%)]	 Loss: 0.000014
Training epoch 64 [55680/67107 (83.02%)]	 Loss: 0.000011
Training epoch 64 [56320/67107 (83.97%)]	 Loss: 0.000009
Training epoch 64 [56960/67107 (84.92%)]	 Loss: 0.000021
Training epoch 64 [57600/67107 (85.88%)]	 Loss: 0.000028
Training epoch 64 [58240/67107 (86.83%)]	 Loss: 0.000007
Training epoch 64 [58880/67107 (87.79%)]	 Loss: 0.000003
Training epoch 64 [59520/67107 (88.74%)]	 Loss: 0.000005
Training epoch 64 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 64 [60800/67107 (90.65%)]	 Loss: 0.000004
Training epoch 64 [61440/67107 (91.60%)]	 Loss: 0.000028
Training epoch 64 [62080/67107 (92.56%)]	 Loss: 0.000005
Training epoch 64 [62720/67107 (93.51%)]	 Loss: 0.000008
Training epoch 64 [63360/67107 (94.47%)]	 Loss: 0.000005
Training epoch 64 [64000/67107 (95.42%)]	 Loss: 0.000021
Training epoch 64 [64640/67107 (96.37%)]	 Loss: 0.000004
Training epoch 64 [65280/67107 (97.33%)]	 Loss: 0.000002
Training epoch 64 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 64 [66560/67107 (99.24%)]	 Loss: 0.000002
Test set: Average Loss: 0.000150
Training epoch 65 [0/67107 (0.00%)]	 Loss: 0.000005
Training epoch 65 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 65 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 65 [1920/67107 (2.86%)]	 Loss: 0.000007
Training epoch 65 [2560/67107 (3.82%)]	 Loss: 0.000058
Training epoch 65 [3200/67107 (4.77%)]	 Loss: 0.000002
Training epoch 65 [3840/67107 (5.73%)]	 Loss: 0.000022
Training epoch 65 [4480/67107 (6.68%)]	 Loss: 0.000011
Training epoch 65 [5120/67107 (7.63%)]	 Loss: 0.000004
Training epoch 65 [5760/67107 (8.59%)]	 Loss: 0.000002
Training epoch 65 [6400/67107 (9.54%)]	 Loss: 0.000024
Training epoch 65 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 65 [7680/67107 (11.45%)]	 Loss: 0.000015
Training epoch 65 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 65 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 65 [9600/67107 (14.31%)]	 Loss: 0.000004
Training epoch 65 [10240/67107 (15.27%)]	 Loss: 0.000016
Training epoch 65 [10880/67107 (16.22%)]	 Loss: 0.000020
Training epoch 65 [11520/67107 (17.18%)]	 Loss: 0.000056
Training epoch 65 [12160/67107 (18.13%)]	 Loss: 0.000016
Training epoch 65 [12800/67107 (19.08%)]	 Loss: 0.000009
Training epoch 65 [13440/67107 (20.04%)]	 Loss: 0.000024
Training epoch 65 [14080/67107 (20.99%)]	 Loss: 0.000008
Training epoch 65 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 65 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 65 [16000/67107 (23.85%)]	 Loss: 0.000002
Training epoch 65 [16640/67107 (24.81%)]	 Loss: 0.000003
Training epoch 65 [17280/67107 (25.76%)]	 Loss: 0.000044
Training epoch 65 [17920/67107 (26.72%)]	 Loss: 0.000025
Training epoch 65 [18560/67107 (27.67%)]	 Loss: 0.000056
Training epoch 65 [19200/67107 (28.63%)]	 Loss: 0.000014
Training epoch 65 [19840/67107 (29.58%)]	 Loss: 0.000002
Training epoch 65 [20480/67107 (30.53%)]	 Loss: 0.000016
Training epoch 65 [21120/67107 (31.49%)]	 Loss: 0.000004
Training epoch 65 [21760/67107 (32.44%)]	 Loss: 0.000006
Training epoch 65 [22400/67107 (33.40%)]	 Loss: 0.000026
Training epoch 65 [23040/67107 (34.35%)]	 Loss: 0.000012
Training epoch 65 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 65 [24320/67107 (36.26%)]	 Loss: 0.000006
Training epoch 65 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 65 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 65 [26240/67107 (39.12%)]	 Loss: 0.000007
Training epoch 65 [26880/67107 (40.08%)]	 Loss: 0.000010
Training epoch 65 [27520/67107 (41.03%)]	 Loss: 0.000001
Training epoch 65 [28160/67107 (41.98%)]	 Loss: 0.000004
Training epoch 65 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 65 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 65 [30080/67107 (44.85%)]	 Loss: 0.000002
Training epoch 65 [30720/67107 (45.80%)]	 Loss: 0.000007
Training epoch 65 [31360/67107 (46.76%)]	 Loss: 0.000019
Training epoch 65 [32000/67107 (47.71%)]	 Loss: 0.000007
Training epoch 65 [32640/67107 (48.66%)]	 Loss: 0.000010
Training epoch 65 [33280/67107 (49.62%)]	 Loss: 0.000006
Training epoch 65 [33920/67107 (50.57%)]	 Loss: 0.000016
Training epoch 65 [34560/67107 (51.53%)]	 Loss: 0.000009
Training epoch 65 [35200/67107 (52.48%)]	 Loss: 0.000008
Training epoch 65 [35840/67107 (53.44%)]	 Loss: 0.000002
Training epoch 65 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 65 [37120/67107 (55.34%)]	 Loss: 0.000001
Training epoch 65 [37760/67107 (56.30%)]	 Loss: 0.000003
Training epoch 65 [38400/67107 (57.25%)]	 Loss: 0.000028
Training epoch 65 [39040/67107 (58.21%)]	 Loss: 0.000008
Training epoch 65 [39680/67107 (59.16%)]	 Loss: 0.000041
Training epoch 65 [40320/67107 (60.11%)]	 Loss: 0.000006
Training epoch 65 [40960/67107 (61.07%)]	 Loss: 0.000001
Training epoch 65 [41600/67107 (62.02%)]	 Loss: 0.000018
Training epoch 65 [42240/67107 (62.98%)]	 Loss: 0.000007
Training epoch 65 [42880/67107 (63.93%)]	 Loss: 0.000010
Training epoch 65 [43520/67107 (64.89%)]	 Loss: 0.000062
Training epoch 65 [44160/67107 (65.84%)]	 Loss: 0.000047
Training epoch 65 [44800/67107 (66.79%)]	 Loss: 0.000022
Training epoch 65 [45440/67107 (67.75%)]	 Loss: 0.000020
Training epoch 65 [46080/67107 (68.70%)]	 Loss: 0.000035
Training epoch 65 [46720/67107 (69.66%)]	 Loss: 0.000004
Training epoch 65 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 65 [48000/67107 (71.56%)]	 Loss: 0.000008
Training epoch 65 [48640/67107 (72.52%)]	 Loss: 0.000061
Training epoch 65 [49280/67107 (73.47%)]	 Loss: 0.000060
Training epoch 65 [49920/67107 (74.43%)]	 Loss: 0.000018
Training epoch 65 [50560/67107 (75.38%)]	 Loss: 0.000051
Training epoch 65 [51200/67107 (76.34%)]	 Loss: 0.000014
Training epoch 65 [51840/67107 (77.29%)]	 Loss: 0.000035
Training epoch 65 [52480/67107 (78.24%)]	 Loss: 0.000014
Training epoch 65 [53120/67107 (79.20%)]	 Loss: 0.000014
Training epoch 65 [53760/67107 (80.15%)]	 Loss: 0.000021
Training epoch 65 [54400/67107 (81.11%)]	 Loss: 0.000036
Training epoch 65 [55040/67107 (82.06%)]	 Loss: 0.000038
Training epoch 65 [55680/67107 (83.02%)]	 Loss: 0.000025
Training epoch 65 [56320/67107 (83.97%)]	 Loss: 0.000043
Training epoch 65 [56960/67107 (84.92%)]	 Loss: 0.000011
Training epoch 65 [57600/67107 (85.88%)]	 Loss: 0.000024
Training epoch 65 [58240/67107 (86.83%)]	 Loss: 0.000015
Training epoch 65 [58880/67107 (87.79%)]	 Loss: 0.000011
Training epoch 65 [59520/67107 (88.74%)]	 Loss: 0.000012
Training epoch 65 [60160/67107 (89.69%)]	 Loss: 0.000012
Training epoch 65 [60800/67107 (90.65%)]	 Loss: 0.000003
Training epoch 65 [61440/67107 (91.60%)]	 Loss: 0.000032
Training epoch 65 [62080/67107 (92.56%)]	 Loss: 0.000003
Training epoch 65 [62720/67107 (93.51%)]	 Loss: 0.000058
Training epoch 65 [63360/67107 (94.47%)]	 Loss: 0.000037
Training epoch 65 [64000/67107 (95.42%)]	 Loss: 0.000003
Training epoch 65 [64640/67107 (96.37%)]	 Loss: 0.000078
Training epoch 65 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 65 [65920/67107 (98.28%)]	 Loss: 0.000008
Training epoch 65 [66560/67107 (99.24%)]	 Loss: 0.000002
Test set: Average Loss: 0.000324
Training epoch 66 [0/67107 (0.00%)]	 Loss: 0.000002
Training epoch 66 [640/67107 (0.95%)]	 Loss: 0.000006
Training epoch 66 [1280/67107 (1.91%)]	 Loss: 0.000013
Training epoch 66 [1920/67107 (2.86%)]	 Loss: 0.000043
Training epoch 66 [2560/67107 (3.82%)]	 Loss: 0.000011
Training epoch 66 [3200/67107 (4.77%)]	 Loss: 0.000004
Training epoch 66 [3840/67107 (5.73%)]	 Loss: 0.000033
Training epoch 66 [4480/67107 (6.68%)]	 Loss: 0.000004
Training epoch 66 [5120/67107 (7.63%)]	 Loss: 0.000009
Training epoch 66 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 66 [6400/67107 (9.54%)]	 Loss: 0.000009
Training epoch 66 [7040/67107 (10.50%)]	 Loss: 0.000021
Training epoch 66 [7680/67107 (11.45%)]	 Loss: 0.000037
Training epoch 66 [8320/67107 (12.40%)]	 Loss: 0.000008
Training epoch 66 [8960/67107 (13.36%)]	 Loss: 0.000024
Training epoch 66 [9600/67107 (14.31%)]	 Loss: 0.000004
Training epoch 66 [10240/67107 (15.27%)]	 Loss: 0.000005
Training epoch 66 [10880/67107 (16.22%)]	 Loss: 0.000015
Training epoch 66 [11520/67107 (17.18%)]	 Loss: 0.000004
Training epoch 66 [12160/67107 (18.13%)]	 Loss: 0.000007
Training epoch 66 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 66 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 66 [14080/67107 (20.99%)]	 Loss: 0.000006
Training epoch 66 [14720/67107 (21.95%)]	 Loss: 0.000006
Training epoch 66 [15360/67107 (22.90%)]	 Loss: 0.000022
Training epoch 66 [16000/67107 (23.85%)]	 Loss: 0.000008
Training epoch 66 [16640/67107 (24.81%)]	 Loss: 0.000009
Training epoch 66 [17280/67107 (25.76%)]	 Loss: 0.000015
Training epoch 66 [17920/67107 (26.72%)]	 Loss: 0.000015
Training epoch 66 [18560/67107 (27.67%)]	 Loss: 0.000014
Training epoch 66 [19200/67107 (28.63%)]	 Loss: 0.000033
Training epoch 66 [19840/67107 (29.58%)]	 Loss: 0.000004
Training epoch 66 [20480/67107 (30.53%)]	 Loss: 0.000021
Training epoch 66 [21120/67107 (31.49%)]	 Loss: 0.000008
Training epoch 66 [21760/67107 (32.44%)]	 Loss: 0.000009
Training epoch 66 [22400/67107 (33.40%)]	 Loss: 0.000004
Training epoch 66 [23040/67107 (34.35%)]	 Loss: 0.000035
Training epoch 66 [23680/67107 (35.31%)]	 Loss: 0.000022
Training epoch 66 [24320/67107 (36.26%)]	 Loss: 0.000014
Training epoch 66 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 66 [25600/67107 (38.17%)]	 Loss: 0.000001
Training epoch 66 [26240/67107 (39.12%)]	 Loss: 0.000002
Training epoch 66 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 66 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 66 [28160/67107 (41.98%)]	 Loss: 0.000024
Training epoch 66 [28800/67107 (42.94%)]	 Loss: 0.000058
Training epoch 66 [29440/67107 (43.89%)]	 Loss: 0.000017
Training epoch 66 [30080/67107 (44.85%)]	 Loss: 0.000087
Training epoch 66 [30720/67107 (45.80%)]	 Loss: 0.000013
Training epoch 66 [31360/67107 (46.76%)]	 Loss: 0.000007
Training epoch 66 [32000/67107 (47.71%)]	 Loss: 0.000004
Training epoch 66 [32640/67107 (48.66%)]	 Loss: 0.000011
Training epoch 66 [33280/67107 (49.62%)]	 Loss: 0.000008
Training epoch 66 [33920/67107 (50.57%)]	 Loss: 0.000212
Training epoch 66 [34560/67107 (51.53%)]	 Loss: 0.000008
Training epoch 66 [35200/67107 (52.48%)]	 Loss: 0.000025
Training epoch 66 [35840/67107 (53.44%)]	 Loss: 0.000046
Training epoch 66 [36480/67107 (54.39%)]	 Loss: 0.000007
Training epoch 66 [37120/67107 (55.34%)]	 Loss: 0.000022
Training epoch 66 [37760/67107 (56.30%)]	 Loss: 0.000004
Training epoch 66 [38400/67107 (57.25%)]	 Loss: 0.000005
Training epoch 66 [39040/67107 (58.21%)]	 Loss: 0.000004
Training epoch 66 [39680/67107 (59.16%)]	 Loss: 0.000003
Training epoch 66 [40320/67107 (60.11%)]	 Loss: 0.000015
Training epoch 66 [40960/67107 (61.07%)]	 Loss: 0.000007
Training epoch 66 [41600/67107 (62.02%)]	 Loss: 0.000007
Training epoch 66 [42240/67107 (62.98%)]	 Loss: 0.000007
Training epoch 66 [42880/67107 (63.93%)]	 Loss: 0.000011
Training epoch 66 [43520/67107 (64.89%)]	 Loss: 0.000005
Training epoch 66 [44160/67107 (65.84%)]	 Loss: 0.000009
Training epoch 66 [44800/67107 (66.79%)]	 Loss: 0.000014
Training epoch 66 [45440/67107 (67.75%)]	 Loss: 0.000004
Training epoch 66 [46080/67107 (68.70%)]	 Loss: 0.000013
Training epoch 66 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 66 [47360/67107 (70.61%)]	 Loss: 0.000017
Training epoch 66 [48000/67107 (71.56%)]	 Loss: 0.000012
Training epoch 66 [48640/67107 (72.52%)]	 Loss: 0.000017
Training epoch 66 [49280/67107 (73.47%)]	 Loss: 0.000013
Training epoch 66 [49920/67107 (74.43%)]	 Loss: 0.000012
Training epoch 66 [50560/67107 (75.38%)]	 Loss: 0.000004
Training epoch 66 [51200/67107 (76.34%)]	 Loss: 0.000008
Training epoch 66 [51840/67107 (77.29%)]	 Loss: 0.000015
Training epoch 66 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 66 [53120/67107 (79.20%)]	 Loss: 0.000002
Training epoch 66 [53760/67107 (80.15%)]	 Loss: 0.000008
Training epoch 66 [54400/67107 (81.11%)]	 Loss: 0.000003
Training epoch 66 [55040/67107 (82.06%)]	 Loss: 0.000008
Training epoch 66 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 66 [56320/67107 (83.97%)]	 Loss: 0.000010
Training epoch 66 [56960/67107 (84.92%)]	 Loss: 0.000023
Training epoch 66 [57600/67107 (85.88%)]	 Loss: 0.000013
Training epoch 66 [58240/67107 (86.83%)]	 Loss: 0.000008
Training epoch 66 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 66 [59520/67107 (88.74%)]	 Loss: 0.000017
Training epoch 66 [60160/67107 (89.69%)]	 Loss: 0.000002
Training epoch 66 [60800/67107 (90.65%)]	 Loss: 0.000037
Training epoch 66 [61440/67107 (91.60%)]	 Loss: 0.000002
Training epoch 66 [62080/67107 (92.56%)]	 Loss: 0.000005
Training epoch 66 [62720/67107 (93.51%)]	 Loss: 0.000006
Training epoch 66 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 66 [64000/67107 (95.42%)]	 Loss: 0.000005
Training epoch 66 [64640/67107 (96.37%)]	 Loss: 0.000006
Training epoch 66 [65280/67107 (97.33%)]	 Loss: 0.000012
Training epoch 66 [65920/67107 (98.28%)]	 Loss: 0.000009
Training epoch 66 [66560/67107 (99.24%)]	 Loss: 0.000029
Test set: Average Loss: 0.000364
Training epoch 67 [0/67107 (0.00%)]	 Loss: 0.000011
Training epoch 67 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 67 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 67 [1920/67107 (2.86%)]	 Loss: 0.000013
Training epoch 67 [2560/67107 (3.82%)]	 Loss: 0.000012
Training epoch 67 [3200/67107 (4.77%)]	 Loss: 0.000014
Training epoch 67 [3840/67107 (5.73%)]	 Loss: 0.000034
Training epoch 67 [4480/67107 (6.68%)]	 Loss: 0.000012
Training epoch 67 [5120/67107 (7.63%)]	 Loss: 0.000019
Training epoch 67 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 67 [6400/67107 (9.54%)]	 Loss: 0.000032
Training epoch 67 [7040/67107 (10.50%)]	 Loss: 0.000011
Training epoch 67 [7680/67107 (11.45%)]	 Loss: 0.000003
Training epoch 67 [8320/67107 (12.40%)]	 Loss: 0.000011
Training epoch 67 [8960/67107 (13.36%)]	 Loss: 0.000009
Training epoch 67 [9600/67107 (14.31%)]	 Loss: 0.000002
Training epoch 67 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 67 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 67 [11520/67107 (17.18%)]	 Loss: 0.000025
Training epoch 67 [12160/67107 (18.13%)]	 Loss: 0.000017
Training epoch 67 [12800/67107 (19.08%)]	 Loss: 0.000015
Training epoch 67 [13440/67107 (20.04%)]	 Loss: 0.000005
Training epoch 67 [14080/67107 (20.99%)]	 Loss: 0.000011
Training epoch 67 [14720/67107 (21.95%)]	 Loss: 0.000024
Training epoch 67 [15360/67107 (22.90%)]	 Loss: 0.000010
Training epoch 67 [16000/67107 (23.85%)]	 Loss: 0.000006
Training epoch 67 [16640/67107 (24.81%)]	 Loss: 0.000019
Training epoch 67 [17280/67107 (25.76%)]	 Loss: 0.000009
Training epoch 67 [17920/67107 (26.72%)]	 Loss: 0.000003
Training epoch 67 [18560/67107 (27.67%)]	 Loss: 0.000014
Training epoch 67 [19200/67107 (28.63%)]	 Loss: 0.000002
Training epoch 67 [19840/67107 (29.58%)]	 Loss: 0.000016
Training epoch 67 [20480/67107 (30.53%)]	 Loss: 0.000003
Training epoch 67 [21120/67107 (31.49%)]	 Loss: 0.000008
Training epoch 67 [21760/67107 (32.44%)]	 Loss: 0.000013
Training epoch 67 [22400/67107 (33.40%)]	 Loss: 0.000018
Training epoch 67 [23040/67107 (34.35%)]	 Loss: 0.000007
Training epoch 67 [23680/67107 (35.31%)]	 Loss: 0.000001
Training epoch 67 [24320/67107 (36.26%)]	 Loss: 0.000001
Training epoch 67 [24960/67107 (37.21%)]	 Loss: 0.000002
Training epoch 67 [25600/67107 (38.17%)]	 Loss: 0.000001
Training epoch 67 [26240/67107 (39.12%)]	 Loss: 0.000002
Training epoch 67 [26880/67107 (40.08%)]	 Loss: 0.000002
Training epoch 67 [27520/67107 (41.03%)]	 Loss: 0.000032
Training epoch 67 [28160/67107 (41.98%)]	 Loss: 0.000018
Training epoch 67 [28800/67107 (42.94%)]	 Loss: 0.000035
Training epoch 67 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 67 [30080/67107 (44.85%)]	 Loss: 0.000003
Training epoch 67 [30720/67107 (45.80%)]	 Loss: 0.000026
Training epoch 67 [31360/67107 (46.76%)]	 Loss: 0.000014
Training epoch 67 [32000/67107 (47.71%)]	 Loss: 0.000012
Training epoch 67 [32640/67107 (48.66%)]	 Loss: 0.000015
Training epoch 67 [33280/67107 (49.62%)]	 Loss: 0.000014
Training epoch 67 [33920/67107 (50.57%)]	 Loss: 0.000032
Training epoch 67 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 67 [35200/67107 (52.48%)]	 Loss: 0.000033
Training epoch 67 [35840/67107 (53.44%)]	 Loss: 0.000044
Training epoch 67 [36480/67107 (54.39%)]	 Loss: 0.000050
Training epoch 67 [37120/67107 (55.34%)]	 Loss: 0.000004
Training epoch 67 [37760/67107 (56.30%)]	 Loss: 0.000003
Training epoch 67 [38400/67107 (57.25%)]	 Loss: 0.000001
Training epoch 67 [39040/67107 (58.21%)]	 Loss: 0.000050
Training epoch 67 [39680/67107 (59.16%)]	 Loss: 0.000048
Training epoch 67 [40320/67107 (60.11%)]	 Loss: 0.000130
Training epoch 67 [40960/67107 (61.07%)]	 Loss: 0.000060
Training epoch 67 [41600/67107 (62.02%)]	 Loss: 0.000083
Training epoch 67 [42240/67107 (62.98%)]	 Loss: 0.000013
Training epoch 67 [42880/67107 (63.93%)]	 Loss: 0.000016
Training epoch 67 [43520/67107 (64.89%)]	 Loss: 0.000013
Training epoch 67 [44160/67107 (65.84%)]	 Loss: 0.000019
Training epoch 67 [44800/67107 (66.79%)]	 Loss: 0.000017
Training epoch 67 [45440/67107 (67.75%)]	 Loss: 0.000004
Training epoch 67 [46080/67107 (68.70%)]	 Loss: 0.000011
Training epoch 67 [46720/67107 (69.66%)]	 Loss: 0.000008
Training epoch 67 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 67 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 67 [48640/67107 (72.52%)]	 Loss: 0.000058
Training epoch 67 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 67 [49920/67107 (74.43%)]	 Loss: 0.000004
Training epoch 67 [50560/67107 (75.38%)]	 Loss: 0.000018
Training epoch 67 [51200/67107 (76.34%)]	 Loss: 0.000017
Training epoch 67 [51840/67107 (77.29%)]	 Loss: 0.000016
Training epoch 67 [52480/67107 (78.24%)]	 Loss: 0.000008
Training epoch 67 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 67 [53760/67107 (80.15%)]	 Loss: 0.000061
Training epoch 67 [54400/67107 (81.11%)]	 Loss: 0.000023
Training epoch 67 [55040/67107 (82.06%)]	 Loss: 0.000024
Training epoch 67 [55680/67107 (83.02%)]	 Loss: 0.000009
Training epoch 67 [56320/67107 (83.97%)]	 Loss: 0.000008
Training epoch 67 [56960/67107 (84.92%)]	 Loss: 0.000006
Training epoch 67 [57600/67107 (85.88%)]	 Loss: 0.000002
Training epoch 67 [58240/67107 (86.83%)]	 Loss: 0.000006
Training epoch 67 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 67 [59520/67107 (88.74%)]	 Loss: 0.000018
Training epoch 67 [60160/67107 (89.69%)]	 Loss: 0.000006
Training epoch 67 [60800/67107 (90.65%)]	 Loss: 0.000008
Training epoch 67 [61440/67107 (91.60%)]	 Loss: 0.000005
Training epoch 67 [62080/67107 (92.56%)]	 Loss: 0.000011
Training epoch 67 [62720/67107 (93.51%)]	 Loss: 0.000006
Training epoch 67 [63360/67107 (94.47%)]	 Loss: 0.000012
Training epoch 67 [64000/67107 (95.42%)]	 Loss: 0.000015
Training epoch 67 [64640/67107 (96.37%)]	 Loss: 0.000015
Training epoch 67 [65280/67107 (97.33%)]	 Loss: 0.000036
Training epoch 67 [65920/67107 (98.28%)]	 Loss: 0.000053
Training epoch 67 [66560/67107 (99.24%)]	 Loss: 0.000009
Test set: Average Loss: 0.000164
Training epoch 68 [0/67107 (0.00%)]	 Loss: 0.000012
Training epoch 68 [640/67107 (0.95%)]	 Loss: 0.000006
Training epoch 68 [1280/67107 (1.91%)]	 Loss: 0.000024
Training epoch 68 [1920/67107 (2.86%)]	 Loss: 0.000009
Training epoch 68 [2560/67107 (3.82%)]	 Loss: 0.000010
Training epoch 68 [3200/67107 (4.77%)]	 Loss: 0.000006
Training epoch 68 [3840/67107 (5.73%)]	 Loss: 0.000036
Training epoch 68 [4480/67107 (6.68%)]	 Loss: 0.000006
Training epoch 68 [5120/67107 (7.63%)]	 Loss: 0.000003
Training epoch 68 [5760/67107 (8.59%)]	 Loss: 0.000024
Training epoch 68 [6400/67107 (9.54%)]	 Loss: 0.000007
Training epoch 68 [7040/67107 (10.50%)]	 Loss: 0.000013
Training epoch 68 [7680/67107 (11.45%)]	 Loss: 0.000010
Training epoch 68 [8320/67107 (12.40%)]	 Loss: 0.000046
Training epoch 68 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 68 [9600/67107 (14.31%)]	 Loss: 0.000031
Training epoch 68 [10240/67107 (15.27%)]	 Loss: 0.000009
Training epoch 68 [10880/67107 (16.22%)]	 Loss: 0.000008
Training epoch 68 [11520/67107 (17.18%)]	 Loss: 0.000001
Training epoch 68 [12160/67107 (18.13%)]	 Loss: 0.000044
Training epoch 68 [12800/67107 (19.08%)]	 Loss: 0.000021
Training epoch 68 [13440/67107 (20.04%)]	 Loss: 0.000007
Training epoch 68 [14080/67107 (20.99%)]	 Loss: 0.000034
Training epoch 68 [14720/67107 (21.95%)]	 Loss: 0.000025
Training epoch 68 [15360/67107 (22.90%)]	 Loss: 0.000011
Training epoch 68 [16000/67107 (23.85%)]	 Loss: 0.000006
Training epoch 68 [16640/67107 (24.81%)]	 Loss: 0.000018
Training epoch 68 [17280/67107 (25.76%)]	 Loss: 0.000008
Training epoch 68 [17920/67107 (26.72%)]	 Loss: 0.000004
Training epoch 68 [18560/67107 (27.67%)]	 Loss: 0.000003
Training epoch 68 [19200/67107 (28.63%)]	 Loss: 0.000002
Training epoch 68 [19840/67107 (29.58%)]	 Loss: 0.000012
Training epoch 68 [20480/67107 (30.53%)]	 Loss: 0.000014
Training epoch 68 [21120/67107 (31.49%)]	 Loss: 0.000012
Training epoch 68 [21760/67107 (32.44%)]	 Loss: 0.000002
Training epoch 68 [22400/67107 (33.40%)]	 Loss: 0.000008
Training epoch 68 [23040/67107 (34.35%)]	 Loss: 0.000003
Training epoch 68 [23680/67107 (35.31%)]	 Loss: 0.000004
Training epoch 68 [24320/67107 (36.26%)]	 Loss: 0.000037
Training epoch 68 [24960/67107 (37.21%)]	 Loss: 0.000012
Training epoch 68 [25600/67107 (38.17%)]	 Loss: 0.000010
Training epoch 68 [26240/67107 (39.12%)]	 Loss: 0.000086
Training epoch 68 [26880/67107 (40.08%)]	 Loss: 0.000026
Training epoch 68 [27520/67107 (41.03%)]	 Loss: 0.000034
Training epoch 68 [28160/67107 (41.98%)]	 Loss: 0.000047
Training epoch 68 [28800/67107 (42.94%)]	 Loss: 0.000007
Training epoch 68 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 68 [30080/67107 (44.85%)]	 Loss: 0.000004
Training epoch 68 [30720/67107 (45.80%)]	 Loss: 0.000002
Training epoch 68 [31360/67107 (46.76%)]	 Loss: 0.000031
Training epoch 68 [32000/67107 (47.71%)]	 Loss: 0.000017
Training epoch 68 [32640/67107 (48.66%)]	 Loss: 0.000044
Training epoch 68 [33280/67107 (49.62%)]	 Loss: 0.000011
Training epoch 68 [33920/67107 (50.57%)]	 Loss: 0.000010
Training epoch 68 [34560/67107 (51.53%)]	 Loss: 0.000014
Training epoch 68 [35200/67107 (52.48%)]	 Loss: 0.000003
Training epoch 68 [35840/67107 (53.44%)]	 Loss: 0.000002
Training epoch 68 [36480/67107 (54.39%)]	 Loss: 0.000001
Training epoch 68 [37120/67107 (55.34%)]	 Loss: 0.000005
Training epoch 68 [37760/67107 (56.30%)]	 Loss: 0.000008
Training epoch 68 [38400/67107 (57.25%)]	 Loss: 0.000005
Training epoch 68 [39040/67107 (58.21%)]	 Loss: 0.000016
Training epoch 68 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 68 [40320/67107 (60.11%)]	 Loss: 0.000015
Training epoch 68 [40960/67107 (61.07%)]	 Loss: 0.000004
Training epoch 68 [41600/67107 (62.02%)]	 Loss: 0.000004
Training epoch 68 [42240/67107 (62.98%)]	 Loss: 0.000020
Training epoch 68 [42880/67107 (63.93%)]	 Loss: 0.000009
Training epoch 68 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 68 [44160/67107 (65.84%)]	 Loss: 0.000008
Training epoch 68 [44800/67107 (66.79%)]	 Loss: 0.000012
Training epoch 68 [45440/67107 (67.75%)]	 Loss: 0.000017
Training epoch 68 [46080/67107 (68.70%)]	 Loss: 0.000008
Training epoch 68 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 68 [47360/67107 (70.61%)]	 Loss: 0.000002
Training epoch 68 [48000/67107 (71.56%)]	 Loss: 0.000002
Training epoch 68 [48640/67107 (72.52%)]	 Loss: 0.000012
Training epoch 68 [49280/67107 (73.47%)]	 Loss: 0.000021
Training epoch 68 [49920/67107 (74.43%)]	 Loss: 0.000029
Training epoch 68 [50560/67107 (75.38%)]	 Loss: 0.000007
Training epoch 68 [51200/67107 (76.34%)]	 Loss: 0.000003
Training epoch 68 [51840/67107 (77.29%)]	 Loss: 0.000006
Training epoch 68 [52480/67107 (78.24%)]	 Loss: 0.000017
Training epoch 68 [53120/67107 (79.20%)]	 Loss: 0.000008
Training epoch 68 [53760/67107 (80.15%)]	 Loss: 0.000055
Training epoch 68 [54400/67107 (81.11%)]	 Loss: 0.000002
Training epoch 68 [55040/67107 (82.06%)]	 Loss: 0.000011
Training epoch 68 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 68 [56320/67107 (83.97%)]	 Loss: 0.000005
Training epoch 68 [56960/67107 (84.92%)]	 Loss: 0.000027
Training epoch 68 [57600/67107 (85.88%)]	 Loss: 0.000002
Training epoch 68 [58240/67107 (86.83%)]	 Loss: 0.000005
Training epoch 68 [58880/67107 (87.79%)]	 Loss: 0.000007
Training epoch 68 [59520/67107 (88.74%)]	 Loss: 0.000002
Training epoch 68 [60160/67107 (89.69%)]	 Loss: 0.000006
Training epoch 68 [60800/67107 (90.65%)]	 Loss: 0.000006
Training epoch 68 [61440/67107 (91.60%)]	 Loss: 0.000019
Training epoch 68 [62080/67107 (92.56%)]	 Loss: 0.000011
Training epoch 68 [62720/67107 (93.51%)]	 Loss: 0.000016
Training epoch 68 [63360/67107 (94.47%)]	 Loss: 0.000059
Training epoch 68 [64000/67107 (95.42%)]	 Loss: 0.000018
Training epoch 68 [64640/67107 (96.37%)]	 Loss: 0.000001
Training epoch 68 [65280/67107 (97.33%)]	 Loss: 0.000005
Training epoch 68 [65920/67107 (98.28%)]	 Loss: 0.000006
Training epoch 68 [66560/67107 (99.24%)]	 Loss: 0.000005
Test set: Average Loss: 0.001023
Training epoch 69 [0/67107 (0.00%)]	 Loss: 0.000021
Training epoch 69 [640/67107 (0.95%)]	 Loss: 0.000008
Training epoch 69 [1280/67107 (1.91%)]	 Loss: 0.000009
Training epoch 69 [1920/67107 (2.86%)]	 Loss: 0.000011
Training epoch 69 [2560/67107 (3.82%)]	 Loss: 0.000006
Training epoch 69 [3200/67107 (4.77%)]	 Loss: 0.000005
Training epoch 69 [3840/67107 (5.73%)]	 Loss: 0.000036
Training epoch 69 [4480/67107 (6.68%)]	 Loss: 0.000017
Training epoch 69 [5120/67107 (7.63%)]	 Loss: 0.000013
Training epoch 69 [5760/67107 (8.59%)]	 Loss: 0.000012
Training epoch 69 [6400/67107 (9.54%)]	 Loss: 0.000063
Training epoch 69 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 69 [7680/67107 (11.45%)]	 Loss: 0.000022
Training epoch 69 [8320/67107 (12.40%)]	 Loss: 0.000009
Training epoch 69 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 69 [9600/67107 (14.31%)]	 Loss: 0.000021
Training epoch 69 [10240/67107 (15.27%)]	 Loss: 0.000018
Training epoch 69 [10880/67107 (16.22%)]	 Loss: 0.000027
Training epoch 69 [11520/67107 (17.18%)]	 Loss: 0.000017
Training epoch 69 [12160/67107 (18.13%)]	 Loss: 0.000001
Training epoch 69 [12800/67107 (19.08%)]	 Loss: 0.000056
Training epoch 69 [13440/67107 (20.04%)]	 Loss: 0.000002
Training epoch 69 [14080/67107 (20.99%)]	 Loss: 0.000007
Training epoch 69 [14720/67107 (21.95%)]	 Loss: 0.000008
Training epoch 69 [15360/67107 (22.90%)]	 Loss: 0.000004
Training epoch 69 [16000/67107 (23.85%)]	 Loss: 0.000029
Training epoch 69 [16640/67107 (24.81%)]	 Loss: 0.000014
Training epoch 69 [17280/67107 (25.76%)]	 Loss: 0.000017
Training epoch 69 [17920/67107 (26.72%)]	 Loss: 0.000007
Training epoch 69 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 69 [19200/67107 (28.63%)]	 Loss: 0.000003
Training epoch 69 [19840/67107 (29.58%)]	 Loss: 0.000033
Training epoch 69 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 69 [21120/67107 (31.49%)]	 Loss: 0.000009
Training epoch 69 [21760/67107 (32.44%)]	 Loss: 0.000033
Training epoch 69 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 69 [23040/67107 (34.35%)]	 Loss: 0.000034
Training epoch 69 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 69 [24320/67107 (36.26%)]	 Loss: 0.000012
Training epoch 69 [24960/67107 (37.21%)]	 Loss: 0.000030
Training epoch 69 [25600/67107 (38.17%)]	 Loss: 0.000023
Training epoch 69 [26240/67107 (39.12%)]	 Loss: 0.000005
Training epoch 69 [26880/67107 (40.08%)]	 Loss: 0.000014
Training epoch 69 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 69 [28160/67107 (41.98%)]	 Loss: 0.000020
Training epoch 69 [28800/67107 (42.94%)]	 Loss: 0.000021
Training epoch 69 [29440/67107 (43.89%)]	 Loss: 0.000005
Training epoch 69 [30080/67107 (44.85%)]	 Loss: 0.000011
Training epoch 69 [30720/67107 (45.80%)]	 Loss: 0.000019
Training epoch 69 [31360/67107 (46.76%)]	 Loss: 0.000023
Training epoch 69 [32000/67107 (47.71%)]	 Loss: 0.000007
Training epoch 69 [32640/67107 (48.66%)]	 Loss: 0.000007
Training epoch 69 [33280/67107 (49.62%)]	 Loss: 0.000011
Training epoch 69 [33920/67107 (50.57%)]	 Loss: 0.000014
Training epoch 69 [34560/67107 (51.53%)]	 Loss: 0.000007
Training epoch 69 [35200/67107 (52.48%)]	 Loss: 0.000007
Training epoch 69 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 69 [36480/67107 (54.39%)]	 Loss: 0.000001
Training epoch 69 [37120/67107 (55.34%)]	 Loss: 0.000008
Training epoch 69 [37760/67107 (56.30%)]	 Loss: 0.000057
Training epoch 69 [38400/67107 (57.25%)]	 Loss: 0.000064
Training epoch 69 [39040/67107 (58.21%)]	 Loss: 0.000004
Training epoch 69 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 69 [40320/67107 (60.11%)]	 Loss: 0.000005
Training epoch 69 [40960/67107 (61.07%)]	 Loss: 0.000008
Training epoch 69 [41600/67107 (62.02%)]	 Loss: 0.000012
Training epoch 69 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 69 [42880/67107 (63.93%)]	 Loss: 0.000009
Training epoch 69 [43520/67107 (64.89%)]	 Loss: 0.000008
Training epoch 69 [44160/67107 (65.84%)]	 Loss: 0.000028
Training epoch 69 [44800/67107 (66.79%)]	 Loss: 0.000042
Training epoch 69 [45440/67107 (67.75%)]	 Loss: 0.000018
Training epoch 69 [46080/67107 (68.70%)]	 Loss: 0.000024
Training epoch 69 [46720/67107 (69.66%)]	 Loss: 0.000006
Training epoch 69 [47360/67107 (70.61%)]	 Loss: 0.000015
Training epoch 69 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 69 [48640/67107 (72.52%)]	 Loss: 0.000006
Training epoch 69 [49280/67107 (73.47%)]	 Loss: 0.000004
Training epoch 69 [49920/67107 (74.43%)]	 Loss: 0.000005
Training epoch 69 [50560/67107 (75.38%)]	 Loss: 0.000012
Training epoch 69 [51200/67107 (76.34%)]	 Loss: 0.000014
Training epoch 69 [51840/67107 (77.29%)]	 Loss: 0.000003
Training epoch 69 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 69 [53120/67107 (79.20%)]	 Loss: 0.000010
Training epoch 69 [53760/67107 (80.15%)]	 Loss: 0.000003
Training epoch 69 [54400/67107 (81.11%)]	 Loss: 0.000008
Training epoch 69 [55040/67107 (82.06%)]	 Loss: 0.000004
Training epoch 69 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 69 [56320/67107 (83.97%)]	 Loss: 0.000008
Training epoch 69 [56960/67107 (84.92%)]	 Loss: 0.000008
Training epoch 69 [57600/67107 (85.88%)]	 Loss: 0.000014
Training epoch 69 [58240/67107 (86.83%)]	 Loss: 0.000023
Training epoch 69 [58880/67107 (87.79%)]	 Loss: 0.000004
Training epoch 69 [59520/67107 (88.74%)]	 Loss: 0.000008
Training epoch 69 [60160/67107 (89.69%)]	 Loss: 0.000006
Training epoch 69 [60800/67107 (90.65%)]	 Loss: 0.000004
Training epoch 69 [61440/67107 (91.60%)]	 Loss: 0.000002
Training epoch 69 [62080/67107 (92.56%)]	 Loss: 0.000008
Training epoch 69 [62720/67107 (93.51%)]	 Loss: 0.000002
Training epoch 69 [63360/67107 (94.47%)]	 Loss: 0.000008
Training epoch 69 [64000/67107 (95.42%)]	 Loss: 0.000003
Training epoch 69 [64640/67107 (96.37%)]	 Loss: 0.000001
Training epoch 69 [65280/67107 (97.33%)]	 Loss: 0.000029
Training epoch 69 [65920/67107 (98.28%)]	 Loss: 0.000007
Training epoch 69 [66560/67107 (99.24%)]	 Loss: 0.000013
Test set: Average Loss: 0.000918
Training epoch 70 [0/67107 (0.00%)]	 Loss: 0.000004
Training epoch 70 [640/67107 (0.95%)]	 Loss: 0.000002
Training epoch 70 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 70 [1920/67107 (2.86%)]	 Loss: 0.000003
Training epoch 70 [2560/67107 (3.82%)]	 Loss: 0.000002
Training epoch 70 [3200/67107 (4.77%)]	 Loss: 0.000004
Training epoch 70 [3840/67107 (5.73%)]	 Loss: 0.000008
Training epoch 70 [4480/67107 (6.68%)]	 Loss: 0.000007
Training epoch 70 [5120/67107 (7.63%)]	 Loss: 0.000012
Training epoch 70 [5760/67107 (8.59%)]	 Loss: 0.000005
Training epoch 70 [6400/67107 (9.54%)]	 Loss: 0.000043
Training epoch 70 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 70 [7680/67107 (11.45%)]	 Loss: 0.000003
Training epoch 70 [8320/67107 (12.40%)]	 Loss: 0.000011
Training epoch 70 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 70 [9600/67107 (14.31%)]	 Loss: 0.000001
Training epoch 70 [10240/67107 (15.27%)]	 Loss: 0.000010
Training epoch 70 [10880/67107 (16.22%)]	 Loss: 0.000005
Training epoch 70 [11520/67107 (17.18%)]	 Loss: 0.000003
Training epoch 70 [12160/67107 (18.13%)]	 Loss: 0.000006
Training epoch 70 [12800/67107 (19.08%)]	 Loss: 0.000006
Training epoch 70 [13440/67107 (20.04%)]	 Loss: 0.000002
Training epoch 70 [14080/67107 (20.99%)]	 Loss: 0.000001
Training epoch 70 [14720/67107 (21.95%)]	 Loss: 0.000023
Training epoch 70 [15360/67107 (22.90%)]	 Loss: 0.000011
Training epoch 70 [16000/67107 (23.85%)]	 Loss: 0.000012
Training epoch 70 [16640/67107 (24.81%)]	 Loss: 0.000017
Training epoch 70 [17280/67107 (25.76%)]	 Loss: 0.000004
Training epoch 70 [17920/67107 (26.72%)]	 Loss: 0.000014
Training epoch 70 [18560/67107 (27.67%)]	 Loss: 0.000030
Training epoch 70 [19200/67107 (28.63%)]	 Loss: 0.000058
Training epoch 70 [19840/67107 (29.58%)]	 Loss: 0.000011
Training epoch 70 [20480/67107 (30.53%)]	 Loss: 0.000024
Training epoch 70 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 70 [21760/67107 (32.44%)]	 Loss: 0.000005
Training epoch 70 [22400/67107 (33.40%)]	 Loss: 0.000003
Training epoch 70 [23040/67107 (34.35%)]	 Loss: 0.000006
Training epoch 70 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 70 [24320/67107 (36.26%)]	 Loss: 0.000017
Training epoch 70 [24960/67107 (37.21%)]	 Loss: 0.000013
Training epoch 70 [25600/67107 (38.17%)]	 Loss: 0.000013
Training epoch 70 [26240/67107 (39.12%)]	 Loss: 0.000012
Training epoch 70 [26880/67107 (40.08%)]	 Loss: 0.000026
Training epoch 70 [27520/67107 (41.03%)]	 Loss: 0.000012
Training epoch 70 [28160/67107 (41.98%)]	 Loss: 0.000005
Training epoch 70 [28800/67107 (42.94%)]	 Loss: 0.000011
Training epoch 70 [29440/67107 (43.89%)]	 Loss: 0.000012
Training epoch 70 [30080/67107 (44.85%)]	 Loss: 0.000004
Training epoch 70 [30720/67107 (45.80%)]	 Loss: 0.000005
Training epoch 70 [31360/67107 (46.76%)]	 Loss: 0.000007
Training epoch 70 [32000/67107 (47.71%)]	 Loss: 0.000016
Training epoch 70 [32640/67107 (48.66%)]	 Loss: 0.000009
Training epoch 70 [33280/67107 (49.62%)]	 Loss: 0.000029
Training epoch 70 [33920/67107 (50.57%)]	 Loss: 0.000008
Training epoch 70 [34560/67107 (51.53%)]	 Loss: 0.000002
Training epoch 70 [35200/67107 (52.48%)]	 Loss: 0.000002
Training epoch 70 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 70 [36480/67107 (54.39%)]	 Loss: 0.000004
Training epoch 70 [37120/67107 (55.34%)]	 Loss: 0.000011
Training epoch 70 [37760/67107 (56.30%)]	 Loss: 0.000019
Training epoch 70 [38400/67107 (57.25%)]	 Loss: 0.000027
Training epoch 70 [39040/67107 (58.21%)]	 Loss: 0.000003
Training epoch 70 [39680/67107 (59.16%)]	 Loss: 0.000026
Training epoch 70 [40320/67107 (60.11%)]	 Loss: 0.000006
Training epoch 70 [40960/67107 (61.07%)]	 Loss: 0.000007
Training epoch 70 [41600/67107 (62.02%)]	 Loss: 0.000037
Training epoch 70 [42240/67107 (62.98%)]	 Loss: 0.000008
Training epoch 70 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 70 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 70 [44160/67107 (65.84%)]	 Loss: 0.000007
Training epoch 70 [44800/67107 (66.79%)]	 Loss: 0.000012
Training epoch 70 [45440/67107 (67.75%)]	 Loss: 0.000013
Training epoch 70 [46080/67107 (68.70%)]	 Loss: 0.000005
Training epoch 70 [46720/67107 (69.66%)]	 Loss: 0.000021
Training epoch 70 [47360/67107 (70.61%)]	 Loss: 0.000014
Training epoch 70 [48000/67107 (71.56%)]	 Loss: 0.000006
Training epoch 70 [48640/67107 (72.52%)]	 Loss: 0.000016
Training epoch 70 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 70 [49920/67107 (74.43%)]	 Loss: 0.000002
Training epoch 70 [50560/67107 (75.38%)]	 Loss: 0.000003
Training epoch 70 [51200/67107 (76.34%)]	 Loss: 0.000012
Training epoch 70 [51840/67107 (77.29%)]	 Loss: 0.000030
Training epoch 70 [52480/67107 (78.24%)]	 Loss: 0.000006
Training epoch 70 [53120/67107 (79.20%)]	 Loss: 0.000002
Training epoch 70 [53760/67107 (80.15%)]	 Loss: 0.000020
Training epoch 70 [54400/67107 (81.11%)]	 Loss: 0.000006
Training epoch 70 [55040/67107 (82.06%)]	 Loss: 0.000004
Training epoch 70 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 70 [56320/67107 (83.97%)]	 Loss: 0.000006
Training epoch 70 [56960/67107 (84.92%)]	 Loss: 0.000018
Training epoch 70 [57600/67107 (85.88%)]	 Loss: 0.000006
Training epoch 70 [58240/67107 (86.83%)]	 Loss: 0.000017
Training epoch 70 [58880/67107 (87.79%)]	 Loss: 0.000008
Training epoch 70 [59520/67107 (88.74%)]	 Loss: 0.000016
Training epoch 70 [60160/67107 (89.69%)]	 Loss: 0.000009
Training epoch 70 [60800/67107 (90.65%)]	 Loss: 0.000019
Training epoch 70 [61440/67107 (91.60%)]	 Loss: 0.000006
Training epoch 70 [62080/67107 (92.56%)]	 Loss: 0.000008
Training epoch 70 [62720/67107 (93.51%)]	 Loss: 0.000003
Training epoch 70 [63360/67107 (94.47%)]	 Loss: 0.000002
Training epoch 70 [64000/67107 (95.42%)]	 Loss: 0.000006
Training epoch 70 [64640/67107 (96.37%)]	 Loss: 0.000005
Training epoch 70 [65280/67107 (97.33%)]	 Loss: 0.000011
Training epoch 70 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 70 [66560/67107 (99.24%)]	 Loss: 0.000010
Test set: Average Loss: 0.001099
Training epoch 71 [0/67107 (0.00%)]	 Loss: 0.000006
Training epoch 71 [640/67107 (0.95%)]	 Loss: 0.000017
Training epoch 71 [1280/67107 (1.91%)]	 Loss: 0.000006
Training epoch 71 [1920/67107 (2.86%)]	 Loss: 0.000005
Training epoch 71 [2560/67107 (3.82%)]	 Loss: 0.000001
Training epoch 71 [3200/67107 (4.77%)]	 Loss: 0.000006
Training epoch 71 [3840/67107 (5.73%)]	 Loss: 0.000001
Training epoch 71 [4480/67107 (6.68%)]	 Loss: 0.000010
Training epoch 71 [5120/67107 (7.63%)]	 Loss: 0.000022
Training epoch 71 [5760/67107 (8.59%)]	 Loss: 0.000008
Training epoch 71 [6400/67107 (9.54%)]	 Loss: 0.000009
Training epoch 71 [7040/67107 (10.50%)]	 Loss: 0.000007
Training epoch 71 [7680/67107 (11.45%)]	 Loss: 0.000058
Training epoch 71 [8320/67107 (12.40%)]	 Loss: 0.000025
Training epoch 71 [8960/67107 (13.36%)]	 Loss: 0.000012
Training epoch 71 [9600/67107 (14.31%)]	 Loss: 0.000005
Training epoch 71 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 71 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 71 [11520/67107 (17.18%)]	 Loss: 0.000001
Training epoch 71 [12160/67107 (18.13%)]	 Loss: 0.000001
Training epoch 71 [12800/67107 (19.08%)]	 Loss: 0.000005
Training epoch 71 [13440/67107 (20.04%)]	 Loss: 0.000011
Training epoch 71 [14080/67107 (20.99%)]	 Loss: 0.000010
Training epoch 71 [14720/67107 (21.95%)]	 Loss: 0.000013
Training epoch 71 [15360/67107 (22.90%)]	 Loss: 0.000012
Training epoch 71 [16000/67107 (23.85%)]	 Loss: 0.000007
Training epoch 71 [16640/67107 (24.81%)]	 Loss: 0.000039
Training epoch 71 [17280/67107 (25.76%)]	 Loss: 0.000026
Training epoch 71 [17920/67107 (26.72%)]	 Loss: 0.000010
Training epoch 71 [18560/67107 (27.67%)]	 Loss: 0.000007
Training epoch 71 [19200/67107 (28.63%)]	 Loss: 0.000005
Training epoch 71 [19840/67107 (29.58%)]	 Loss: 0.000011
Training epoch 71 [20480/67107 (30.53%)]	 Loss: 0.000019
Training epoch 71 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 71 [21760/67107 (32.44%)]	 Loss: 0.000001
Training epoch 71 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 71 [23040/67107 (34.35%)]	 Loss: 0.000027
Training epoch 71 [23680/67107 (35.31%)]	 Loss: 0.000017
Training epoch 71 [24320/67107 (36.26%)]	 Loss: 0.000014
Training epoch 71 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 71 [25600/67107 (38.17%)]	 Loss: 0.000004
Training epoch 71 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 71 [26880/67107 (40.08%)]	 Loss: 0.000013
Training epoch 71 [27520/67107 (41.03%)]	 Loss: 0.000014
Training epoch 71 [28160/67107 (41.98%)]	 Loss: 0.000005
Training epoch 71 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 71 [29440/67107 (43.89%)]	 Loss: 0.000004
Training epoch 71 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 71 [30720/67107 (45.80%)]	 Loss: 0.000006
Training epoch 71 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 71 [32000/67107 (47.71%)]	 Loss: 0.000015
Training epoch 71 [32640/67107 (48.66%)]	 Loss: 0.000067
Training epoch 71 [33280/67107 (49.62%)]	 Loss: 0.000030
Training epoch 71 [33920/67107 (50.57%)]	 Loss: 0.000019
Training epoch 71 [34560/67107 (51.53%)]	 Loss: 0.000011
Training epoch 71 [35200/67107 (52.48%)]	 Loss: 0.000012
Training epoch 71 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 71 [36480/67107 (54.39%)]	 Loss: 0.000009
Training epoch 71 [37120/67107 (55.34%)]	 Loss: 0.000004
Training epoch 71 [37760/67107 (56.30%)]	 Loss: 0.000005
Training epoch 71 [38400/67107 (57.25%)]	 Loss: 0.000002
Training epoch 71 [39040/67107 (58.21%)]	 Loss: 0.000001
Training epoch 71 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 71 [40320/67107 (60.11%)]	 Loss: 0.000048
Training epoch 71 [40960/67107 (61.07%)]	 Loss: 0.000020
Training epoch 71 [41600/67107 (62.02%)]	 Loss: 0.000022
Training epoch 71 [42240/67107 (62.98%)]	 Loss: 0.000007
Training epoch 71 [42880/67107 (63.93%)]	 Loss: 0.000005
Training epoch 71 [43520/67107 (64.89%)]	 Loss: 0.000009
Training epoch 71 [44160/67107 (65.84%)]	 Loss: 0.000006
Training epoch 71 [44800/67107 (66.79%)]	 Loss: 0.000017
Training epoch 71 [45440/67107 (67.75%)]	 Loss: 0.000002
Training epoch 71 [46080/67107 (68.70%)]	 Loss: 0.000006
Training epoch 71 [46720/67107 (69.66%)]	 Loss: 0.000038
Training epoch 71 [47360/67107 (70.61%)]	 Loss: 0.000011
Training epoch 71 [48000/67107 (71.56%)]	 Loss: 0.000016
Training epoch 71 [48640/67107 (72.52%)]	 Loss: 0.000005
Training epoch 71 [49280/67107 (73.47%)]	 Loss: 0.000011
Training epoch 71 [49920/67107 (74.43%)]	 Loss: 0.000005
Training epoch 71 [50560/67107 (75.38%)]	 Loss: 0.000029
Training epoch 71 [51200/67107 (76.34%)]	 Loss: 0.000006
Training epoch 71 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 71 [52480/67107 (78.24%)]	 Loss: 0.000006
Training epoch 71 [53120/67107 (79.20%)]	 Loss: 0.000005
Training epoch 71 [53760/67107 (80.15%)]	 Loss: 0.000009
Training epoch 71 [54400/67107 (81.11%)]	 Loss: 0.000013
Training epoch 71 [55040/67107 (82.06%)]	 Loss: 0.000012
Training epoch 71 [55680/67107 (83.02%)]	 Loss: 0.000025
Training epoch 71 [56320/67107 (83.97%)]	 Loss: 0.000021
Training epoch 71 [56960/67107 (84.92%)]	 Loss: 0.000009
Training epoch 71 [57600/67107 (85.88%)]	 Loss: 0.000012
Training epoch 71 [58240/67107 (86.83%)]	 Loss: 0.000005
Training epoch 71 [58880/67107 (87.79%)]	 Loss: 0.000006
Training epoch 71 [59520/67107 (88.74%)]	 Loss: 0.000014
Training epoch 71 [60160/67107 (89.69%)]	 Loss: 0.000012
Training epoch 71 [60800/67107 (90.65%)]	 Loss: 0.000008
Training epoch 71 [61440/67107 (91.60%)]	 Loss: 0.000016
Training epoch 71 [62080/67107 (92.56%)]	 Loss: 0.000008
Training epoch 71 [62720/67107 (93.51%)]	 Loss: 0.000005
Training epoch 71 [63360/67107 (94.47%)]	 Loss: 0.000003
Training epoch 71 [64000/67107 (95.42%)]	 Loss: 0.000005
Training epoch 71 [64640/67107 (96.37%)]	 Loss: 0.000004
Training epoch 71 [65280/67107 (97.33%)]	 Loss: 0.000003
Training epoch 71 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 71 [66560/67107 (99.24%)]	 Loss: 0.000010
Test set: Average Loss: 0.000596
Training epoch 72 [0/67107 (0.00%)]	 Loss: 0.000001
Training epoch 72 [640/67107 (0.95%)]	 Loss: 0.000007
Training epoch 72 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 72 [1920/67107 (2.86%)]	 Loss: 0.000014
Training epoch 72 [2560/67107 (3.82%)]	 Loss: 0.000065
Training epoch 72 [3200/67107 (4.77%)]	 Loss: 0.000009
Training epoch 72 [3840/67107 (5.73%)]	 Loss: 0.000041
Training epoch 72 [4480/67107 (6.68%)]	 Loss: 0.000010
Training epoch 72 [5120/67107 (7.63%)]	 Loss: 0.000011
Training epoch 72 [5760/67107 (8.59%)]	 Loss: 0.000003
Training epoch 72 [6400/67107 (9.54%)]	 Loss: 0.000012
Training epoch 72 [7040/67107 (10.50%)]	 Loss: 0.000007
Training epoch 72 [7680/67107 (11.45%)]	 Loss: 0.000020
Training epoch 72 [8320/67107 (12.40%)]	 Loss: 0.000008
Training epoch 72 [8960/67107 (13.36%)]	 Loss: 0.000013
Training epoch 72 [9600/67107 (14.31%)]	 Loss: 0.000059
Training epoch 72 [10240/67107 (15.27%)]	 Loss: 0.000011
Training epoch 72 [10880/67107 (16.22%)]	 Loss: 0.000005
Training epoch 72 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 72 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 72 [12800/67107 (19.08%)]	 Loss: 0.000002
Training epoch 72 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 72 [14080/67107 (20.99%)]	 Loss: 0.000003
Training epoch 72 [14720/67107 (21.95%)]	 Loss: 0.000007
Training epoch 72 [15360/67107 (22.90%)]	 Loss: 0.000025
Training epoch 72 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 72 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 72 [17280/67107 (25.76%)]	 Loss: 0.000002
Training epoch 72 [17920/67107 (26.72%)]	 Loss: 0.000002
Training epoch 72 [18560/67107 (27.67%)]	 Loss: 0.000003
Training epoch 72 [19200/67107 (28.63%)]	 Loss: 0.000012
Training epoch 72 [19840/67107 (29.58%)]	 Loss: 0.000003
Training epoch 72 [20480/67107 (30.53%)]	 Loss: 0.000021
Training epoch 72 [21120/67107 (31.49%)]	 Loss: 0.000024
Training epoch 72 [21760/67107 (32.44%)]	 Loss: 0.000003
Training epoch 72 [22400/67107 (33.40%)]	 Loss: 0.000003
Training epoch 72 [23040/67107 (34.35%)]	 Loss: 0.000001
Training epoch 72 [23680/67107 (35.31%)]	 Loss: 0.000012
Training epoch 72 [24320/67107 (36.26%)]	 Loss: 0.000019
Training epoch 72 [24960/67107 (37.21%)]	 Loss: 0.000002
Training epoch 72 [25600/67107 (38.17%)]	 Loss: 0.000001
Training epoch 72 [26240/67107 (39.12%)]	 Loss: 0.000009
Training epoch 72 [26880/67107 (40.08%)]	 Loss: 0.000004
Training epoch 72 [27520/67107 (41.03%)]	 Loss: 0.000003
Training epoch 72 [28160/67107 (41.98%)]	 Loss: 0.000002
Training epoch 72 [28800/67107 (42.94%)]	 Loss: 0.000004
Training epoch 72 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 72 [30080/67107 (44.85%)]	 Loss: 0.000010
Training epoch 72 [30720/67107 (45.80%)]	 Loss: 0.000007
Training epoch 72 [31360/67107 (46.76%)]	 Loss: 0.000026
Training epoch 72 [32000/67107 (47.71%)]	 Loss: 0.000010
Training epoch 72 [32640/67107 (48.66%)]	 Loss: 0.000003
Training epoch 72 [33280/67107 (49.62%)]	 Loss: 0.000059
Training epoch 72 [33920/67107 (50.57%)]	 Loss: 0.000003
Training epoch 72 [34560/67107 (51.53%)]	 Loss: 0.000005
Training epoch 72 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 72 [35840/67107 (53.44%)]	 Loss: 0.000005
Training epoch 72 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 72 [37120/67107 (55.34%)]	 Loss: 0.000008
Training epoch 72 [37760/67107 (56.30%)]	 Loss: 0.000001
Training epoch 72 [38400/67107 (57.25%)]	 Loss: 0.000027
Training epoch 72 [39040/67107 (58.21%)]	 Loss: 0.000011
Training epoch 72 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 72 [40320/67107 (60.11%)]	 Loss: 0.000024
Training epoch 72 [40960/67107 (61.07%)]	 Loss: 0.000005
Training epoch 72 [41600/67107 (62.02%)]	 Loss: 0.000006
Training epoch 72 [42240/67107 (62.98%)]	 Loss: 0.000015
Training epoch 72 [42880/67107 (63.93%)]	 Loss: 0.000027
Training epoch 72 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 72 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 72 [44800/67107 (66.79%)]	 Loss: 0.000010
Training epoch 72 [45440/67107 (67.75%)]	 Loss: 0.000005
Training epoch 72 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 72 [46720/67107 (69.66%)]	 Loss: 0.000005
Training epoch 72 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 72 [48000/67107 (71.56%)]	 Loss: 0.000029
Training epoch 72 [48640/67107 (72.52%)]	 Loss: 0.000007
Training epoch 72 [49280/67107 (73.47%)]	 Loss: 0.000012
Training epoch 72 [49920/67107 (74.43%)]	 Loss: 0.000041
Training epoch 72 [50560/67107 (75.38%)]	 Loss: 0.000004
Training epoch 72 [51200/67107 (76.34%)]	 Loss: 0.000015
Training epoch 72 [51840/67107 (77.29%)]	 Loss: 0.000005
Training epoch 72 [52480/67107 (78.24%)]	 Loss: 0.000007
Training epoch 72 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 72 [53760/67107 (80.15%)]	 Loss: 0.000005
Training epoch 72 [54400/67107 (81.11%)]	 Loss: 0.000002
Training epoch 72 [55040/67107 (82.06%)]	 Loss: 0.000013
Training epoch 72 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 72 [56320/67107 (83.97%)]	 Loss: 0.000032
Training epoch 72 [56960/67107 (84.92%)]	 Loss: 0.000009
Training epoch 72 [57600/67107 (85.88%)]	 Loss: 0.000026
Training epoch 72 [58240/67107 (86.83%)]	 Loss: 0.000004
Training epoch 72 [58880/67107 (87.79%)]	 Loss: 0.000003
Training epoch 72 [59520/67107 (88.74%)]	 Loss: 0.000003
Training epoch 72 [60160/67107 (89.69%)]	 Loss: 0.000022
Training epoch 72 [60800/67107 (90.65%)]	 Loss: 0.000005
Training epoch 72 [61440/67107 (91.60%)]	 Loss: 0.000018
Training epoch 72 [62080/67107 (92.56%)]	 Loss: 0.000007
Training epoch 72 [62720/67107 (93.51%)]	 Loss: 0.000009
Training epoch 72 [63360/67107 (94.47%)]	 Loss: 0.000016
Training epoch 72 [64000/67107 (95.42%)]	 Loss: 0.000014
Training epoch 72 [64640/67107 (96.37%)]	 Loss: 0.000016
Training epoch 72 [65280/67107 (97.33%)]	 Loss: 0.000005
Training epoch 72 [65920/67107 (98.28%)]	 Loss: 0.000006
Training epoch 72 [66560/67107 (99.24%)]	 Loss: 0.000013
Test set: Average Loss: 0.000413
Training epoch 73 [0/67107 (0.00%)]	 Loss: 0.000040
Training epoch 73 [640/67107 (0.95%)]	 Loss: 0.000016
Training epoch 73 [1280/67107 (1.91%)]	 Loss: 0.000009
Training epoch 73 [1920/67107 (2.86%)]	 Loss: 0.000003
Training epoch 73 [2560/67107 (3.82%)]	 Loss: 0.000008
Training epoch 73 [3200/67107 (4.77%)]	 Loss: 0.000014
Training epoch 73 [3840/67107 (5.73%)]	 Loss: 0.000072
Training epoch 73 [4480/67107 (6.68%)]	 Loss: 0.000021
Training epoch 73 [5120/67107 (7.63%)]	 Loss: 0.000015
Training epoch 73 [5760/67107 (8.59%)]	 Loss: 0.000007
Training epoch 73 [6400/67107 (9.54%)]	 Loss: 0.000014
Training epoch 73 [7040/67107 (10.50%)]	 Loss: 0.000006
Training epoch 73 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 73 [8320/67107 (12.40%)]	 Loss: 0.000015
Training epoch 73 [8960/67107 (13.36%)]	 Loss: 0.000030
Training epoch 73 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 73 [10240/67107 (15.27%)]	 Loss: 0.000022
Training epoch 73 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 73 [11520/67107 (17.18%)]	 Loss: 0.000012
Training epoch 73 [12160/67107 (18.13%)]	 Loss: 0.000009
Training epoch 73 [12800/67107 (19.08%)]	 Loss: 0.000078
Training epoch 73 [13440/67107 (20.04%)]	 Loss: 0.000014
Training epoch 73 [14080/67107 (20.99%)]	 Loss: 0.000010
Training epoch 73 [14720/67107 (21.95%)]	 Loss: 0.000007
Training epoch 73 [15360/67107 (22.90%)]	 Loss: 0.000004
Training epoch 73 [16000/67107 (23.85%)]	 Loss: 0.000011
Training epoch 73 [16640/67107 (24.81%)]	 Loss: 0.000012
Training epoch 73 [17280/67107 (25.76%)]	 Loss: 0.000011
Training epoch 73 [17920/67107 (26.72%)]	 Loss: 0.000017
Training epoch 73 [18560/67107 (27.67%)]	 Loss: 0.000029
Training epoch 73 [19200/67107 (28.63%)]	 Loss: 0.000022
Training epoch 73 [19840/67107 (29.58%)]	 Loss: 0.000003
Training epoch 73 [20480/67107 (30.53%)]	 Loss: 0.000009
Training epoch 73 [21120/67107 (31.49%)]	 Loss: 0.000034
Training epoch 73 [21760/67107 (32.44%)]	 Loss: 0.000054
Training epoch 73 [22400/67107 (33.40%)]	 Loss: 0.000015
Training epoch 73 [23040/67107 (34.35%)]	 Loss: 0.000007
Training epoch 73 [23680/67107 (35.31%)]	 Loss: 0.000010
Training epoch 73 [24320/67107 (36.26%)]	 Loss: 0.000000
Training epoch 73 [24960/67107 (37.21%)]	 Loss: 0.000005
Training epoch 73 [25600/67107 (38.17%)]	 Loss: 0.000043
Training epoch 73 [26240/67107 (39.12%)]	 Loss: 0.000007
Training epoch 73 [26880/67107 (40.08%)]	 Loss: 0.000010
Training epoch 73 [27520/67107 (41.03%)]	 Loss: 0.000017
Training epoch 73 [28160/67107 (41.98%)]	 Loss: 0.000015
Training epoch 73 [28800/67107 (42.94%)]	 Loss: 0.000005
Training epoch 73 [29440/67107 (43.89%)]	 Loss: 0.000007
Training epoch 73 [30080/67107 (44.85%)]	 Loss: 0.000019
Training epoch 73 [30720/67107 (45.80%)]	 Loss: 0.000020
Training epoch 73 [31360/67107 (46.76%)]	 Loss: 0.000050
Training epoch 73 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 73 [32640/67107 (48.66%)]	 Loss: 0.000014
Training epoch 73 [33280/67107 (49.62%)]	 Loss: 0.000007
Training epoch 73 [33920/67107 (50.57%)]	 Loss: 0.000008
Training epoch 73 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 73 [35200/67107 (52.48%)]	 Loss: 0.000007
Training epoch 73 [35840/67107 (53.44%)]	 Loss: 0.000011
Training epoch 73 [36480/67107 (54.39%)]	 Loss: 0.000007
Training epoch 73 [37120/67107 (55.34%)]	 Loss: 0.000003
Training epoch 73 [37760/67107 (56.30%)]	 Loss: 0.000024
Training epoch 73 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 73 [39040/67107 (58.21%)]	 Loss: 0.000004
Training epoch 73 [39680/67107 (59.16%)]	 Loss: 0.000003
Training epoch 73 [40320/67107 (60.11%)]	 Loss: 0.000012
Training epoch 73 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 73 [41600/67107 (62.02%)]	 Loss: 0.000006
Training epoch 73 [42240/67107 (62.98%)]	 Loss: 0.000021
Training epoch 73 [42880/67107 (63.93%)]	 Loss: 0.000008
Training epoch 73 [43520/67107 (64.89%)]	 Loss: 0.000041
Training epoch 73 [44160/67107 (65.84%)]	 Loss: 0.000017
Training epoch 73 [44800/67107 (66.79%)]	 Loss: 0.000017
Training epoch 73 [45440/67107 (67.75%)]	 Loss: 0.000018
Training epoch 73 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 73 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 73 [47360/67107 (70.61%)]	 Loss: 0.000025
Training epoch 73 [48000/67107 (71.56%)]	 Loss: 0.000005
Training epoch 73 [48640/67107 (72.52%)]	 Loss: 0.000010
Training epoch 73 [49280/67107 (73.47%)]	 Loss: 0.000008
Training epoch 73 [49920/67107 (74.43%)]	 Loss: 0.000022
Training epoch 73 [50560/67107 (75.38%)]	 Loss: 0.000016
Training epoch 73 [51200/67107 (76.34%)]	 Loss: 0.000030
Training epoch 73 [51840/67107 (77.29%)]	 Loss: 0.000009
Training epoch 73 [52480/67107 (78.24%)]	 Loss: 0.000006
Training epoch 73 [53120/67107 (79.20%)]	 Loss: 0.000001
Training epoch 73 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 73 [54400/67107 (81.11%)]	 Loss: 0.000004
Training epoch 73 [55040/67107 (82.06%)]	 Loss: 0.000002
Training epoch 73 [55680/67107 (83.02%)]	 Loss: 0.000020
Training epoch 73 [56320/67107 (83.97%)]	 Loss: 0.000002
Training epoch 73 [56960/67107 (84.92%)]	 Loss: 0.000033
Training epoch 73 [57600/67107 (85.88%)]	 Loss: 0.000009
Training epoch 73 [58240/67107 (86.83%)]	 Loss: 0.000009
Training epoch 73 [58880/67107 (87.79%)]	 Loss: 0.000009
Training epoch 73 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 73 [60160/67107 (89.69%)]	 Loss: 0.000002
Training epoch 73 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 73 [61440/67107 (91.60%)]	 Loss: 0.000002
Training epoch 73 [62080/67107 (92.56%)]	 Loss: 0.000015
Training epoch 73 [62720/67107 (93.51%)]	 Loss: 0.000013
Training epoch 73 [63360/67107 (94.47%)]	 Loss: 0.000009
Training epoch 73 [64000/67107 (95.42%)]	 Loss: 0.000005
Training epoch 73 [64640/67107 (96.37%)]	 Loss: 0.000020
Training epoch 73 [65280/67107 (97.33%)]	 Loss: 0.000002
Training epoch 73 [65920/67107 (98.28%)]	 Loss: 0.000020
Training epoch 73 [66560/67107 (99.24%)]	 Loss: 0.000004
Test set: Average Loss: 0.000138
Training epoch 74 [0/67107 (0.00%)]	 Loss: 0.000004
Training epoch 74 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 74 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 74 [1920/67107 (2.86%)]	 Loss: 0.000006
Training epoch 74 [2560/67107 (3.82%)]	 Loss: 0.000010
Training epoch 74 [3200/67107 (4.77%)]	 Loss: 0.000010
Training epoch 74 [3840/67107 (5.73%)]	 Loss: 0.000014
Training epoch 74 [4480/67107 (6.68%)]	 Loss: 0.000006
Training epoch 74 [5120/67107 (7.63%)]	 Loss: 0.000020
Training epoch 74 [5760/67107 (8.59%)]	 Loss: 0.000002
Training epoch 74 [6400/67107 (9.54%)]	 Loss: 0.000002
Training epoch 74 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 74 [7680/67107 (11.45%)]	 Loss: 0.000010
Training epoch 74 [8320/67107 (12.40%)]	 Loss: 0.000024
Training epoch 74 [8960/67107 (13.36%)]	 Loss: 0.000014
Training epoch 74 [9600/67107 (14.31%)]	 Loss: 0.000010
Training epoch 74 [10240/67107 (15.27%)]	 Loss: 0.000008
Training epoch 74 [10880/67107 (16.22%)]	 Loss: 0.000016
Training epoch 74 [11520/67107 (17.18%)]	 Loss: 0.000004
Training epoch 74 [12160/67107 (18.13%)]	 Loss: 0.000006
Training epoch 74 [12800/67107 (19.08%)]	 Loss: 0.000013
Training epoch 74 [13440/67107 (20.04%)]	 Loss: 0.000032
Training epoch 74 [14080/67107 (20.99%)]	 Loss: 0.000022
Training epoch 74 [14720/67107 (21.95%)]	 Loss: 0.000020
Training epoch 74 [15360/67107 (22.90%)]	 Loss: 0.000005
Training epoch 74 [16000/67107 (23.85%)]	 Loss: 0.000015
Training epoch 74 [16640/67107 (24.81%)]	 Loss: 0.000008
Training epoch 74 [17280/67107 (25.76%)]	 Loss: 0.000004
Training epoch 74 [17920/67107 (26.72%)]	 Loss: 0.000002
Training epoch 74 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 74 [19200/67107 (28.63%)]	 Loss: 0.000011
Training epoch 74 [19840/67107 (29.58%)]	 Loss: 0.000005
Training epoch 74 [20480/67107 (30.53%)]	 Loss: 0.000004
Training epoch 74 [21120/67107 (31.49%)]	 Loss: 0.000001
Training epoch 74 [21760/67107 (32.44%)]	 Loss: 0.000009
Training epoch 74 [22400/67107 (33.40%)]	 Loss: 0.000018
Training epoch 74 [23040/67107 (34.35%)]	 Loss: 0.000043
Training epoch 74 [23680/67107 (35.31%)]	 Loss: 0.000001
Training epoch 74 [24320/67107 (36.26%)]	 Loss: 0.000002
Training epoch 74 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 74 [25600/67107 (38.17%)]	 Loss: 0.000002
Training epoch 74 [26240/67107 (39.12%)]	 Loss: 0.000008
Training epoch 74 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 74 [27520/67107 (41.03%)]	 Loss: 0.000005
Training epoch 74 [28160/67107 (41.98%)]	 Loss: 0.000023
Training epoch 74 [28800/67107 (42.94%)]	 Loss: 0.000005
Training epoch 74 [29440/67107 (43.89%)]	 Loss: 0.000017
Training epoch 74 [30080/67107 (44.85%)]	 Loss: 0.000003
Training epoch 74 [30720/67107 (45.80%)]	 Loss: 0.000017
Training epoch 74 [31360/67107 (46.76%)]	 Loss: 0.000003
Training epoch 74 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 74 [32640/67107 (48.66%)]	 Loss: 0.000014
Training epoch 74 [33280/67107 (49.62%)]	 Loss: 0.000003
Training epoch 74 [33920/67107 (50.57%)]	 Loss: 0.000009
Training epoch 74 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 74 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 74 [35840/67107 (53.44%)]	 Loss: 0.000028
Training epoch 74 [36480/67107 (54.39%)]	 Loss: 0.000018
Training epoch 74 [37120/67107 (55.34%)]	 Loss: 0.000038
Training epoch 74 [37760/67107 (56.30%)]	 Loss: 0.000007
Training epoch 74 [38400/67107 (57.25%)]	 Loss: 0.000006
Training epoch 74 [39040/67107 (58.21%)]	 Loss: 0.000006
Training epoch 74 [39680/67107 (59.16%)]	 Loss: 0.000019
Training epoch 74 [40320/67107 (60.11%)]	 Loss: 0.000002
Training epoch 74 [40960/67107 (61.07%)]	 Loss: 0.000005
Training epoch 74 [41600/67107 (62.02%)]	 Loss: 0.000017
Training epoch 74 [42240/67107 (62.98%)]	 Loss: 0.000013
Training epoch 74 [42880/67107 (63.93%)]	 Loss: 0.000019
Training epoch 74 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 74 [44160/67107 (65.84%)]	 Loss: 0.000018
Training epoch 74 [44800/67107 (66.79%)]	 Loss: 0.000051
Training epoch 74 [45440/67107 (67.75%)]	 Loss: 0.000010
Training epoch 74 [46080/67107 (68.70%)]	 Loss: 0.000003
Training epoch 74 [46720/67107 (69.66%)]	 Loss: 0.000001
Training epoch 74 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 74 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 74 [48640/67107 (72.52%)]	 Loss: 0.000015
Training epoch 74 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 74 [49920/67107 (74.43%)]	 Loss: 0.000014
Training epoch 74 [50560/67107 (75.38%)]	 Loss: 0.000007
Training epoch 74 [51200/67107 (76.34%)]	 Loss: 0.000022
Training epoch 74 [51840/67107 (77.29%)]	 Loss: 0.000018
Training epoch 74 [52480/67107 (78.24%)]	 Loss: 0.000052
Training epoch 74 [53120/67107 (79.20%)]	 Loss: 0.000003
Training epoch 74 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 74 [54400/67107 (81.11%)]	 Loss: 0.000003
Training epoch 74 [55040/67107 (82.06%)]	 Loss: 0.000008
Training epoch 74 [55680/67107 (83.02%)]	 Loss: 0.000001
Training epoch 74 [56320/67107 (83.97%)]	 Loss: 0.000002
Training epoch 74 [56960/67107 (84.92%)]	 Loss: 0.000005
Training epoch 74 [57600/67107 (85.88%)]	 Loss: 0.000002
Training epoch 74 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 74 [58880/67107 (87.79%)]	 Loss: 0.000004
Training epoch 74 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 74 [60160/67107 (89.69%)]	 Loss: 0.000015
Training epoch 74 [60800/67107 (90.65%)]	 Loss: 0.000013
Training epoch 74 [61440/67107 (91.60%)]	 Loss: 0.000007
Training epoch 74 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 74 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 74 [63360/67107 (94.47%)]	 Loss: 0.000001
Training epoch 74 [64000/67107 (95.42%)]	 Loss: 0.000004
Training epoch 74 [64640/67107 (96.37%)]	 Loss: 0.000001
Training epoch 74 [65280/67107 (97.33%)]	 Loss: 0.000027
Training epoch 74 [65920/67107 (98.28%)]	 Loss: 0.000020
Training epoch 74 [66560/67107 (99.24%)]	 Loss: 0.000004
Test set: Average Loss: 0.000199
Training epoch 75 [0/67107 (0.00%)]	 Loss: 0.000014
Training epoch 75 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 75 [1280/67107 (1.91%)]	 Loss: 0.000004
Training epoch 75 [1920/67107 (2.86%)]	 Loss: 0.000008
Training epoch 75 [2560/67107 (3.82%)]	 Loss: 0.000012
Training epoch 75 [3200/67107 (4.77%)]	 Loss: 0.000005
Training epoch 75 [3840/67107 (5.73%)]	 Loss: 0.000007
Training epoch 75 [4480/67107 (6.68%)]	 Loss: 0.000005
Training epoch 75 [5120/67107 (7.63%)]	 Loss: 0.000011
Training epoch 75 [5760/67107 (8.59%)]	 Loss: 0.000007
Training epoch 75 [6400/67107 (9.54%)]	 Loss: 0.000013
Training epoch 75 [7040/67107 (10.50%)]	 Loss: 0.000006
Training epoch 75 [7680/67107 (11.45%)]	 Loss: 0.000002
Training epoch 75 [8320/67107 (12.40%)]	 Loss: 0.000024
Training epoch 75 [8960/67107 (13.36%)]	 Loss: 0.000005
Training epoch 75 [9600/67107 (14.31%)]	 Loss: 0.000004
Training epoch 75 [10240/67107 (15.27%)]	 Loss: 0.000007
Training epoch 75 [10880/67107 (16.22%)]	 Loss: 0.000006
Training epoch 75 [11520/67107 (17.18%)]	 Loss: 0.000018
Training epoch 75 [12160/67107 (18.13%)]	 Loss: 0.000007
Training epoch 75 [12800/67107 (19.08%)]	 Loss: 0.000009
Training epoch 75 [13440/67107 (20.04%)]	 Loss: 0.000016
Training epoch 75 [14080/67107 (20.99%)]	 Loss: 0.000003
Training epoch 75 [14720/67107 (21.95%)]	 Loss: 0.000008
Training epoch 75 [15360/67107 (22.90%)]	 Loss: 0.000015
Training epoch 75 [16000/67107 (23.85%)]	 Loss: 0.000005
Training epoch 75 [16640/67107 (24.81%)]	 Loss: 0.000024
Training epoch 75 [17280/67107 (25.76%)]	 Loss: 0.000045
Training epoch 75 [17920/67107 (26.72%)]	 Loss: 0.000017
Training epoch 75 [18560/67107 (27.67%)]	 Loss: 0.000024
Training epoch 75 [19200/67107 (28.63%)]	 Loss: 0.000015
Training epoch 75 [19840/67107 (29.58%)]	 Loss: 0.000027
Training epoch 75 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 75 [21120/67107 (31.49%)]	 Loss: 0.000015
Training epoch 75 [21760/67107 (32.44%)]	 Loss: 0.000013
Training epoch 75 [22400/67107 (33.40%)]	 Loss: 0.000007
Training epoch 75 [23040/67107 (34.35%)]	 Loss: 0.000010
Training epoch 75 [23680/67107 (35.31%)]	 Loss: 0.000005
Training epoch 75 [24320/67107 (36.26%)]	 Loss: 0.000043
Training epoch 75 [24960/67107 (37.21%)]	 Loss: 0.000009
Training epoch 75 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 75 [26240/67107 (39.12%)]	 Loss: 0.000031
Training epoch 75 [26880/67107 (40.08%)]	 Loss: 0.000010
Training epoch 75 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 75 [28160/67107 (41.98%)]	 Loss: 0.000015
Training epoch 75 [28800/67107 (42.94%)]	 Loss: 0.000006
Training epoch 75 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 75 [30080/67107 (44.85%)]	 Loss: 0.000002
Training epoch 75 [30720/67107 (45.80%)]	 Loss: 0.000005
Training epoch 75 [31360/67107 (46.76%)]	 Loss: 0.000001
Training epoch 75 [32000/67107 (47.71%)]	 Loss: 0.000010
Training epoch 75 [32640/67107 (48.66%)]	 Loss: 0.000007
Training epoch 75 [33280/67107 (49.62%)]	 Loss: 0.000005
Training epoch 75 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 75 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 75 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 75 [35840/67107 (53.44%)]	 Loss: 0.000005
Training epoch 75 [36480/67107 (54.39%)]	 Loss: 0.000012
Training epoch 75 [37120/67107 (55.34%)]	 Loss: 0.000002
Training epoch 75 [37760/67107 (56.30%)]	 Loss: 0.000005
Training epoch 75 [38400/67107 (57.25%)]	 Loss: 0.000002
Training epoch 75 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 75 [39680/67107 (59.16%)]	 Loss: 0.000009
Training epoch 75 [40320/67107 (60.11%)]	 Loss: 0.000012
Training epoch 75 [40960/67107 (61.07%)]	 Loss: 0.000010
Training epoch 75 [41600/67107 (62.02%)]	 Loss: 0.000007
Training epoch 75 [42240/67107 (62.98%)]	 Loss: 0.000018
Training epoch 75 [42880/67107 (63.93%)]	 Loss: 0.000010
Training epoch 75 [43520/67107 (64.89%)]	 Loss: 0.000013
Training epoch 75 [44160/67107 (65.84%)]	 Loss: 0.000003
Training epoch 75 [44800/67107 (66.79%)]	 Loss: 0.000069
Training epoch 75 [45440/67107 (67.75%)]	 Loss: 0.000010
Training epoch 75 [46080/67107 (68.70%)]	 Loss: 0.000014
Training epoch 75 [46720/67107 (69.66%)]	 Loss: 0.000060
Training epoch 75 [47360/67107 (70.61%)]	 Loss: 0.000027
Training epoch 75 [48000/67107 (71.56%)]	 Loss: 0.000009
Training epoch 75 [48640/67107 (72.52%)]	 Loss: 0.000060
Training epoch 75 [49280/67107 (73.47%)]	 Loss: 0.000125
Training epoch 75 [49920/67107 (74.43%)]	 Loss: 0.000012
Training epoch 75 [50560/67107 (75.38%)]	 Loss: 0.000018
Training epoch 75 [51200/67107 (76.34%)]	 Loss: 0.000004
Training epoch 75 [51840/67107 (77.29%)]	 Loss: 0.000007
Training epoch 75 [52480/67107 (78.24%)]	 Loss: 0.000020
Training epoch 75 [53120/67107 (79.20%)]	 Loss: 0.000018
Training epoch 75 [53760/67107 (80.15%)]	 Loss: 0.000010
Training epoch 75 [54400/67107 (81.11%)]	 Loss: 0.000010
Training epoch 75 [55040/67107 (82.06%)]	 Loss: 0.000015
Training epoch 75 [55680/67107 (83.02%)]	 Loss: 0.000033
Training epoch 75 [56320/67107 (83.97%)]	 Loss: 0.000006
Training epoch 75 [56960/67107 (84.92%)]	 Loss: 0.000003
Training epoch 75 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 75 [58240/67107 (86.83%)]	 Loss: 0.000013
Training epoch 75 [58880/67107 (87.79%)]	 Loss: 0.000007
Training epoch 75 [59520/67107 (88.74%)]	 Loss: 0.000007
Training epoch 75 [60160/67107 (89.69%)]	 Loss: 0.000002
Training epoch 75 [60800/67107 (90.65%)]	 Loss: 0.000027
Training epoch 75 [61440/67107 (91.60%)]	 Loss: 0.000031
Training epoch 75 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 75 [62720/67107 (93.51%)]	 Loss: 0.000023
Training epoch 75 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 75 [64000/67107 (95.42%)]	 Loss: 0.000009
Training epoch 75 [64640/67107 (96.37%)]	 Loss: 0.000023
Training epoch 75 [65280/67107 (97.33%)]	 Loss: 0.000034
Training epoch 75 [65920/67107 (98.28%)]	 Loss: 0.000011
Training epoch 75 [66560/67107 (99.24%)]	 Loss: 0.000012
Test set: Average Loss: 0.000184
Training epoch 76 [0/67107 (0.00%)]	 Loss: 0.000001
Training epoch 76 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 76 [1280/67107 (1.91%)]	 Loss: 0.000008
Training epoch 76 [1920/67107 (2.86%)]	 Loss: 0.000023
Training epoch 76 [2560/67107 (3.82%)]	 Loss: 0.000007
Training epoch 76 [3200/67107 (4.77%)]	 Loss: 0.000014
Training epoch 76 [3840/67107 (5.73%)]	 Loss: 0.000003
Training epoch 76 [4480/67107 (6.68%)]	 Loss: 0.000003
Training epoch 76 [5120/67107 (7.63%)]	 Loss: 0.000007
Training epoch 76 [5760/67107 (8.59%)]	 Loss: 0.000030
Training epoch 76 [6400/67107 (9.54%)]	 Loss: 0.000007
Training epoch 76 [7040/67107 (10.50%)]	 Loss: 0.000006
Training epoch 76 [7680/67107 (11.45%)]	 Loss: 0.000004
Training epoch 76 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 76 [8960/67107 (13.36%)]	 Loss: 0.000025
Training epoch 76 [9600/67107 (14.31%)]	 Loss: 0.000009
Training epoch 76 [10240/67107 (15.27%)]	 Loss: 0.000024
Training epoch 76 [10880/67107 (16.22%)]	 Loss: 0.000011
Training epoch 76 [11520/67107 (17.18%)]	 Loss: 0.000010
Training epoch 76 [12160/67107 (18.13%)]	 Loss: 0.000030
Training epoch 76 [12800/67107 (19.08%)]	 Loss: 0.000010
Training epoch 76 [13440/67107 (20.04%)]	 Loss: 0.000007
Training epoch 76 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 76 [14720/67107 (21.95%)]	 Loss: 0.000002
Training epoch 76 [15360/67107 (22.90%)]	 Loss: 0.000004
Training epoch 76 [16000/67107 (23.85%)]	 Loss: 0.000056
Training epoch 76 [16640/67107 (24.81%)]	 Loss: 0.000013
Training epoch 76 [17280/67107 (25.76%)]	 Loss: 0.000007
Training epoch 76 [17920/67107 (26.72%)]	 Loss: 0.000005
Training epoch 76 [18560/67107 (27.67%)]	 Loss: 0.000012
Training epoch 76 [19200/67107 (28.63%)]	 Loss: 0.000007
Training epoch 76 [19840/67107 (29.58%)]	 Loss: 0.000014
Training epoch 76 [20480/67107 (30.53%)]	 Loss: 0.000016
Training epoch 76 [21120/67107 (31.49%)]	 Loss: 0.000011
Training epoch 76 [21760/67107 (32.44%)]	 Loss: 0.000007
Training epoch 76 [22400/67107 (33.40%)]	 Loss: 0.000007
Training epoch 76 [23040/67107 (34.35%)]	 Loss: 0.000001
Training epoch 76 [23680/67107 (35.31%)]	 Loss: 0.000015
Training epoch 76 [24320/67107 (36.26%)]	 Loss: 0.000006
Training epoch 76 [24960/67107 (37.21%)]	 Loss: 0.000004
Training epoch 76 [25600/67107 (38.17%)]	 Loss: 0.000014
Training epoch 76 [26240/67107 (39.12%)]	 Loss: 0.000002
Training epoch 76 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 76 [27520/67107 (41.03%)]	 Loss: 0.000009
Training epoch 76 [28160/67107 (41.98%)]	 Loss: 0.000029
Training epoch 76 [28800/67107 (42.94%)]	 Loss: 0.000006
Training epoch 76 [29440/67107 (43.89%)]	 Loss: 0.000004
Training epoch 76 [30080/67107 (44.85%)]	 Loss: 0.000001
Training epoch 76 [30720/67107 (45.80%)]	 Loss: 0.000001
Training epoch 76 [31360/67107 (46.76%)]	 Loss: 0.000003
Training epoch 76 [32000/67107 (47.71%)]	 Loss: 0.000033
Training epoch 76 [32640/67107 (48.66%)]	 Loss: 0.000005
Training epoch 76 [33280/67107 (49.62%)]	 Loss: 0.000022
Training epoch 76 [33920/67107 (50.57%)]	 Loss: 0.000007
Training epoch 76 [34560/67107 (51.53%)]	 Loss: 0.000017
Training epoch 76 [35200/67107 (52.48%)]	 Loss: 0.000009
Training epoch 76 [35840/67107 (53.44%)]	 Loss: 0.000005
Training epoch 76 [36480/67107 (54.39%)]	 Loss: 0.000073
Training epoch 76 [37120/67107 (55.34%)]	 Loss: 0.000010
Training epoch 76 [37760/67107 (56.30%)]	 Loss: 0.000021
Training epoch 76 [38400/67107 (57.25%)]	 Loss: 0.000023
Training epoch 76 [39040/67107 (58.21%)]	 Loss: 0.000014
Training epoch 76 [39680/67107 (59.16%)]	 Loss: 0.000012
Training epoch 76 [40320/67107 (60.11%)]	 Loss: 0.000004
Training epoch 76 [40960/67107 (61.07%)]	 Loss: 0.000019
Training epoch 76 [41600/67107 (62.02%)]	 Loss: 0.000004
Training epoch 76 [42240/67107 (62.98%)]	 Loss: 0.000012
Training epoch 76 [42880/67107 (63.93%)]	 Loss: 0.000005
Training epoch 76 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 76 [44160/67107 (65.84%)]	 Loss: 0.000019
Training epoch 76 [44800/67107 (66.79%)]	 Loss: 0.000004
Training epoch 76 [45440/67107 (67.75%)]	 Loss: 0.000003
Training epoch 76 [46080/67107 (68.70%)]	 Loss: 0.000002
Training epoch 76 [46720/67107 (69.66%)]	 Loss: 0.000031
Training epoch 76 [47360/67107 (70.61%)]	 Loss: 0.000013
Training epoch 76 [48000/67107 (71.56%)]	 Loss: 0.000027
Training epoch 76 [48640/67107 (72.52%)]	 Loss: 0.000002
Training epoch 76 [49280/67107 (73.47%)]	 Loss: 0.000004
Training epoch 76 [49920/67107 (74.43%)]	 Loss: 0.000002
Training epoch 76 [50560/67107 (75.38%)]	 Loss: 0.000017
Training epoch 76 [51200/67107 (76.34%)]	 Loss: 0.000014
Training epoch 76 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 76 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 76 [53120/67107 (79.20%)]	 Loss: 0.000004
Training epoch 76 [53760/67107 (80.15%)]	 Loss: 0.000003
Training epoch 76 [54400/67107 (81.11%)]	 Loss: 0.000033
Training epoch 76 [55040/67107 (82.06%)]	 Loss: 0.000018
Training epoch 76 [55680/67107 (83.02%)]	 Loss: 0.000010
Training epoch 76 [56320/67107 (83.97%)]	 Loss: 0.000005
Training epoch 76 [56960/67107 (84.92%)]	 Loss: 0.000004
Training epoch 76 [57600/67107 (85.88%)]	 Loss: 0.000018
Training epoch 76 [58240/67107 (86.83%)]	 Loss: 0.000044
Training epoch 76 [58880/67107 (87.79%)]	 Loss: 0.000019
Training epoch 76 [59520/67107 (88.74%)]	 Loss: 0.000056
Training epoch 76 [60160/67107 (89.69%)]	 Loss: 0.000002
Training epoch 76 [60800/67107 (90.65%)]	 Loss: 0.000006
Training epoch 76 [61440/67107 (91.60%)]	 Loss: 0.000015
Training epoch 76 [62080/67107 (92.56%)]	 Loss: 0.000006
Training epoch 76 [62720/67107 (93.51%)]	 Loss: 0.000029
Training epoch 76 [63360/67107 (94.47%)]	 Loss: 0.000017
Training epoch 76 [64000/67107 (95.42%)]	 Loss: 0.000018
Training epoch 76 [64640/67107 (96.37%)]	 Loss: 0.000001
Training epoch 76 [65280/67107 (97.33%)]	 Loss: 0.000002
Training epoch 76 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 76 [66560/67107 (99.24%)]	 Loss: 0.000003
Test set: Average Loss: 0.000675
Training epoch 77 [0/67107 (0.00%)]	 Loss: 0.000098
Training epoch 77 [640/67107 (0.95%)]	 Loss: 0.000011
Training epoch 77 [1280/67107 (1.91%)]	 Loss: 0.000007
Training epoch 77 [1920/67107 (2.86%)]	 Loss: 0.000032
Training epoch 77 [2560/67107 (3.82%)]	 Loss: 0.000022
Training epoch 77 [3200/67107 (4.77%)]	 Loss: 0.000006
Training epoch 77 [3840/67107 (5.73%)]	 Loss: 0.000006
Training epoch 77 [4480/67107 (6.68%)]	 Loss: 0.000006
Training epoch 77 [5120/67107 (7.63%)]	 Loss: 0.000005
Training epoch 77 [5760/67107 (8.59%)]	 Loss: 0.000035
Training epoch 77 [6400/67107 (9.54%)]	 Loss: 0.000023
Training epoch 77 [7040/67107 (10.50%)]	 Loss: 0.000009
Training epoch 77 [7680/67107 (11.45%)]	 Loss: 0.000004
Training epoch 77 [8320/67107 (12.40%)]	 Loss: 0.000020
Training epoch 77 [8960/67107 (13.36%)]	 Loss: 0.000014
Training epoch 77 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 77 [10240/67107 (15.27%)]	 Loss: 0.000003
Training epoch 77 [10880/67107 (16.22%)]	 Loss: 0.000004
Training epoch 77 [11520/67107 (17.18%)]	 Loss: 0.000002
Training epoch 77 [12160/67107 (18.13%)]	 Loss: 0.000003
Training epoch 77 [12800/67107 (19.08%)]	 Loss: 0.000042
Training epoch 77 [13440/67107 (20.04%)]	 Loss: 0.000004
Training epoch 77 [14080/67107 (20.99%)]	 Loss: 0.000008
Training epoch 77 [14720/67107 (21.95%)]	 Loss: 0.000015
Training epoch 77 [15360/67107 (22.90%)]	 Loss: 0.000006
Training epoch 77 [16000/67107 (23.85%)]	 Loss: 0.000011
Training epoch 77 [16640/67107 (24.81%)]	 Loss: 0.000009
Training epoch 77 [17280/67107 (25.76%)]	 Loss: 0.000005
Training epoch 77 [17920/67107 (26.72%)]	 Loss: 0.000002
Training epoch 77 [18560/67107 (27.67%)]	 Loss: 0.000003
Training epoch 77 [19200/67107 (28.63%)]	 Loss: 0.000010
Training epoch 77 [19840/67107 (29.58%)]	 Loss: 0.000024
Training epoch 77 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 77 [21120/67107 (31.49%)]	 Loss: 0.000009
Training epoch 77 [21760/67107 (32.44%)]	 Loss: 0.000005
Training epoch 77 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 77 [23040/67107 (34.35%)]	 Loss: 0.000026
Training epoch 77 [23680/67107 (35.31%)]	 Loss: 0.000016
Training epoch 77 [24320/67107 (36.26%)]	 Loss: 0.000012
Training epoch 77 [24960/67107 (37.21%)]	 Loss: 0.000036
Training epoch 77 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 77 [26240/67107 (39.12%)]	 Loss: 0.000014
Training epoch 77 [26880/67107 (40.08%)]	 Loss: 0.000034
Training epoch 77 [27520/67107 (41.03%)]	 Loss: 0.000005
Training epoch 77 [28160/67107 (41.98%)]	 Loss: 0.000045
Training epoch 77 [28800/67107 (42.94%)]	 Loss: 0.000003
Training epoch 77 [29440/67107 (43.89%)]	 Loss: 0.000009
Training epoch 77 [30080/67107 (44.85%)]	 Loss: 0.000003
Training epoch 77 [30720/67107 (45.80%)]	 Loss: 0.000003
Training epoch 77 [31360/67107 (46.76%)]	 Loss: 0.000001
Training epoch 77 [32000/67107 (47.71%)]	 Loss: 0.000002
Training epoch 77 [32640/67107 (48.66%)]	 Loss: 0.000002
Training epoch 77 [33280/67107 (49.62%)]	 Loss: 0.000017
Training epoch 77 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 77 [34560/67107 (51.53%)]	 Loss: 0.000017
Training epoch 77 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 77 [35840/67107 (53.44%)]	 Loss: 0.000010
Training epoch 77 [36480/67107 (54.39%)]	 Loss: 0.000003
Training epoch 77 [37120/67107 (55.34%)]	 Loss: 0.000016
Training epoch 77 [37760/67107 (56.30%)]	 Loss: 0.000020
Training epoch 77 [38400/67107 (57.25%)]	 Loss: 0.000001
Training epoch 77 [39040/67107 (58.21%)]	 Loss: 0.000007
Training epoch 77 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 77 [40320/67107 (60.11%)]	 Loss: 0.000024
Training epoch 77 [40960/67107 (61.07%)]	 Loss: 0.000011
Training epoch 77 [41600/67107 (62.02%)]	 Loss: 0.000011
Training epoch 77 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 77 [42880/67107 (63.93%)]	 Loss: 0.000008
Training epoch 77 [43520/67107 (64.89%)]	 Loss: 0.000007
Training epoch 77 [44160/67107 (65.84%)]	 Loss: 0.000010
Training epoch 77 [44800/67107 (66.79%)]	 Loss: 0.000007
Training epoch 77 [45440/67107 (67.75%)]	 Loss: 0.000002
Training epoch 77 [46080/67107 (68.70%)]	 Loss: 0.000000
Training epoch 77 [46720/67107 (69.66%)]	 Loss: 0.000025
Training epoch 77 [47360/67107 (70.61%)]	 Loss: 0.000013
Training epoch 77 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 77 [48640/67107 (72.52%)]	 Loss: 0.000011
Training epoch 77 [49280/67107 (73.47%)]	 Loss: 0.000010
Training epoch 77 [49920/67107 (74.43%)]	 Loss: 0.000008
Training epoch 77 [50560/67107 (75.38%)]	 Loss: 0.000019
Training epoch 77 [51200/67107 (76.34%)]	 Loss: 0.000003
Training epoch 77 [51840/67107 (77.29%)]	 Loss: 0.000003
Training epoch 77 [52480/67107 (78.24%)]	 Loss: 0.000002
Training epoch 77 [53120/67107 (79.20%)]	 Loss: 0.000015
Training epoch 77 [53760/67107 (80.15%)]	 Loss: 0.000005
Training epoch 77 [54400/67107 (81.11%)]	 Loss: 0.000031
Training epoch 77 [55040/67107 (82.06%)]	 Loss: 0.000028
Training epoch 77 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 77 [56320/67107 (83.97%)]	 Loss: 0.000005
Training epoch 77 [56960/67107 (84.92%)]	 Loss: 0.000003
Training epoch 77 [57600/67107 (85.88%)]	 Loss: 0.000015
Training epoch 77 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 77 [58880/67107 (87.79%)]	 Loss: 0.000001
Training epoch 77 [59520/67107 (88.74%)]	 Loss: 0.000002
Training epoch 77 [60160/67107 (89.69%)]	 Loss: 0.000003
Training epoch 77 [60800/67107 (90.65%)]	 Loss: 0.000004
Training epoch 77 [61440/67107 (91.60%)]	 Loss: 0.000008
Training epoch 77 [62080/67107 (92.56%)]	 Loss: 0.000009
Training epoch 77 [62720/67107 (93.51%)]	 Loss: 0.000007
Training epoch 77 [63360/67107 (94.47%)]	 Loss: 0.000004
Training epoch 77 [64000/67107 (95.42%)]	 Loss: 0.000019
Training epoch 77 [64640/67107 (96.37%)]	 Loss: 0.000008
Training epoch 77 [65280/67107 (97.33%)]	 Loss: 0.000002
Training epoch 77 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 77 [66560/67107 (99.24%)]	 Loss: 0.000005
Test set: Average Loss: 0.001667
Training epoch 78 [0/67107 (0.00%)]	 Loss: 0.000044
Training epoch 78 [640/67107 (0.95%)]	 Loss: 0.000011
Training epoch 78 [1280/67107 (1.91%)]	 Loss: 0.000005
Training epoch 78 [1920/67107 (2.86%)]	 Loss: 0.000026
Training epoch 78 [2560/67107 (3.82%)]	 Loss: 0.000006
Training epoch 78 [3200/67107 (4.77%)]	 Loss: 0.000003
Training epoch 78 [3840/67107 (5.73%)]	 Loss: 0.000003
Training epoch 78 [4480/67107 (6.68%)]	 Loss: 0.000010
Training epoch 78 [5120/67107 (7.63%)]	 Loss: 0.000013
Training epoch 78 [5760/67107 (8.59%)]	 Loss: 0.000009
Training epoch 78 [6400/67107 (9.54%)]	 Loss: 0.000008
Training epoch 78 [7040/67107 (10.50%)]	 Loss: 0.000028
Training epoch 78 [7680/67107 (11.45%)]	 Loss: 0.000012
Training epoch 78 [8320/67107 (12.40%)]	 Loss: 0.000004
Training epoch 78 [8960/67107 (13.36%)]	 Loss: 0.000016
Training epoch 78 [9600/67107 (14.31%)]	 Loss: 0.000001
Training epoch 78 [10240/67107 (15.27%)]	 Loss: 0.000013
Training epoch 78 [10880/67107 (16.22%)]	 Loss: 0.000065
Training epoch 78 [11520/67107 (17.18%)]	 Loss: 0.000003
Training epoch 78 [12160/67107 (18.13%)]	 Loss: 0.000016
Training epoch 78 [12800/67107 (19.08%)]	 Loss: 0.000018
Training epoch 78 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 78 [14080/67107 (20.99%)]	 Loss: 0.000013
Training epoch 78 [14720/67107 (21.95%)]	 Loss: 0.000009
Training epoch 78 [15360/67107 (22.90%)]	 Loss: 0.000034
Training epoch 78 [16000/67107 (23.85%)]	 Loss: 0.000009
Training epoch 78 [16640/67107 (24.81%)]	 Loss: 0.000016
Training epoch 78 [17280/67107 (25.76%)]	 Loss: 0.000006
Training epoch 78 [17920/67107 (26.72%)]	 Loss: 0.000012
Training epoch 78 [18560/67107 (27.67%)]	 Loss: 0.000027
Training epoch 78 [19200/67107 (28.63%)]	 Loss: 0.000020
Training epoch 78 [19840/67107 (29.58%)]	 Loss: 0.000013
Training epoch 78 [20480/67107 (30.53%)]	 Loss: 0.000002
Training epoch 78 [21120/67107 (31.49%)]	 Loss: 0.000009
Training epoch 78 [21760/67107 (32.44%)]	 Loss: 0.000017
Training epoch 78 [22400/67107 (33.40%)]	 Loss: 0.000008
Training epoch 78 [23040/67107 (34.35%)]	 Loss: 0.000006
Training epoch 78 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 78 [24320/67107 (36.26%)]	 Loss: 0.000002
Training epoch 78 [24960/67107 (37.21%)]	 Loss: 0.000021
Training epoch 78 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 78 [26240/67107 (39.12%)]	 Loss: 0.000007
Training epoch 78 [26880/67107 (40.08%)]	 Loss: 0.000009
Training epoch 78 [27520/67107 (41.03%)]	 Loss: 0.000004
Training epoch 78 [28160/67107 (41.98%)]	 Loss: 0.000002
Training epoch 78 [28800/67107 (42.94%)]	 Loss: 0.000037
Training epoch 78 [29440/67107 (43.89%)]	 Loss: 0.000007
Training epoch 78 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 78 [30720/67107 (45.80%)]	 Loss: 0.000001
Training epoch 78 [31360/67107 (46.76%)]	 Loss: 0.000005
Training epoch 78 [32000/67107 (47.71%)]	 Loss: 0.000023
Training epoch 78 [32640/67107 (48.66%)]	 Loss: 0.000004
Training epoch 78 [33280/67107 (49.62%)]	 Loss: 0.000004
Training epoch 78 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 78 [34560/67107 (51.53%)]	 Loss: 0.000007
Training epoch 78 [35200/67107 (52.48%)]	 Loss: 0.000025
Training epoch 78 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 78 [36480/67107 (54.39%)]	 Loss: 0.000013
Training epoch 78 [37120/67107 (55.34%)]	 Loss: 0.000002
Training epoch 78 [37760/67107 (56.30%)]	 Loss: 0.000017
Training epoch 78 [38400/67107 (57.25%)]	 Loss: 0.000003
Training epoch 78 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 78 [39680/67107 (59.16%)]	 Loss: 0.000010
Training epoch 78 [40320/67107 (60.11%)]	 Loss: 0.000005
Training epoch 78 [40960/67107 (61.07%)]	 Loss: 0.000005
Training epoch 78 [41600/67107 (62.02%)]	 Loss: 0.000003
Training epoch 78 [42240/67107 (62.98%)]	 Loss: 0.000005
Training epoch 78 [42880/67107 (63.93%)]	 Loss: 0.000026
Training epoch 78 [43520/67107 (64.89%)]	 Loss: 0.000025
Training epoch 78 [44160/67107 (65.84%)]	 Loss: 0.000015
Training epoch 78 [44800/67107 (66.79%)]	 Loss: 0.000026
Training epoch 78 [45440/67107 (67.75%)]	 Loss: 0.000016
Training epoch 78 [46080/67107 (68.70%)]	 Loss: 0.000005
Training epoch 78 [46720/67107 (69.66%)]	 Loss: 0.000005
Training epoch 78 [47360/67107 (70.61%)]	 Loss: 0.000023
Training epoch 78 [48000/67107 (71.56%)]	 Loss: 0.000005
Training epoch 78 [48640/67107 (72.52%)]	 Loss: 0.000004
Training epoch 78 [49280/67107 (73.47%)]	 Loss: 0.000005
Training epoch 78 [49920/67107 (74.43%)]	 Loss: 0.000002
Training epoch 78 [50560/67107 (75.38%)]	 Loss: 0.000017
Training epoch 78 [51200/67107 (76.34%)]	 Loss: 0.000003
Training epoch 78 [51840/67107 (77.29%)]	 Loss: 0.000017
Training epoch 78 [52480/67107 (78.24%)]	 Loss: 0.000001
Training epoch 78 [53120/67107 (79.20%)]	 Loss: 0.000002
Training epoch 78 [53760/67107 (80.15%)]	 Loss: 0.000007
Training epoch 78 [54400/67107 (81.11%)]	 Loss: 0.000002
Training epoch 78 [55040/67107 (82.06%)]	 Loss: 0.000003
Training epoch 78 [55680/67107 (83.02%)]	 Loss: 0.000005
Training epoch 78 [56320/67107 (83.97%)]	 Loss: 0.000002
Training epoch 78 [56960/67107 (84.92%)]	 Loss: 0.000004
Training epoch 78 [57600/67107 (85.88%)]	 Loss: 0.000012
Training epoch 78 [58240/67107 (86.83%)]	 Loss: 0.000013
Training epoch 78 [58880/67107 (87.79%)]	 Loss: 0.000031
Training epoch 78 [59520/67107 (88.74%)]	 Loss: 0.000004
Training epoch 78 [60160/67107 (89.69%)]	 Loss: 0.000010
Training epoch 78 [60800/67107 (90.65%)]	 Loss: 0.000007
Training epoch 78 [61440/67107 (91.60%)]	 Loss: 0.000013
Training epoch 78 [62080/67107 (92.56%)]	 Loss: 0.000006
Training epoch 78 [62720/67107 (93.51%)]	 Loss: 0.000009
Training epoch 78 [63360/67107 (94.47%)]	 Loss: 0.000003
Training epoch 78 [64000/67107 (95.42%)]	 Loss: 0.000014
Training epoch 78 [64640/67107 (96.37%)]	 Loss: 0.000029
Training epoch 78 [65280/67107 (97.33%)]	 Loss: 0.000009
Training epoch 78 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 78 [66560/67107 (99.24%)]	 Loss: 0.000009
Test set: Average Loss: 0.004201
Training epoch 79 [0/67107 (0.00%)]	 Loss: 0.000003
Training epoch 79 [640/67107 (0.95%)]	 Loss: 0.000010
Training epoch 79 [1280/67107 (1.91%)]	 Loss: 0.000073
Training epoch 79 [1920/67107 (2.86%)]	 Loss: 0.000009
Training epoch 79 [2560/67107 (3.82%)]	 Loss: 0.000004
Training epoch 79 [3200/67107 (4.77%)]	 Loss: 0.000009
Training epoch 79 [3840/67107 (5.73%)]	 Loss: 0.000004
Training epoch 79 [4480/67107 (6.68%)]	 Loss: 0.000046
Training epoch 79 [5120/67107 (7.63%)]	 Loss: 0.000044
Training epoch 79 [5760/67107 (8.59%)]	 Loss: 0.000033
Training epoch 79 [6400/67107 (9.54%)]	 Loss: 0.000003
Training epoch 79 [7040/67107 (10.50%)]	 Loss: 0.000004
Training epoch 79 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 79 [8320/67107 (12.40%)]	 Loss: 0.000007
Training epoch 79 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 79 [9600/67107 (14.31%)]	 Loss: 0.000004
Training epoch 79 [10240/67107 (15.27%)]	 Loss: 0.000009
Training epoch 79 [10880/67107 (16.22%)]	 Loss: 0.000005
Training epoch 79 [11520/67107 (17.18%)]	 Loss: 0.000020
Training epoch 79 [12160/67107 (18.13%)]	 Loss: 0.000009
Training epoch 79 [12800/67107 (19.08%)]	 Loss: 0.000009
Training epoch 79 [13440/67107 (20.04%)]	 Loss: 0.000004
Training epoch 79 [14080/67107 (20.99%)]	 Loss: 0.000003
Training epoch 79 [14720/67107 (21.95%)]	 Loss: 0.000076
Training epoch 79 [15360/67107 (22.90%)]	 Loss: 0.000027
Training epoch 79 [16000/67107 (23.85%)]	 Loss: 0.000033
Training epoch 79 [16640/67107 (24.81%)]	 Loss: 0.000011
Training epoch 79 [17280/67107 (25.76%)]	 Loss: 0.000017
Training epoch 79 [17920/67107 (26.72%)]	 Loss: 0.000007
Training epoch 79 [18560/67107 (27.67%)]	 Loss: 0.000002
Training epoch 79 [19200/67107 (28.63%)]	 Loss: 0.000014
Training epoch 79 [19840/67107 (29.58%)]	 Loss: 0.000011
Training epoch 79 [20480/67107 (30.53%)]	 Loss: 0.000009
Training epoch 79 [21120/67107 (31.49%)]	 Loss: 0.000009
Training epoch 79 [21760/67107 (32.44%)]	 Loss: 0.000016
Training epoch 79 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 79 [23040/67107 (34.35%)]	 Loss: 0.000001
Training epoch 79 [23680/67107 (35.31%)]	 Loss: 0.000003
Training epoch 79 [24320/67107 (36.26%)]	 Loss: 0.000002
Training epoch 79 [24960/67107 (37.21%)]	 Loss: 0.000028
Training epoch 79 [25600/67107 (38.17%)]	 Loss: 0.000017
Training epoch 79 [26240/67107 (39.12%)]	 Loss: 0.000021
Training epoch 79 [26880/67107 (40.08%)]	 Loss: 0.000016
Training epoch 79 [27520/67107 (41.03%)]	 Loss: 0.000011
Training epoch 79 [28160/67107 (41.98%)]	 Loss: 0.000019
Training epoch 79 [28800/67107 (42.94%)]	 Loss: 0.000007
Training epoch 79 [29440/67107 (43.89%)]	 Loss: 0.000004
Training epoch 79 [30080/67107 (44.85%)]	 Loss: 0.000020
Training epoch 79 [30720/67107 (45.80%)]	 Loss: 0.000011
Training epoch 79 [31360/67107 (46.76%)]	 Loss: 0.000013
Training epoch 79 [32000/67107 (47.71%)]	 Loss: 0.000002
Training epoch 79 [32640/67107 (48.66%)]	 Loss: 0.000013
Training epoch 79 [33280/67107 (49.62%)]	 Loss: 0.000062
Training epoch 79 [33920/67107 (50.57%)]	 Loss: 0.000012
Training epoch 79 [34560/67107 (51.53%)]	 Loss: 0.000010
Training epoch 79 [35200/67107 (52.48%)]	 Loss: 0.000003
Training epoch 79 [35840/67107 (53.44%)]	 Loss: 0.000007
Training epoch 79 [36480/67107 (54.39%)]	 Loss: 0.000003
Training epoch 79 [37120/67107 (55.34%)]	 Loss: 0.000003
Training epoch 79 [37760/67107 (56.30%)]	 Loss: 0.000002
Training epoch 79 [38400/67107 (57.25%)]	 Loss: 0.000009
Training epoch 79 [39040/67107 (58.21%)]	 Loss: 0.000021
Training epoch 79 [39680/67107 (59.16%)]	 Loss: 0.000009
Training epoch 79 [40320/67107 (60.11%)]	 Loss: 0.000003
Training epoch 79 [40960/67107 (61.07%)]	 Loss: 0.000015
Training epoch 79 [41600/67107 (62.02%)]	 Loss: 0.000013
Training epoch 79 [42240/67107 (62.98%)]	 Loss: 0.000020
Training epoch 79 [42880/67107 (63.93%)]	 Loss: 0.000011
Training epoch 79 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 79 [44160/67107 (65.84%)]	 Loss: 0.000004
Training epoch 79 [44800/67107 (66.79%)]	 Loss: 0.000002
Training epoch 79 [45440/67107 (67.75%)]	 Loss: 0.000014
Training epoch 79 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 79 [46720/67107 (69.66%)]	 Loss: 0.000004
Training epoch 79 [47360/67107 (70.61%)]	 Loss: 0.000004
Training epoch 79 [48000/67107 (71.56%)]	 Loss: 0.000017
Training epoch 79 [48640/67107 (72.52%)]	 Loss: 0.000005
Training epoch 79 [49280/67107 (73.47%)]	 Loss: 0.000029
Training epoch 79 [49920/67107 (74.43%)]	 Loss: 0.000007
Training epoch 79 [50560/67107 (75.38%)]	 Loss: 0.000038
Training epoch 79 [51200/67107 (76.34%)]	 Loss: 0.000009
Training epoch 79 [51840/67107 (77.29%)]	 Loss: 0.000030
Training epoch 79 [52480/67107 (78.24%)]	 Loss: 0.000004
Training epoch 79 [53120/67107 (79.20%)]	 Loss: 0.000011
Training epoch 79 [53760/67107 (80.15%)]	 Loss: 0.000028
Training epoch 79 [54400/67107 (81.11%)]	 Loss: 0.000004
Training epoch 79 [55040/67107 (82.06%)]	 Loss: 0.000016
Training epoch 79 [55680/67107 (83.02%)]	 Loss: 0.000033
Training epoch 79 [56320/67107 (83.97%)]	 Loss: 0.000015
Training epoch 79 [56960/67107 (84.92%)]	 Loss: 0.000006
Training epoch 79 [57600/67107 (85.88%)]	 Loss: 0.000002
Training epoch 79 [58240/67107 (86.83%)]	 Loss: 0.000002
Training epoch 79 [58880/67107 (87.79%)]	 Loss: 0.000004
Training epoch 79 [59520/67107 (88.74%)]	 Loss: 0.000002
Training epoch 79 [60160/67107 (89.69%)]	 Loss: 0.000018
Training epoch 79 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 79 [61440/67107 (91.60%)]	 Loss: 0.000017
Training epoch 79 [62080/67107 (92.56%)]	 Loss: 0.000023
Training epoch 79 [62720/67107 (93.51%)]	 Loss: 0.000012
Training epoch 79 [63360/67107 (94.47%)]	 Loss: 0.000056
Training epoch 79 [64000/67107 (95.42%)]	 Loss: 0.000009
Training epoch 79 [64640/67107 (96.37%)]	 Loss: 0.000004
Training epoch 79 [65280/67107 (97.33%)]	 Loss: 0.000003
Training epoch 79 [65920/67107 (98.28%)]	 Loss: 0.000014
Training epoch 79 [66560/67107 (99.24%)]	 Loss: 0.000012
Test set: Average Loss: 0.000910
Training epoch 80 [0/67107 (0.00%)]	 Loss: 0.000012
Training epoch 80 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 80 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 80 [1920/67107 (2.86%)]	 Loss: 0.000001
Training epoch 80 [2560/67107 (3.82%)]	 Loss: 0.000004
Training epoch 80 [3200/67107 (4.77%)]	 Loss: 0.000057
Training epoch 80 [3840/67107 (5.73%)]	 Loss: 0.000006
Training epoch 80 [4480/67107 (6.68%)]	 Loss: 0.000003
Training epoch 80 [5120/67107 (7.63%)]	 Loss: 0.000023
Training epoch 80 [5760/67107 (8.59%)]	 Loss: 0.000009
Training epoch 80 [6400/67107 (9.54%)]	 Loss: 0.000010
Training epoch 80 [7040/67107 (10.50%)]	 Loss: 0.000015
Training epoch 80 [7680/67107 (11.45%)]	 Loss: 0.000013
Training epoch 80 [8320/67107 (12.40%)]	 Loss: 0.000038
Training epoch 80 [8960/67107 (13.36%)]	 Loss: 0.000031
Training epoch 80 [9600/67107 (14.31%)]	 Loss: 0.000002
Training epoch 80 [10240/67107 (15.27%)]	 Loss: 0.000008
Training epoch 80 [10880/67107 (16.22%)]	 Loss: 0.000003
Training epoch 80 [11520/67107 (17.18%)]	 Loss: 0.000009
Training epoch 80 [12160/67107 (18.13%)]	 Loss: 0.000006
Training epoch 80 [12800/67107 (19.08%)]	 Loss: 0.000002
Training epoch 80 [13440/67107 (20.04%)]	 Loss: 0.000016
Training epoch 80 [14080/67107 (20.99%)]	 Loss: 0.000006
Training epoch 80 [14720/67107 (21.95%)]	 Loss: 0.000032
Training epoch 80 [15360/67107 (22.90%)]	 Loss: 0.000009
Training epoch 80 [16000/67107 (23.85%)]	 Loss: 0.000005
Training epoch 80 [16640/67107 (24.81%)]	 Loss: 0.000012
Training epoch 80 [17280/67107 (25.76%)]	 Loss: 0.000007
Training epoch 80 [17920/67107 (26.72%)]	 Loss: 0.000025
Training epoch 80 [18560/67107 (27.67%)]	 Loss: 0.000036
Training epoch 80 [19200/67107 (28.63%)]	 Loss: 0.000015
Training epoch 80 [19840/67107 (29.58%)]	 Loss: 0.000004
Training epoch 80 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 80 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 80 [21760/67107 (32.44%)]	 Loss: 0.000004
Training epoch 80 [22400/67107 (33.40%)]	 Loss: 0.000003
Training epoch 80 [23040/67107 (34.35%)]	 Loss: 0.000007
Training epoch 80 [23680/67107 (35.31%)]	 Loss: 0.000011
Training epoch 80 [24320/67107 (36.26%)]	 Loss: 0.000011
Training epoch 80 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 80 [25600/67107 (38.17%)]	 Loss: 0.000002
Training epoch 80 [26240/67107 (39.12%)]	 Loss: 0.000006
Training epoch 80 [26880/67107 (40.08%)]	 Loss: 0.000001
Training epoch 80 [27520/67107 (41.03%)]	 Loss: 0.000010
Training epoch 80 [28160/67107 (41.98%)]	 Loss: 0.000125
Training epoch 80 [28800/67107 (42.94%)]	 Loss: 0.000008
Training epoch 80 [29440/67107 (43.89%)]	 Loss: 0.000016
Training epoch 80 [30080/67107 (44.85%)]	 Loss: 0.000009
Training epoch 80 [30720/67107 (45.80%)]	 Loss: 0.000011
Training epoch 80 [31360/67107 (46.76%)]	 Loss: 0.000008
Training epoch 80 [32000/67107 (47.71%)]	 Loss: 0.000020
Training epoch 80 [32640/67107 (48.66%)]	 Loss: 0.000006
Training epoch 80 [33280/67107 (49.62%)]	 Loss: 0.000007
Training epoch 80 [33920/67107 (50.57%)]	 Loss: 0.000007
Training epoch 80 [34560/67107 (51.53%)]	 Loss: 0.000003
Training epoch 80 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 80 [35840/67107 (53.44%)]	 Loss: 0.000001
Training epoch 80 [36480/67107 (54.39%)]	 Loss: 0.000003
Training epoch 80 [37120/67107 (55.34%)]	 Loss: 0.000002
Training epoch 80 [37760/67107 (56.30%)]	 Loss: 0.000006
Training epoch 80 [38400/67107 (57.25%)]	 Loss: 0.000002
Training epoch 80 [39040/67107 (58.21%)]	 Loss: 0.000008
Training epoch 80 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 80 [40320/67107 (60.11%)]	 Loss: 0.000007
Training epoch 80 [40960/67107 (61.07%)]	 Loss: 0.000022
Training epoch 80 [41600/67107 (62.02%)]	 Loss: 0.000008
Training epoch 80 [42240/67107 (62.98%)]	 Loss: 0.000004
Training epoch 80 [42880/67107 (63.93%)]	 Loss: 0.000001
Training epoch 80 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 80 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 80 [44800/67107 (66.79%)]	 Loss: 0.000017
Training epoch 80 [45440/67107 (67.75%)]	 Loss: 0.000010
Training epoch 80 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 80 [46720/67107 (69.66%)]	 Loss: 0.000007
Training epoch 80 [47360/67107 (70.61%)]	 Loss: 0.000013
Training epoch 80 [48000/67107 (71.56%)]	 Loss: 0.000052
Training epoch 80 [48640/67107 (72.52%)]	 Loss: 0.000027
Training epoch 80 [49280/67107 (73.47%)]	 Loss: 0.000024
Training epoch 80 [49920/67107 (74.43%)]	 Loss: 0.000019
Training epoch 80 [50560/67107 (75.38%)]	 Loss: 0.000007
Training epoch 80 [51200/67107 (76.34%)]	 Loss: 0.000005
Training epoch 80 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 80 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 80 [53120/67107 (79.20%)]	 Loss: 0.000009
Training epoch 80 [53760/67107 (80.15%)]	 Loss: 0.000015
Training epoch 80 [54400/67107 (81.11%)]	 Loss: 0.000003
Training epoch 80 [55040/67107 (82.06%)]	 Loss: 0.000002
Training epoch 80 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 80 [56320/67107 (83.97%)]	 Loss: 0.000019
Training epoch 80 [56960/67107 (84.92%)]	 Loss: 0.000010
Training epoch 80 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 80 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 80 [58880/67107 (87.79%)]	 Loss: 0.000006
Training epoch 80 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 80 [60160/67107 (89.69%)]	 Loss: 0.000004
Training epoch 80 [60800/67107 (90.65%)]	 Loss: 0.000010
Training epoch 80 [61440/67107 (91.60%)]	 Loss: 0.000003
Training epoch 80 [62080/67107 (92.56%)]	 Loss: 0.000024
Training epoch 80 [62720/67107 (93.51%)]	 Loss: 0.000008
Training epoch 80 [63360/67107 (94.47%)]	 Loss: 0.000014
Training epoch 80 [64000/67107 (95.42%)]	 Loss: 0.000003
Training epoch 80 [64640/67107 (96.37%)]	 Loss: 0.000002
Training epoch 80 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 80 [65920/67107 (98.28%)]	 Loss: 0.000010
Training epoch 80 [66560/67107 (99.24%)]	 Loss: 0.000008
Test set: Average Loss: 0.000122
Training epoch 81 [0/67107 (0.00%)]	 Loss: 0.000014
Training epoch 81 [640/67107 (0.95%)]	 Loss: 0.000032
Training epoch 81 [1280/67107 (1.91%)]	 Loss: 0.000006
Training epoch 81 [1920/67107 (2.86%)]	 Loss: 0.000002
Training epoch 81 [2560/67107 (3.82%)]	 Loss: 0.000012
Training epoch 81 [3200/67107 (4.77%)]	 Loss: 0.000049
Training epoch 81 [3840/67107 (5.73%)]	 Loss: 0.000020
Training epoch 81 [4480/67107 (6.68%)]	 Loss: 0.000014
Training epoch 81 [5120/67107 (7.63%)]	 Loss: 0.000020
Training epoch 81 [5760/67107 (8.59%)]	 Loss: 0.000057
Training epoch 81 [6400/67107 (9.54%)]	 Loss: 0.000059
Training epoch 81 [7040/67107 (10.50%)]	 Loss: 0.000024
Training epoch 81 [7680/67107 (11.45%)]	 Loss: 0.000051
Training epoch 81 [8320/67107 (12.40%)]	 Loss: 0.000044
Training epoch 81 [8960/67107 (13.36%)]	 Loss: 0.000018
Training epoch 81 [9600/67107 (14.31%)]	 Loss: 0.000002
Training epoch 81 [10240/67107 (15.27%)]	 Loss: 0.000003
Training epoch 81 [10880/67107 (16.22%)]	 Loss: 0.000049
Training epoch 81 [11520/67107 (17.18%)]	 Loss: 0.000009
Training epoch 81 [12160/67107 (18.13%)]	 Loss: 0.000005
Training epoch 81 [12800/67107 (19.08%)]	 Loss: 0.000009
Training epoch 81 [13440/67107 (20.04%)]	 Loss: 0.000025
Training epoch 81 [14080/67107 (20.99%)]	 Loss: 0.000025
Training epoch 81 [14720/67107 (21.95%)]	 Loss: 0.000024
Training epoch 81 [15360/67107 (22.90%)]	 Loss: 0.000018
Training epoch 81 [16000/67107 (23.85%)]	 Loss: 0.000007
Training epoch 81 [16640/67107 (24.81%)]	 Loss: 0.000024
Training epoch 81 [17280/67107 (25.76%)]	 Loss: 0.000024
Training epoch 81 [17920/67107 (26.72%)]	 Loss: 0.000017
Training epoch 81 [18560/67107 (27.67%)]	 Loss: 0.000015
Training epoch 81 [19200/67107 (28.63%)]	 Loss: 0.000033
Training epoch 81 [19840/67107 (29.58%)]	 Loss: 0.000007
Training epoch 81 [20480/67107 (30.53%)]	 Loss: 0.000019
Training epoch 81 [21120/67107 (31.49%)]	 Loss: 0.000010
Training epoch 81 [21760/67107 (32.44%)]	 Loss: 0.000003
Training epoch 81 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 81 [23040/67107 (34.35%)]	 Loss: 0.000004
Training epoch 81 [23680/67107 (35.31%)]	 Loss: 0.000007
Training epoch 81 [24320/67107 (36.26%)]	 Loss: 0.000003
Training epoch 81 [24960/67107 (37.21%)]	 Loss: 0.000013
Training epoch 81 [25600/67107 (38.17%)]	 Loss: 0.000023
Training epoch 81 [26240/67107 (39.12%)]	 Loss: 0.000012
Training epoch 81 [26880/67107 (40.08%)]	 Loss: 0.000007
Training epoch 81 [27520/67107 (41.03%)]	 Loss: 0.000006
Training epoch 81 [28160/67107 (41.98%)]	 Loss: 0.000009
Training epoch 81 [28800/67107 (42.94%)]	 Loss: 0.000015
Training epoch 81 [29440/67107 (43.89%)]	 Loss: 0.000004
Training epoch 81 [30080/67107 (44.85%)]	 Loss: 0.000031
Training epoch 81 [30720/67107 (45.80%)]	 Loss: 0.000019
Training epoch 81 [31360/67107 (46.76%)]	 Loss: 0.000013
Training epoch 81 [32000/67107 (47.71%)]	 Loss: 0.000009
Training epoch 81 [32640/67107 (48.66%)]	 Loss: 0.000001
Training epoch 81 [33280/67107 (49.62%)]	 Loss: 0.000002
Training epoch 81 [33920/67107 (50.57%)]	 Loss: 0.000003
Training epoch 81 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 81 [35200/67107 (52.48%)]	 Loss: 0.000017
Training epoch 81 [35840/67107 (53.44%)]	 Loss: 0.000006
Training epoch 81 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 81 [37120/67107 (55.34%)]	 Loss: 0.000010
Training epoch 81 [37760/67107 (56.30%)]	 Loss: 0.000009
Training epoch 81 [38400/67107 (57.25%)]	 Loss: 0.000034
Training epoch 81 [39040/67107 (58.21%)]	 Loss: 0.000009
Training epoch 81 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 81 [40320/67107 (60.11%)]	 Loss: 0.000002
Training epoch 81 [40960/67107 (61.07%)]	 Loss: 0.000004
Training epoch 81 [41600/67107 (62.02%)]	 Loss: 0.000003
Training epoch 81 [42240/67107 (62.98%)]	 Loss: 0.000065
Training epoch 81 [42880/67107 (63.93%)]	 Loss: 0.000013
Training epoch 81 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 81 [44160/67107 (65.84%)]	 Loss: 0.000015
Training epoch 81 [44800/67107 (66.79%)]	 Loss: 0.000008
Training epoch 81 [45440/67107 (67.75%)]	 Loss: 0.000008
Training epoch 81 [46080/67107 (68.70%)]	 Loss: 0.000002
Training epoch 81 [46720/67107 (69.66%)]	 Loss: 0.000009
Training epoch 81 [47360/67107 (70.61%)]	 Loss: 0.000001
Training epoch 81 [48000/67107 (71.56%)]	 Loss: 0.000004
Training epoch 81 [48640/67107 (72.52%)]	 Loss: 0.000008
Training epoch 81 [49280/67107 (73.47%)]	 Loss: 0.000007
Training epoch 81 [49920/67107 (74.43%)]	 Loss: 0.000045
Training epoch 81 [50560/67107 (75.38%)]	 Loss: 0.000005
Training epoch 81 [51200/67107 (76.34%)]	 Loss: 0.000008
Training epoch 81 [51840/67107 (77.29%)]	 Loss: 0.000005
Training epoch 81 [52480/67107 (78.24%)]	 Loss: 0.000022
Training epoch 81 [53120/67107 (79.20%)]	 Loss: 0.000002
Training epoch 81 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 81 [54400/67107 (81.11%)]	 Loss: 0.000006
Training epoch 81 [55040/67107 (82.06%)]	 Loss: 0.000002
Training epoch 81 [55680/67107 (83.02%)]	 Loss: 0.000007
Training epoch 81 [56320/67107 (83.97%)]	 Loss: 0.000001
Training epoch 81 [56960/67107 (84.92%)]	 Loss: 0.000001
Training epoch 81 [57600/67107 (85.88%)]	 Loss: 0.000003
Training epoch 81 [58240/67107 (86.83%)]	 Loss: 0.000006
Training epoch 81 [58880/67107 (87.79%)]	 Loss: 0.000001
Training epoch 81 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 81 [60160/67107 (89.69%)]	 Loss: 0.000010
Training epoch 81 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 81 [61440/67107 (91.60%)]	 Loss: 0.000004
Training epoch 81 [62080/67107 (92.56%)]	 Loss: 0.000012
Training epoch 81 [62720/67107 (93.51%)]	 Loss: 0.000009
Training epoch 81 [63360/67107 (94.47%)]	 Loss: 0.000006
Training epoch 81 [64000/67107 (95.42%)]	 Loss: 0.000006
Training epoch 81 [64640/67107 (96.37%)]	 Loss: 0.000016
Training epoch 81 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 81 [65920/67107 (98.28%)]	 Loss: 0.000037
Training epoch 81 [66560/67107 (99.24%)]	 Loss: 0.000021
Test set: Average Loss: 0.000578
Training epoch 82 [0/67107 (0.00%)]	 Loss: 0.000016
Training epoch 82 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 82 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 82 [1920/67107 (2.86%)]	 Loss: 0.000007
Training epoch 82 [2560/67107 (3.82%)]	 Loss: 0.000005
Training epoch 82 [3200/67107 (4.77%)]	 Loss: 0.000002
Training epoch 82 [3840/67107 (5.73%)]	 Loss: 0.000015
Training epoch 82 [4480/67107 (6.68%)]	 Loss: 0.000003
Training epoch 82 [5120/67107 (7.63%)]	 Loss: 0.000004
Training epoch 82 [5760/67107 (8.59%)]	 Loss: 0.000020
Training epoch 82 [6400/67107 (9.54%)]	 Loss: 0.000003
Training epoch 82 [7040/67107 (10.50%)]	 Loss: 0.000026
Training epoch 82 [7680/67107 (11.45%)]	 Loss: 0.000019
Training epoch 82 [8320/67107 (12.40%)]	 Loss: 0.000006
Training epoch 82 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 82 [9600/67107 (14.31%)]	 Loss: 0.000002
Training epoch 82 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 82 [10880/67107 (16.22%)]	 Loss: 0.000007
Training epoch 82 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 82 [12160/67107 (18.13%)]	 Loss: 0.000009
Training epoch 82 [12800/67107 (19.08%)]	 Loss: 0.000021
Training epoch 82 [13440/67107 (20.04%)]	 Loss: 0.000002
Training epoch 82 [14080/67107 (20.99%)]	 Loss: 0.000011
Training epoch 82 [14720/67107 (21.95%)]	 Loss: 0.000003
Training epoch 82 [15360/67107 (22.90%)]	 Loss: 0.000005
Training epoch 82 [16000/67107 (23.85%)]	 Loss: 0.000002
Training epoch 82 [16640/67107 (24.81%)]	 Loss: 0.000001
Training epoch 82 [17280/67107 (25.76%)]	 Loss: 0.000014
Training epoch 82 [17920/67107 (26.72%)]	 Loss: 0.000012
Training epoch 82 [18560/67107 (27.67%)]	 Loss: 0.000010
Training epoch 82 [19200/67107 (28.63%)]	 Loss: 0.000002
Training epoch 82 [19840/67107 (29.58%)]	 Loss: 0.000030
Training epoch 82 [20480/67107 (30.53%)]	 Loss: 0.000012
Training epoch 82 [21120/67107 (31.49%)]	 Loss: 0.000003
Training epoch 82 [21760/67107 (32.44%)]	 Loss: 0.000006
Training epoch 82 [22400/67107 (33.40%)]	 Loss: 0.000004
Training epoch 82 [23040/67107 (34.35%)]	 Loss: 0.000003
Training epoch 82 [23680/67107 (35.31%)]	 Loss: 0.000003
Training epoch 82 [24320/67107 (36.26%)]	 Loss: 0.000002
Training epoch 82 [24960/67107 (37.21%)]	 Loss: 0.000001
Training epoch 82 [25600/67107 (38.17%)]	 Loss: 0.000003
Training epoch 82 [26240/67107 (39.12%)]	 Loss: 0.000003
Training epoch 82 [26880/67107 (40.08%)]	 Loss: 0.000005
Training epoch 82 [27520/67107 (41.03%)]	 Loss: 0.000020
Training epoch 82 [28160/67107 (41.98%)]	 Loss: 0.000003
Training epoch 82 [28800/67107 (42.94%)]	 Loss: 0.000003
Training epoch 82 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 82 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 82 [30720/67107 (45.80%)]	 Loss: 0.000004
Training epoch 82 [31360/67107 (46.76%)]	 Loss: 0.000009
Training epoch 82 [32000/67107 (47.71%)]	 Loss: 0.000008
Training epoch 82 [32640/67107 (48.66%)]	 Loss: 0.000006
Training epoch 82 [33280/67107 (49.62%)]	 Loss: 0.000014
Training epoch 82 [33920/67107 (50.57%)]	 Loss: 0.000009
Training epoch 82 [34560/67107 (51.53%)]	 Loss: 0.000024
Training epoch 82 [35200/67107 (52.48%)]	 Loss: 0.000007
Training epoch 82 [35840/67107 (53.44%)]	 Loss: 0.000002
Training epoch 82 [36480/67107 (54.39%)]	 Loss: 0.000002
Training epoch 82 [37120/67107 (55.34%)]	 Loss: 0.000011
Training epoch 82 [37760/67107 (56.30%)]	 Loss: 0.000054
Training epoch 82 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 82 [39040/67107 (58.21%)]	 Loss: 0.000004
Training epoch 82 [39680/67107 (59.16%)]	 Loss: 0.000010
Training epoch 82 [40320/67107 (60.11%)]	 Loss: 0.000009
Training epoch 82 [40960/67107 (61.07%)]	 Loss: 0.000028
Training epoch 82 [41600/67107 (62.02%)]	 Loss: 0.000012
Training epoch 82 [42240/67107 (62.98%)]	 Loss: 0.000015
Training epoch 82 [42880/67107 (63.93%)]	 Loss: 0.000014
Training epoch 82 [43520/67107 (64.89%)]	 Loss: 0.000043
Training epoch 82 [44160/67107 (65.84%)]	 Loss: 0.000152
Training epoch 82 [44800/67107 (66.79%)]	 Loss: 0.000016
Training epoch 82 [45440/67107 (67.75%)]	 Loss: 0.000011
Training epoch 82 [46080/67107 (68.70%)]	 Loss: 0.000013
Training epoch 82 [46720/67107 (69.66%)]	 Loss: 0.000011
Training epoch 82 [47360/67107 (70.61%)]	 Loss: 0.000050
Training epoch 82 [48000/67107 (71.56%)]	 Loss: 0.000037
Training epoch 82 [48640/67107 (72.52%)]	 Loss: 0.000008
Training epoch 82 [49280/67107 (73.47%)]	 Loss: 0.000033
Training epoch 82 [49920/67107 (74.43%)]	 Loss: 0.000010
Training epoch 82 [50560/67107 (75.38%)]	 Loss: 0.000019
Training epoch 82 [51200/67107 (76.34%)]	 Loss: 0.000016
Training epoch 82 [51840/67107 (77.29%)]	 Loss: 0.000030
Training epoch 82 [52480/67107 (78.24%)]	 Loss: 0.000004
Training epoch 82 [53120/67107 (79.20%)]	 Loss: 0.000012
Training epoch 82 [53760/67107 (80.15%)]	 Loss: 0.000007
Training epoch 82 [54400/67107 (81.11%)]	 Loss: 0.000027
Training epoch 82 [55040/67107 (82.06%)]	 Loss: 0.000012
Training epoch 82 [55680/67107 (83.02%)]	 Loss: 0.000028
Training epoch 82 [56320/67107 (83.97%)]	 Loss: 0.000007
Training epoch 82 [56960/67107 (84.92%)]	 Loss: 0.000001
Training epoch 82 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 82 [58240/67107 (86.83%)]	 Loss: 0.000006
Training epoch 82 [58880/67107 (87.79%)]	 Loss: 0.000006
Training epoch 82 [59520/67107 (88.74%)]	 Loss: 0.000003
Training epoch 82 [60160/67107 (89.69%)]	 Loss: 0.000003
Training epoch 82 [60800/67107 (90.65%)]	 Loss: 0.000003
Training epoch 82 [61440/67107 (91.60%)]	 Loss: 0.000008
Training epoch 82 [62080/67107 (92.56%)]	 Loss: 0.000003
Training epoch 82 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 82 [63360/67107 (94.47%)]	 Loss: 0.000005
Training epoch 82 [64000/67107 (95.42%)]	 Loss: 0.000014
Training epoch 82 [64640/67107 (96.37%)]	 Loss: 0.000004
Training epoch 82 [65280/67107 (97.33%)]	 Loss: 0.000001
Training epoch 82 [65920/67107 (98.28%)]	 Loss: 0.000031
Training epoch 82 [66560/67107 (99.24%)]	 Loss: 0.000009
Test set: Average Loss: 0.001780
Training epoch 83 [0/67107 (0.00%)]	 Loss: 0.000005
Training epoch 83 [640/67107 (0.95%)]	 Loss: 0.000013
Training epoch 83 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 83 [1920/67107 (2.86%)]	 Loss: 0.000010
Training epoch 83 [2560/67107 (3.82%)]	 Loss: 0.000004
Training epoch 83 [3200/67107 (4.77%)]	 Loss: 0.000007
Training epoch 83 [3840/67107 (5.73%)]	 Loss: 0.000001
Training epoch 83 [4480/67107 (6.68%)]	 Loss: 0.000003
Training epoch 83 [5120/67107 (7.63%)]	 Loss: 0.000010
Training epoch 83 [5760/67107 (8.59%)]	 Loss: 0.000009
Training epoch 83 [6400/67107 (9.54%)]	 Loss: 0.000005
Training epoch 83 [7040/67107 (10.50%)]	 Loss: 0.000009
Training epoch 83 [7680/67107 (11.45%)]	 Loss: 0.000014
Training epoch 83 [8320/67107 (12.40%)]	 Loss: 0.000006
Training epoch 83 [8960/67107 (13.36%)]	 Loss: 0.000012
Training epoch 83 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 83 [10240/67107 (15.27%)]	 Loss: 0.000006
Training epoch 83 [10880/67107 (16.22%)]	 Loss: 0.000016
Training epoch 83 [11520/67107 (17.18%)]	 Loss: 0.000026
Training epoch 83 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 83 [12800/67107 (19.08%)]	 Loss: 0.000006
Training epoch 83 [13440/67107 (20.04%)]	 Loss: 0.000007
Training epoch 83 [14080/67107 (20.99%)]	 Loss: 0.000009
Training epoch 83 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 83 [15360/67107 (22.90%)]	 Loss: 0.000014
Training epoch 83 [16000/67107 (23.85%)]	 Loss: 0.000043
Training epoch 83 [16640/67107 (24.81%)]	 Loss: 0.000006
Training epoch 83 [17280/67107 (25.76%)]	 Loss: 0.000016
Training epoch 83 [17920/67107 (26.72%)]	 Loss: 0.000003
Training epoch 83 [18560/67107 (27.67%)]	 Loss: 0.000004
Training epoch 83 [19200/67107 (28.63%)]	 Loss: 0.000045
Training epoch 83 [19840/67107 (29.58%)]	 Loss: 0.000010
Training epoch 83 [20480/67107 (30.53%)]	 Loss: 0.000009
Training epoch 83 [21120/67107 (31.49%)]	 Loss: 0.000003
Training epoch 83 [21760/67107 (32.44%)]	 Loss: 0.000002
Training epoch 83 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 83 [23040/67107 (34.35%)]	 Loss: 0.000018
Training epoch 83 [23680/67107 (35.31%)]	 Loss: 0.000007
Training epoch 83 [24320/67107 (36.26%)]	 Loss: 0.000013
Training epoch 83 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 83 [25600/67107 (38.17%)]	 Loss: 0.000009
Training epoch 83 [26240/67107 (39.12%)]	 Loss: 0.000017
Training epoch 83 [26880/67107 (40.08%)]	 Loss: 0.000026
Training epoch 83 [27520/67107 (41.03%)]	 Loss: 0.000018
Training epoch 83 [28160/67107 (41.98%)]	 Loss: 0.000011
Training epoch 83 [28800/67107 (42.94%)]	 Loss: 0.000009
Training epoch 83 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 83 [30080/67107 (44.85%)]	 Loss: 0.000009
Training epoch 83 [30720/67107 (45.80%)]	 Loss: 0.000005
Training epoch 83 [31360/67107 (46.76%)]	 Loss: 0.000014
Training epoch 83 [32000/67107 (47.71%)]	 Loss: 0.000013
Training epoch 83 [32640/67107 (48.66%)]	 Loss: 0.000026
Training epoch 83 [33280/67107 (49.62%)]	 Loss: 0.000004
Training epoch 83 [33920/67107 (50.57%)]	 Loss: 0.000010
Training epoch 83 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 83 [35200/67107 (52.48%)]	 Loss: 0.000006
Training epoch 83 [35840/67107 (53.44%)]	 Loss: 0.000020
Training epoch 83 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 83 [37120/67107 (55.34%)]	 Loss: 0.000004
Training epoch 83 [37760/67107 (56.30%)]	 Loss: 0.000001
Training epoch 83 [38400/67107 (57.25%)]	 Loss: 0.000022
Training epoch 83 [39040/67107 (58.21%)]	 Loss: 0.000015
Training epoch 83 [39680/67107 (59.16%)]	 Loss: 0.000010
Training epoch 83 [40320/67107 (60.11%)]	 Loss: 0.000007
Training epoch 83 [40960/67107 (61.07%)]	 Loss: 0.000047
Training epoch 83 [41600/67107 (62.02%)]	 Loss: 0.000003
Training epoch 83 [42240/67107 (62.98%)]	 Loss: 0.000020
Training epoch 83 [42880/67107 (63.93%)]	 Loss: 0.000016
Training epoch 83 [43520/67107 (64.89%)]	 Loss: 0.000015
Training epoch 83 [44160/67107 (65.84%)]	 Loss: 0.000008
Training epoch 83 [44800/67107 (66.79%)]	 Loss: 0.000007
Training epoch 83 [45440/67107 (67.75%)]	 Loss: 0.000012
Training epoch 83 [46080/67107 (68.70%)]	 Loss: 0.000005
Training epoch 83 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 83 [47360/67107 (70.61%)]	 Loss: 0.000003
Training epoch 83 [48000/67107 (71.56%)]	 Loss: 0.000006
Training epoch 83 [48640/67107 (72.52%)]	 Loss: 0.000006
Training epoch 83 [49280/67107 (73.47%)]	 Loss: 0.000001
Training epoch 83 [49920/67107 (74.43%)]	 Loss: 0.000005
Training epoch 83 [50560/67107 (75.38%)]	 Loss: 0.000002
Training epoch 83 [51200/67107 (76.34%)]	 Loss: 0.000002
Training epoch 83 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 83 [52480/67107 (78.24%)]	 Loss: 0.000003
Training epoch 83 [53120/67107 (79.20%)]	 Loss: 0.000005
Training epoch 83 [53760/67107 (80.15%)]	 Loss: 0.000009
Training epoch 83 [54400/67107 (81.11%)]	 Loss: 0.000008
Training epoch 83 [55040/67107 (82.06%)]	 Loss: 0.000022
Training epoch 83 [55680/67107 (83.02%)]	 Loss: 0.000007
Training epoch 83 [56320/67107 (83.97%)]	 Loss: 0.000003
Training epoch 83 [56960/67107 (84.92%)]	 Loss: 0.000014
Training epoch 83 [57600/67107 (85.88%)]	 Loss: 0.000006
Training epoch 83 [58240/67107 (86.83%)]	 Loss: 0.000007
Training epoch 83 [58880/67107 (87.79%)]	 Loss: 0.000006
Training epoch 83 [59520/67107 (88.74%)]	 Loss: 0.000018
Training epoch 83 [60160/67107 (89.69%)]	 Loss: 0.000014
Training epoch 83 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 83 [61440/67107 (91.60%)]	 Loss: 0.000009
Training epoch 83 [62080/67107 (92.56%)]	 Loss: 0.000066
Training epoch 83 [62720/67107 (93.51%)]	 Loss: 0.000012
Training epoch 83 [63360/67107 (94.47%)]	 Loss: 0.000002
Training epoch 83 [64000/67107 (95.42%)]	 Loss: 0.000007
Training epoch 83 [64640/67107 (96.37%)]	 Loss: 0.000001
Training epoch 83 [65280/67107 (97.33%)]	 Loss: 0.000049
Training epoch 83 [65920/67107 (98.28%)]	 Loss: 0.000051
Training epoch 83 [66560/67107 (99.24%)]	 Loss: 0.000065
Test set: Average Loss: 0.001104
Training epoch 84 [0/67107 (0.00%)]	 Loss: 0.000027
Training epoch 84 [640/67107 (0.95%)]	 Loss: 0.000012
Training epoch 84 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 84 [1920/67107 (2.86%)]	 Loss: 0.000023
Training epoch 84 [2560/67107 (3.82%)]	 Loss: 0.000034
Training epoch 84 [3200/67107 (4.77%)]	 Loss: 0.000011
Training epoch 84 [3840/67107 (5.73%)]	 Loss: 0.000008
Training epoch 84 [4480/67107 (6.68%)]	 Loss: 0.000008
Training epoch 84 [5120/67107 (7.63%)]	 Loss: 0.000004
Training epoch 84 [5760/67107 (8.59%)]	 Loss: 0.000007
Training epoch 84 [6400/67107 (9.54%)]	 Loss: 0.000063
Training epoch 84 [7040/67107 (10.50%)]	 Loss: 0.000015
Training epoch 84 [7680/67107 (11.45%)]	 Loss: 0.000020
Training epoch 84 [8320/67107 (12.40%)]	 Loss: 0.000020
Training epoch 84 [8960/67107 (13.36%)]	 Loss: 0.000068
Training epoch 84 [9600/67107 (14.31%)]	 Loss: 0.000012
Training epoch 84 [10240/67107 (15.27%)]	 Loss: 0.000029
Training epoch 84 [10880/67107 (16.22%)]	 Loss: 0.000004
Training epoch 84 [11520/67107 (17.18%)]	 Loss: 0.000003
Training epoch 84 [12160/67107 (18.13%)]	 Loss: 0.000033
Training epoch 84 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 84 [13440/67107 (20.04%)]	 Loss: 0.000022
Training epoch 84 [14080/67107 (20.99%)]	 Loss: 0.000026
Training epoch 84 [14720/67107 (21.95%)]	 Loss: 0.000009
Training epoch 84 [15360/67107 (22.90%)]	 Loss: 0.000020
Training epoch 84 [16000/67107 (23.85%)]	 Loss: 0.000070
Training epoch 84 [16640/67107 (24.81%)]	 Loss: 0.000031
Training epoch 84 [17280/67107 (25.76%)]	 Loss: 0.000004
Training epoch 84 [17920/67107 (26.72%)]	 Loss: 0.000001
Training epoch 84 [18560/67107 (27.67%)]	 Loss: 0.000007
Training epoch 84 [19200/67107 (28.63%)]	 Loss: 0.000005
Training epoch 84 [19840/67107 (29.58%)]	 Loss: 0.000013
Training epoch 84 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 84 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 84 [21760/67107 (32.44%)]	 Loss: 0.000006
Training epoch 84 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 84 [23040/67107 (34.35%)]	 Loss: 0.000006
Training epoch 84 [23680/67107 (35.31%)]	 Loss: 0.000009
Training epoch 84 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 84 [24960/67107 (37.21%)]	 Loss: 0.000003
Training epoch 84 [25600/67107 (38.17%)]	 Loss: 0.000002
Training epoch 84 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 84 [26880/67107 (40.08%)]	 Loss: 0.000002
Training epoch 84 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 84 [28160/67107 (41.98%)]	 Loss: 0.000016
Training epoch 84 [28800/67107 (42.94%)]	 Loss: 0.000029
Training epoch 84 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 84 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 84 [30720/67107 (45.80%)]	 Loss: 0.000004
Training epoch 84 [31360/67107 (46.76%)]	 Loss: 0.000005
Training epoch 84 [32000/67107 (47.71%)]	 Loss: 0.000009
Training epoch 84 [32640/67107 (48.66%)]	 Loss: 0.000011
Training epoch 84 [33280/67107 (49.62%)]	 Loss: 0.000009
Training epoch 84 [33920/67107 (50.57%)]	 Loss: 0.000007
Training epoch 84 [34560/67107 (51.53%)]	 Loss: 0.000007
Training epoch 84 [35200/67107 (52.48%)]	 Loss: 0.000003
Training epoch 84 [35840/67107 (53.44%)]	 Loss: 0.000037
Training epoch 84 [36480/67107 (54.39%)]	 Loss: 0.000027
Training epoch 84 [37120/67107 (55.34%)]	 Loss: 0.000017
Training epoch 84 [37760/67107 (56.30%)]	 Loss: 0.000011
Training epoch 84 [38400/67107 (57.25%)]	 Loss: 0.000011
Training epoch 84 [39040/67107 (58.21%)]	 Loss: 0.000006
Training epoch 84 [39680/67107 (59.16%)]	 Loss: 0.000001
Training epoch 84 [40320/67107 (60.11%)]	 Loss: 0.000001
Training epoch 84 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 84 [41600/67107 (62.02%)]	 Loss: 0.000005
Training epoch 84 [42240/67107 (62.98%)]	 Loss: 0.000003
Training epoch 84 [42880/67107 (63.93%)]	 Loss: 0.000009
Training epoch 84 [43520/67107 (64.89%)]	 Loss: 0.000002
Training epoch 84 [44160/67107 (65.84%)]	 Loss: 0.000009
Training epoch 84 [44800/67107 (66.79%)]	 Loss: 0.000008
Training epoch 84 [45440/67107 (67.75%)]	 Loss: 0.000013
Training epoch 84 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 84 [46720/67107 (69.66%)]	 Loss: 0.000002
Training epoch 84 [47360/67107 (70.61%)]	 Loss: 0.000015
Training epoch 84 [48000/67107 (71.56%)]	 Loss: 0.000009
Training epoch 84 [48640/67107 (72.52%)]	 Loss: 0.000002
Training epoch 84 [49280/67107 (73.47%)]	 Loss: 0.000005
Training epoch 84 [49920/67107 (74.43%)]	 Loss: 0.000004
Training epoch 84 [50560/67107 (75.38%)]	 Loss: 0.000001
Training epoch 84 [51200/67107 (76.34%)]	 Loss: 0.000006
Training epoch 84 [51840/67107 (77.29%)]	 Loss: 0.000021
Training epoch 84 [52480/67107 (78.24%)]	 Loss: 0.000021
Training epoch 84 [53120/67107 (79.20%)]	 Loss: 0.000015
Training epoch 84 [53760/67107 (80.15%)]	 Loss: 0.000006
Training epoch 84 [54400/67107 (81.11%)]	 Loss: 0.000028
Training epoch 84 [55040/67107 (82.06%)]	 Loss: 0.000016
Training epoch 84 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 84 [56320/67107 (83.97%)]	 Loss: 0.000002
Training epoch 84 [56960/67107 (84.92%)]	 Loss: 0.000003
Training epoch 84 [57600/67107 (85.88%)]	 Loss: 0.000001
Training epoch 84 [58240/67107 (86.83%)]	 Loss: 0.000002
Training epoch 84 [58880/67107 (87.79%)]	 Loss: 0.000004
Training epoch 84 [59520/67107 (88.74%)]	 Loss: 0.000013
Training epoch 84 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 84 [60800/67107 (90.65%)]	 Loss: 0.000014
Training epoch 84 [61440/67107 (91.60%)]	 Loss: 0.000009
Training epoch 84 [62080/67107 (92.56%)]	 Loss: 0.000019
Training epoch 84 [62720/67107 (93.51%)]	 Loss: 0.000025
Training epoch 84 [63360/67107 (94.47%)]	 Loss: 0.000021
Training epoch 84 [64000/67107 (95.42%)]	 Loss: 0.000007
Training epoch 84 [64640/67107 (96.37%)]	 Loss: 0.000021
Training epoch 84 [65280/67107 (97.33%)]	 Loss: 0.000007
Training epoch 84 [65920/67107 (98.28%)]	 Loss: 0.000041
Training epoch 84 [66560/67107 (99.24%)]	 Loss: 0.000006
Test set: Average Loss: 0.000644
Training epoch 85 [0/67107 (0.00%)]	 Loss: 0.000050
Training epoch 85 [640/67107 (0.95%)]	 Loss: 0.000009
Training epoch 85 [1280/67107 (1.91%)]	 Loss: 0.000021
Training epoch 85 [1920/67107 (2.86%)]	 Loss: 0.000004
Training epoch 85 [2560/67107 (3.82%)]	 Loss: 0.000003
Training epoch 85 [3200/67107 (4.77%)]	 Loss: 0.000004
Training epoch 85 [3840/67107 (5.73%)]	 Loss: 0.000016
Training epoch 85 [4480/67107 (6.68%)]	 Loss: 0.000005
Training epoch 85 [5120/67107 (7.63%)]	 Loss: 0.000028
Training epoch 85 [5760/67107 (8.59%)]	 Loss: 0.000010
Training epoch 85 [6400/67107 (9.54%)]	 Loss: 0.000004
Training epoch 85 [7040/67107 (10.50%)]	 Loss: 0.000020
Training epoch 85 [7680/67107 (11.45%)]	 Loss: 0.000007
Training epoch 85 [8320/67107 (12.40%)]	 Loss: 0.000016
Training epoch 85 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 85 [9600/67107 (14.31%)]	 Loss: 0.000004
Training epoch 85 [10240/67107 (15.27%)]	 Loss: 0.000009
Training epoch 85 [10880/67107 (16.22%)]	 Loss: 0.000002
Training epoch 85 [11520/67107 (17.18%)]	 Loss: 0.000008
Training epoch 85 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 85 [12800/67107 (19.08%)]	 Loss: 0.000048
Training epoch 85 [13440/67107 (20.04%)]	 Loss: 0.000008
Training epoch 85 [14080/67107 (20.99%)]	 Loss: 0.000029
Training epoch 85 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 85 [15360/67107 (22.90%)]	 Loss: 0.000006
Training epoch 85 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 85 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 85 [17280/67107 (25.76%)]	 Loss: 0.000052
Training epoch 85 [17920/67107 (26.72%)]	 Loss: 0.000017
Training epoch 85 [18560/67107 (27.67%)]	 Loss: 0.000011
Training epoch 85 [19200/67107 (28.63%)]	 Loss: 0.000017
Training epoch 85 [19840/67107 (29.58%)]	 Loss: 0.000019
Training epoch 85 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 85 [21120/67107 (31.49%)]	 Loss: 0.000009
Training epoch 85 [21760/67107 (32.44%)]	 Loss: 0.000009
Training epoch 85 [22400/67107 (33.40%)]	 Loss: 0.000057
Training epoch 85 [23040/67107 (34.35%)]	 Loss: 0.000025
Training epoch 85 [23680/67107 (35.31%)]	 Loss: 0.000037
Training epoch 85 [24320/67107 (36.26%)]	 Loss: 0.000029
Training epoch 85 [24960/67107 (37.21%)]	 Loss: 0.000010
Training epoch 85 [25600/67107 (38.17%)]	 Loss: 0.000001
Training epoch 85 [26240/67107 (39.12%)]	 Loss: 0.000001
Training epoch 85 [26880/67107 (40.08%)]	 Loss: 0.000002
Training epoch 85 [27520/67107 (41.03%)]	 Loss: 0.000027
Training epoch 85 [28160/67107 (41.98%)]	 Loss: 0.000005
Training epoch 85 [28800/67107 (42.94%)]	 Loss: 0.000013
Training epoch 85 [29440/67107 (43.89%)]	 Loss: 0.000003
Training epoch 85 [30080/67107 (44.85%)]	 Loss: 0.000005
Training epoch 85 [30720/67107 (45.80%)]	 Loss: 0.000003
Training epoch 85 [31360/67107 (46.76%)]	 Loss: 0.000002
Training epoch 85 [32000/67107 (47.71%)]	 Loss: 0.000013
Training epoch 85 [32640/67107 (48.66%)]	 Loss: 0.000002
Training epoch 85 [33280/67107 (49.62%)]	 Loss: 0.000004
Training epoch 85 [33920/67107 (50.57%)]	 Loss: 0.000001
Training epoch 85 [34560/67107 (51.53%)]	 Loss: 0.000025
Training epoch 85 [35200/67107 (52.48%)]	 Loss: 0.000015
Training epoch 85 [35840/67107 (53.44%)]	 Loss: 0.000002
Training epoch 85 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 85 [37120/67107 (55.34%)]	 Loss: 0.000002
Training epoch 85 [37760/67107 (56.30%)]	 Loss: 0.000006
Training epoch 85 [38400/67107 (57.25%)]	 Loss: 0.000041
Training epoch 85 [39040/67107 (58.21%)]	 Loss: 0.000015
Training epoch 85 [39680/67107 (59.16%)]	 Loss: 0.000014
Training epoch 85 [40320/67107 (60.11%)]	 Loss: 0.000005
Training epoch 85 [40960/67107 (61.07%)]	 Loss: 0.000004
Training epoch 85 [41600/67107 (62.02%)]	 Loss: 0.000018
Training epoch 85 [42240/67107 (62.98%)]	 Loss: 0.000006
Training epoch 85 [42880/67107 (63.93%)]	 Loss: 0.000035
Training epoch 85 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 85 [44160/67107 (65.84%)]	 Loss: 0.000009
Training epoch 85 [44800/67107 (66.79%)]	 Loss: 0.000003
Training epoch 85 [45440/67107 (67.75%)]	 Loss: 0.000014
Training epoch 85 [46080/67107 (68.70%)]	 Loss: 0.000018
Training epoch 85 [46720/67107 (69.66%)]	 Loss: 0.000010
Training epoch 85 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 85 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 85 [48640/67107 (72.52%)]	 Loss: 0.000002
Training epoch 85 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 85 [49920/67107 (74.43%)]	 Loss: 0.000007
Training epoch 85 [50560/67107 (75.38%)]	 Loss: 0.000010
Training epoch 85 [51200/67107 (76.34%)]	 Loss: 0.000003
Training epoch 85 [51840/67107 (77.29%)]	 Loss: 0.000009
Training epoch 85 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 85 [53120/67107 (79.20%)]	 Loss: 0.000004
Training epoch 85 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 85 [54400/67107 (81.11%)]	 Loss: 0.000007
Training epoch 85 [55040/67107 (82.06%)]	 Loss: 0.000003
Training epoch 85 [55680/67107 (83.02%)]	 Loss: 0.000017
Training epoch 85 [56320/67107 (83.97%)]	 Loss: 0.000011
Training epoch 85 [56960/67107 (84.92%)]	 Loss: 0.000025
Training epoch 85 [57600/67107 (85.88%)]	 Loss: 0.000011
Training epoch 85 [58240/67107 (86.83%)]	 Loss: 0.000004
Training epoch 85 [58880/67107 (87.79%)]	 Loss: 0.000015
Training epoch 85 [59520/67107 (88.74%)]	 Loss: 0.000002
Training epoch 85 [60160/67107 (89.69%)]	 Loss: 0.000004
Training epoch 85 [60800/67107 (90.65%)]	 Loss: 0.000007
Training epoch 85 [61440/67107 (91.60%)]	 Loss: 0.000024
Training epoch 85 [62080/67107 (92.56%)]	 Loss: 0.000001
Training epoch 85 [62720/67107 (93.51%)]	 Loss: 0.000002
Training epoch 85 [63360/67107 (94.47%)]	 Loss: 0.000002
Training epoch 85 [64000/67107 (95.42%)]	 Loss: 0.000006
Training epoch 85 [64640/67107 (96.37%)]	 Loss: 0.000016
Training epoch 85 [65280/67107 (97.33%)]	 Loss: 0.000007
Training epoch 85 [65920/67107 (98.28%)]	 Loss: 0.000003
Training epoch 85 [66560/67107 (99.24%)]	 Loss: 0.000024
Test set: Average Loss: 0.001381
Training epoch 86 [0/67107 (0.00%)]	 Loss: 0.000016
Training epoch 86 [640/67107 (0.95%)]	 Loss: 0.000022
Training epoch 86 [1280/67107 (1.91%)]	 Loss: 0.000009
Training epoch 86 [1920/67107 (2.86%)]	 Loss: 0.000001
Training epoch 86 [2560/67107 (3.82%)]	 Loss: 0.000034
Training epoch 86 [3200/67107 (4.77%)]	 Loss: 0.000023
Training epoch 86 [3840/67107 (5.73%)]	 Loss: 0.000004
Training epoch 86 [4480/67107 (6.68%)]	 Loss: 0.000011
Training epoch 86 [5120/67107 (7.63%)]	 Loss: 0.000033
Training epoch 86 [5760/67107 (8.59%)]	 Loss: 0.000011
Training epoch 86 [6400/67107 (9.54%)]	 Loss: 0.000002
Training epoch 86 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 86 [7680/67107 (11.45%)]	 Loss: 0.000008
Training epoch 86 [8320/67107 (12.40%)]	 Loss: 0.000019
Training epoch 86 [8960/67107 (13.36%)]	 Loss: 0.000019
Training epoch 86 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 86 [10240/67107 (15.27%)]	 Loss: 0.000024
Training epoch 86 [10880/67107 (16.22%)]	 Loss: 0.000036
Training epoch 86 [11520/67107 (17.18%)]	 Loss: 0.000025
Training epoch 86 [12160/67107 (18.13%)]	 Loss: 0.000005
Training epoch 86 [12800/67107 (19.08%)]	 Loss: 0.000006
Training epoch 86 [13440/67107 (20.04%)]	 Loss: 0.000012
Training epoch 86 [14080/67107 (20.99%)]	 Loss: 0.000064
Training epoch 86 [14720/67107 (21.95%)]	 Loss: 0.000003
Training epoch 86 [15360/67107 (22.90%)]	 Loss: 0.000007
Training epoch 86 [16000/67107 (23.85%)]	 Loss: 0.000031
Training epoch 86 [16640/67107 (24.81%)]	 Loss: 0.000011
Training epoch 86 [17280/67107 (25.76%)]	 Loss: 0.000024
Training epoch 86 [17920/67107 (26.72%)]	 Loss: 0.000039
Training epoch 86 [18560/67107 (27.67%)]	 Loss: 0.000041
Training epoch 86 [19200/67107 (28.63%)]	 Loss: 0.000009
Training epoch 86 [19840/67107 (29.58%)]	 Loss: 0.000005
Training epoch 86 [20480/67107 (30.53%)]	 Loss: 0.000002
Training epoch 86 [21120/67107 (31.49%)]	 Loss: 0.000032
Training epoch 86 [21760/67107 (32.44%)]	 Loss: 0.000005
Training epoch 86 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 86 [23040/67107 (34.35%)]	 Loss: 0.000035
Training epoch 86 [23680/67107 (35.31%)]	 Loss: 0.000012
Training epoch 86 [24320/67107 (36.26%)]	 Loss: 0.000016
Training epoch 86 [24960/67107 (37.21%)]	 Loss: 0.000011
Training epoch 86 [25600/67107 (38.17%)]	 Loss: 0.000004
Training epoch 86 [26240/67107 (39.12%)]	 Loss: 0.000007
Training epoch 86 [26880/67107 (40.08%)]	 Loss: 0.000026
Training epoch 86 [27520/67107 (41.03%)]	 Loss: 0.000006
Training epoch 86 [28160/67107 (41.98%)]	 Loss: 0.000010
Training epoch 86 [28800/67107 (42.94%)]	 Loss: 0.000002
Training epoch 86 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 86 [30080/67107 (44.85%)]	 Loss: 0.000008
Training epoch 86 [30720/67107 (45.80%)]	 Loss: 0.000003
Training epoch 86 [31360/67107 (46.76%)]	 Loss: 0.000025
Training epoch 86 [32000/67107 (47.71%)]	 Loss: 0.000003
Training epoch 86 [32640/67107 (48.66%)]	 Loss: 0.000002
Training epoch 86 [33280/67107 (49.62%)]	 Loss: 0.000005
Training epoch 86 [33920/67107 (50.57%)]	 Loss: 0.000007
Training epoch 86 [34560/67107 (51.53%)]	 Loss: 0.000002
Training epoch 86 [35200/67107 (52.48%)]	 Loss: 0.000003
Training epoch 86 [35840/67107 (53.44%)]	 Loss: 0.000001
Training epoch 86 [36480/67107 (54.39%)]	 Loss: 0.000003
Training epoch 86 [37120/67107 (55.34%)]	 Loss: 0.000012
Training epoch 86 [37760/67107 (56.30%)]	 Loss: 0.000009
Training epoch 86 [38400/67107 (57.25%)]	 Loss: 0.000010
Training epoch 86 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 86 [39680/67107 (59.16%)]	 Loss: 0.000005
Training epoch 86 [40320/67107 (60.11%)]	 Loss: 0.000025
Training epoch 86 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 86 [41600/67107 (62.02%)]	 Loss: 0.000008
Training epoch 86 [42240/67107 (62.98%)]	 Loss: 0.000015
Training epoch 86 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 86 [43520/67107 (64.89%)]	 Loss: 0.000010
Training epoch 86 [44160/67107 (65.84%)]	 Loss: 0.000039
Training epoch 86 [44800/67107 (66.79%)]	 Loss: 0.000034
Training epoch 86 [45440/67107 (67.75%)]	 Loss: 0.000007
Training epoch 86 [46080/67107 (68.70%)]	 Loss: 0.000010
Training epoch 86 [46720/67107 (69.66%)]	 Loss: 0.000011
Training epoch 86 [47360/67107 (70.61%)]	 Loss: 0.000004
Training epoch 86 [48000/67107 (71.56%)]	 Loss: 0.000002
Training epoch 86 [48640/67107 (72.52%)]	 Loss: 0.000003
Training epoch 86 [49280/67107 (73.47%)]	 Loss: 0.000010
Training epoch 86 [49920/67107 (74.43%)]	 Loss: 0.000016
Training epoch 86 [50560/67107 (75.38%)]	 Loss: 0.000009
Training epoch 86 [51200/67107 (76.34%)]	 Loss: 0.000022
Training epoch 86 [51840/67107 (77.29%)]	 Loss: 0.000017
Training epoch 86 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 86 [53120/67107 (79.20%)]	 Loss: 0.000001
Training epoch 86 [53760/67107 (80.15%)]	 Loss: 0.000004
Training epoch 86 [54400/67107 (81.11%)]	 Loss: 0.000007
Training epoch 86 [55040/67107 (82.06%)]	 Loss: 0.000005
Training epoch 86 [55680/67107 (83.02%)]	 Loss: 0.000001
Training epoch 86 [56320/67107 (83.97%)]	 Loss: 0.000026
Training epoch 86 [56960/67107 (84.92%)]	 Loss: 0.000009
Training epoch 86 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 86 [58240/67107 (86.83%)]	 Loss: 0.000015
Training epoch 86 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 86 [59520/67107 (88.74%)]	 Loss: 0.000021
Training epoch 86 [60160/67107 (89.69%)]	 Loss: 0.000023
Training epoch 86 [60800/67107 (90.65%)]	 Loss: 0.000013
Training epoch 86 [61440/67107 (91.60%)]	 Loss: 0.000005
Training epoch 86 [62080/67107 (92.56%)]	 Loss: 0.000006
Training epoch 86 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 86 [63360/67107 (94.47%)]	 Loss: 0.000015
Training epoch 86 [64000/67107 (95.42%)]	 Loss: 0.000008
Training epoch 86 [64640/67107 (96.37%)]	 Loss: 0.000010
Training epoch 86 [65280/67107 (97.33%)]	 Loss: 0.000001
Training epoch 86 [65920/67107 (98.28%)]	 Loss: 0.000062
Training epoch 86 [66560/67107 (99.24%)]	 Loss: 0.000016
Test set: Average Loss: 0.006285
Training epoch 87 [0/67107 (0.00%)]	 Loss: 0.000011
Training epoch 87 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 87 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 87 [1920/67107 (2.86%)]	 Loss: 0.000003
Training epoch 87 [2560/67107 (3.82%)]	 Loss: 0.000018
Training epoch 87 [3200/67107 (4.77%)]	 Loss: 0.000011
Training epoch 87 [3840/67107 (5.73%)]	 Loss: 0.000012
Training epoch 87 [4480/67107 (6.68%)]	 Loss: 0.000019
Training epoch 87 [5120/67107 (7.63%)]	 Loss: 0.000049
Training epoch 87 [5760/67107 (8.59%)]	 Loss: 0.000003
Training epoch 87 [6400/67107 (9.54%)]	 Loss: 0.000006
Training epoch 87 [7040/67107 (10.50%)]	 Loss: 0.000001
Training epoch 87 [7680/67107 (11.45%)]	 Loss: 0.000008
Training epoch 87 [8320/67107 (12.40%)]	 Loss: 0.000004
Training epoch 87 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 87 [9600/67107 (14.31%)]	 Loss: 0.000000
Training epoch 87 [10240/67107 (15.27%)]	 Loss: 0.000016
Training epoch 87 [10880/67107 (16.22%)]	 Loss: 0.000007
Training epoch 87 [11520/67107 (17.18%)]	 Loss: 0.000006
Training epoch 87 [12160/67107 (18.13%)]	 Loss: 0.000014
Training epoch 87 [12800/67107 (19.08%)]	 Loss: 0.000012
Training epoch 87 [13440/67107 (20.04%)]	 Loss: 0.000035
Training epoch 87 [14080/67107 (20.99%)]	 Loss: 0.000011
Training epoch 87 [14720/67107 (21.95%)]	 Loss: 0.000003
Training epoch 87 [15360/67107 (22.90%)]	 Loss: 0.000004
Training epoch 87 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 87 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 87 [17280/67107 (25.76%)]	 Loss: 0.000008
Training epoch 87 [17920/67107 (26.72%)]	 Loss: 0.000002
Training epoch 87 [18560/67107 (27.67%)]	 Loss: 0.000015
Training epoch 87 [19200/67107 (28.63%)]	 Loss: 0.000020
Training epoch 87 [19840/67107 (29.58%)]	 Loss: 0.000011
Training epoch 87 [20480/67107 (30.53%)]	 Loss: 0.000004
Training epoch 87 [21120/67107 (31.49%)]	 Loss: 0.000007
Training epoch 87 [21760/67107 (32.44%)]	 Loss: 0.000012
Training epoch 87 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 87 [23040/67107 (34.35%)]	 Loss: 0.000003
Training epoch 87 [23680/67107 (35.31%)]	 Loss: 0.000001
Training epoch 87 [24320/67107 (36.26%)]	 Loss: 0.000000
Training epoch 87 [24960/67107 (37.21%)]	 Loss: 0.000004
Training epoch 87 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 87 [26240/67107 (39.12%)]	 Loss: 0.000005
Training epoch 87 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 87 [27520/67107 (41.03%)]	 Loss: 0.000019
Training epoch 87 [28160/67107 (41.98%)]	 Loss: 0.000015
Training epoch 87 [28800/67107 (42.94%)]	 Loss: 0.000009
Training epoch 87 [29440/67107 (43.89%)]	 Loss: 0.000007
Training epoch 87 [30080/67107 (44.85%)]	 Loss: 0.000007
Training epoch 87 [30720/67107 (45.80%)]	 Loss: 0.000006
Training epoch 87 [31360/67107 (46.76%)]	 Loss: 0.000002
Training epoch 87 [32000/67107 (47.71%)]	 Loss: 0.000018
Training epoch 87 [32640/67107 (48.66%)]	 Loss: 0.000013
Training epoch 87 [33280/67107 (49.62%)]	 Loss: 0.000006
Training epoch 87 [33920/67107 (50.57%)]	 Loss: 0.000009
Training epoch 87 [34560/67107 (51.53%)]	 Loss: 0.000022
Training epoch 87 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 87 [35840/67107 (53.44%)]	 Loss: 0.000013
Training epoch 87 [36480/67107 (54.39%)]	 Loss: 0.000001
Training epoch 87 [37120/67107 (55.34%)]	 Loss: 0.000006
Training epoch 87 [37760/67107 (56.30%)]	 Loss: 0.000002
Training epoch 87 [38400/67107 (57.25%)]	 Loss: 0.000006
Training epoch 87 [39040/67107 (58.21%)]	 Loss: 0.000055
Training epoch 87 [39680/67107 (59.16%)]	 Loss: 0.000011
Training epoch 87 [40320/67107 (60.11%)]	 Loss: 0.000009
Training epoch 87 [40960/67107 (61.07%)]	 Loss: 0.000017
Training epoch 87 [41600/67107 (62.02%)]	 Loss: 0.000039
Training epoch 87 [42240/67107 (62.98%)]	 Loss: 0.000008
Training epoch 87 [42880/67107 (63.93%)]	 Loss: 0.000012
Training epoch 87 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 87 [44160/67107 (65.84%)]	 Loss: 0.000005
Training epoch 87 [44800/67107 (66.79%)]	 Loss: 0.000010
Training epoch 87 [45440/67107 (67.75%)]	 Loss: 0.000003
Training epoch 87 [46080/67107 (68.70%)]	 Loss: 0.000007
Training epoch 87 [46720/67107 (69.66%)]	 Loss: 0.000004
Training epoch 87 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 87 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 87 [48640/67107 (72.52%)]	 Loss: 0.000005
Training epoch 87 [49280/67107 (73.47%)]	 Loss: 0.000019
Training epoch 87 [49920/67107 (74.43%)]	 Loss: 0.000006
Training epoch 87 [50560/67107 (75.38%)]	 Loss: 0.000010
Training epoch 87 [51200/67107 (76.34%)]	 Loss: 0.000011
Training epoch 87 [51840/67107 (77.29%)]	 Loss: 0.000007
Training epoch 87 [52480/67107 (78.24%)]	 Loss: 0.000002
Training epoch 87 [53120/67107 (79.20%)]	 Loss: 0.000001
Training epoch 87 [53760/67107 (80.15%)]	 Loss: 0.000003
Training epoch 87 [54400/67107 (81.11%)]	 Loss: 0.000008
Training epoch 87 [55040/67107 (82.06%)]	 Loss: 0.000010
Training epoch 87 [55680/67107 (83.02%)]	 Loss: 0.000001
Training epoch 87 [56320/67107 (83.97%)]	 Loss: 0.000026
Training epoch 87 [56960/67107 (84.92%)]	 Loss: 0.000002
Training epoch 87 [57600/67107 (85.88%)]	 Loss: 0.000009
Training epoch 87 [58240/67107 (86.83%)]	 Loss: 0.000013
Training epoch 87 [58880/67107 (87.79%)]	 Loss: 0.000020
Training epoch 87 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 87 [60160/67107 (89.69%)]	 Loss: 0.000050
Training epoch 87 [60800/67107 (90.65%)]	 Loss: 0.000008
Training epoch 87 [61440/67107 (91.60%)]	 Loss: 0.000029
Training epoch 87 [62080/67107 (92.56%)]	 Loss: 0.000032
Training epoch 87 [62720/67107 (93.51%)]	 Loss: 0.000011
Training epoch 87 [63360/67107 (94.47%)]	 Loss: 0.000029
Training epoch 87 [64000/67107 (95.42%)]	 Loss: 0.000034
Training epoch 87 [64640/67107 (96.37%)]	 Loss: 0.000015
Training epoch 87 [65280/67107 (97.33%)]	 Loss: 0.000011
Training epoch 87 [65920/67107 (98.28%)]	 Loss: 0.000003
Training epoch 87 [66560/67107 (99.24%)]	 Loss: 0.000004
Test set: Average Loss: 0.000123
Training epoch 88 [0/67107 (0.00%)]	 Loss: 0.000007
Training epoch 88 [640/67107 (0.95%)]	 Loss: 0.000002
Training epoch 88 [1280/67107 (1.91%)]	 Loss: 0.000006
Training epoch 88 [1920/67107 (2.86%)]	 Loss: 0.000016
Training epoch 88 [2560/67107 (3.82%)]	 Loss: 0.000013
Training epoch 88 [3200/67107 (4.77%)]	 Loss: 0.000003
Training epoch 88 [3840/67107 (5.73%)]	 Loss: 0.000013
Training epoch 88 [4480/67107 (6.68%)]	 Loss: 0.000034
Training epoch 88 [5120/67107 (7.63%)]	 Loss: 0.000005
Training epoch 88 [5760/67107 (8.59%)]	 Loss: 0.000005
Training epoch 88 [6400/67107 (9.54%)]	 Loss: 0.000004
Training epoch 88 [7040/67107 (10.50%)]	 Loss: 0.000012
Training epoch 88 [7680/67107 (11.45%)]	 Loss: 0.000013
Training epoch 88 [8320/67107 (12.40%)]	 Loss: 0.000013
Training epoch 88 [8960/67107 (13.36%)]	 Loss: 0.000019
Training epoch 88 [9600/67107 (14.31%)]	 Loss: 0.000018
Training epoch 88 [10240/67107 (15.27%)]	 Loss: 0.000012
Training epoch 88 [10880/67107 (16.22%)]	 Loss: 0.000007
Training epoch 88 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 88 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 88 [12800/67107 (19.08%)]	 Loss: 0.000006
Training epoch 88 [13440/67107 (20.04%)]	 Loss: 0.000024
Training epoch 88 [14080/67107 (20.99%)]	 Loss: 0.000035
Training epoch 88 [14720/67107 (21.95%)]	 Loss: 0.000014
Training epoch 88 [15360/67107 (22.90%)]	 Loss: 0.000016
Training epoch 88 [16000/67107 (23.85%)]	 Loss: 0.000020
Training epoch 88 [16640/67107 (24.81%)]	 Loss: 0.000019
Training epoch 88 [17280/67107 (25.76%)]	 Loss: 0.000002
Training epoch 88 [17920/67107 (26.72%)]	 Loss: 0.000002
Training epoch 88 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 88 [19200/67107 (28.63%)]	 Loss: 0.000004
Training epoch 88 [19840/67107 (29.58%)]	 Loss: 0.000018
Training epoch 88 [20480/67107 (30.53%)]	 Loss: 0.000003
Training epoch 88 [21120/67107 (31.49%)]	 Loss: 0.000008
Training epoch 88 [21760/67107 (32.44%)]	 Loss: 0.000009
Training epoch 88 [22400/67107 (33.40%)]	 Loss: 0.000007
Training epoch 88 [23040/67107 (34.35%)]	 Loss: 0.000012
Training epoch 88 [23680/67107 (35.31%)]	 Loss: 0.000014
Training epoch 88 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 88 [24960/67107 (37.21%)]	 Loss: 0.000004
Training epoch 88 [25600/67107 (38.17%)]	 Loss: 0.000003
Training epoch 88 [26240/67107 (39.12%)]	 Loss: 0.000005
Training epoch 88 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 88 [27520/67107 (41.03%)]	 Loss: 0.000023
Training epoch 88 [28160/67107 (41.98%)]	 Loss: 0.000014
Training epoch 88 [28800/67107 (42.94%)]	 Loss: 0.000015
Training epoch 88 [29440/67107 (43.89%)]	 Loss: 0.000009
Training epoch 88 [30080/67107 (44.85%)]	 Loss: 0.000004
Training epoch 88 [30720/67107 (45.80%)]	 Loss: 0.000008
Training epoch 88 [31360/67107 (46.76%)]	 Loss: 0.000004
Training epoch 88 [32000/67107 (47.71%)]	 Loss: 0.000004
Training epoch 88 [32640/67107 (48.66%)]	 Loss: 0.000002
Training epoch 88 [33280/67107 (49.62%)]	 Loss: 0.000008
Training epoch 88 [33920/67107 (50.57%)]	 Loss: 0.000017
Training epoch 88 [34560/67107 (51.53%)]	 Loss: 0.000008
Training epoch 88 [35200/67107 (52.48%)]	 Loss: 0.000039
Training epoch 88 [35840/67107 (53.44%)]	 Loss: 0.000007
Training epoch 88 [36480/67107 (54.39%)]	 Loss: 0.000007
Training epoch 88 [37120/67107 (55.34%)]	 Loss: 0.000047
Training epoch 88 [37760/67107 (56.30%)]	 Loss: 0.000040
Training epoch 88 [38400/67107 (57.25%)]	 Loss: 0.000008
Training epoch 88 [39040/67107 (58.21%)]	 Loss: 0.000006
Training epoch 88 [39680/67107 (59.16%)]	 Loss: 0.000041
Training epoch 88 [40320/67107 (60.11%)]	 Loss: 0.000007
Training epoch 88 [40960/67107 (61.07%)]	 Loss: 0.000009
Training epoch 88 [41600/67107 (62.02%)]	 Loss: 0.000009
Training epoch 88 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 88 [42880/67107 (63.93%)]	 Loss: 0.000008
Training epoch 88 [43520/67107 (64.89%)]	 Loss: 0.000006
Training epoch 88 [44160/67107 (65.84%)]	 Loss: 0.000006
Training epoch 88 [44800/67107 (66.79%)]	 Loss: 0.000002
Training epoch 88 [45440/67107 (67.75%)]	 Loss: 0.000005
Training epoch 88 [46080/67107 (68.70%)]	 Loss: 0.000001
Training epoch 88 [46720/67107 (69.66%)]	 Loss: 0.000018
Training epoch 88 [47360/67107 (70.61%)]	 Loss: 0.000023
Training epoch 88 [48000/67107 (71.56%)]	 Loss: 0.000005
Training epoch 88 [48640/67107 (72.52%)]	 Loss: 0.000012
Training epoch 88 [49280/67107 (73.47%)]	 Loss: 0.000006
Training epoch 88 [49920/67107 (74.43%)]	 Loss: 0.000004
Training epoch 88 [50560/67107 (75.38%)]	 Loss: 0.000009
Training epoch 88 [51200/67107 (76.34%)]	 Loss: 0.000004
Training epoch 88 [51840/67107 (77.29%)]	 Loss: 0.000004
Training epoch 88 [52480/67107 (78.24%)]	 Loss: 0.000017
Training epoch 88 [53120/67107 (79.20%)]	 Loss: 0.000012
Training epoch 88 [53760/67107 (80.15%)]	 Loss: 0.000017
Training epoch 88 [54400/67107 (81.11%)]	 Loss: 0.000015
Training epoch 88 [55040/67107 (82.06%)]	 Loss: 0.000011
Training epoch 88 [55680/67107 (83.02%)]	 Loss: 0.000024
Training epoch 88 [56320/67107 (83.97%)]	 Loss: 0.000010
Training epoch 88 [56960/67107 (84.92%)]	 Loss: 0.000005
Training epoch 88 [57600/67107 (85.88%)]	 Loss: 0.000000
Training epoch 88 [58240/67107 (86.83%)]	 Loss: 0.000003
Training epoch 88 [58880/67107 (87.79%)]	 Loss: 0.000006
Training epoch 88 [59520/67107 (88.74%)]	 Loss: 0.000005
Training epoch 88 [60160/67107 (89.69%)]	 Loss: 0.000001
Training epoch 88 [60800/67107 (90.65%)]	 Loss: 0.000015
Training epoch 88 [61440/67107 (91.60%)]	 Loss: 0.000030
Training epoch 88 [62080/67107 (92.56%)]	 Loss: 0.000069
Training epoch 88 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 88 [63360/67107 (94.47%)]	 Loss: 0.000006
Training epoch 88 [64000/67107 (95.42%)]	 Loss: 0.000001
Training epoch 88 [64640/67107 (96.37%)]	 Loss: 0.000011
Training epoch 88 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 88 [65920/67107 (98.28%)]	 Loss: 0.000005
Training epoch 88 [66560/67107 (99.24%)]	 Loss: 0.000005
Test set: Average Loss: 0.000137
Training epoch 89 [0/67107 (0.00%)]	 Loss: 0.000004
Training epoch 89 [640/67107 (0.95%)]	 Loss: 0.000005
Training epoch 89 [1280/67107 (1.91%)]	 Loss: 0.000007
Training epoch 89 [1920/67107 (2.86%)]	 Loss: 0.000008
Training epoch 89 [2560/67107 (3.82%)]	 Loss: 0.000011
Training epoch 89 [3200/67107 (4.77%)]	 Loss: 0.000008
Training epoch 89 [3840/67107 (5.73%)]	 Loss: 0.000005
Training epoch 89 [4480/67107 (6.68%)]	 Loss: 0.000007
Training epoch 89 [5120/67107 (7.63%)]	 Loss: 0.000011
Training epoch 89 [5760/67107 (8.59%)]	 Loss: 0.000003
Training epoch 89 [6400/67107 (9.54%)]	 Loss: 0.000003
Training epoch 89 [7040/67107 (10.50%)]	 Loss: 0.000086
Training epoch 89 [7680/67107 (11.45%)]	 Loss: 0.000005
Training epoch 89 [8320/67107 (12.40%)]	 Loss: 0.000012
Training epoch 89 [8960/67107 (13.36%)]	 Loss: 0.000005
Training epoch 89 [9600/67107 (14.31%)]	 Loss: 0.000015
Training epoch 89 [10240/67107 (15.27%)]	 Loss: 0.000032
Training epoch 89 [10880/67107 (16.22%)]	 Loss: 0.000012
Training epoch 89 [11520/67107 (17.18%)]	 Loss: 0.000001
Training epoch 89 [12160/67107 (18.13%)]	 Loss: 0.000001
Training epoch 89 [12800/67107 (19.08%)]	 Loss: 0.000003
Training epoch 89 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 89 [14080/67107 (20.99%)]	 Loss: 0.000003
Training epoch 89 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 89 [15360/67107 (22.90%)]	 Loss: 0.000003
Training epoch 89 [16000/67107 (23.85%)]	 Loss: 0.000010
Training epoch 89 [16640/67107 (24.81%)]	 Loss: 0.000005
Training epoch 89 [17280/67107 (25.76%)]	 Loss: 0.000020
Training epoch 89 [17920/67107 (26.72%)]	 Loss: 0.000008
Training epoch 89 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 89 [19200/67107 (28.63%)]	 Loss: 0.000016
Training epoch 89 [19840/67107 (29.58%)]	 Loss: 0.000006
Training epoch 89 [20480/67107 (30.53%)]	 Loss: 0.000013
Training epoch 89 [21120/67107 (31.49%)]	 Loss: 0.000007
Training epoch 89 [21760/67107 (32.44%)]	 Loss: 0.000003
Training epoch 89 [22400/67107 (33.40%)]	 Loss: 0.000005
Training epoch 89 [23040/67107 (34.35%)]	 Loss: 0.000001
Training epoch 89 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 89 [24320/67107 (36.26%)]	 Loss: 0.000029
Training epoch 89 [24960/67107 (37.21%)]	 Loss: 0.000019
Training epoch 89 [25600/67107 (38.17%)]	 Loss: 0.000029
Training epoch 89 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 89 [26880/67107 (40.08%)]	 Loss: 0.000013
Training epoch 89 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 89 [28160/67107 (41.98%)]	 Loss: 0.000007
Training epoch 89 [28800/67107 (42.94%)]	 Loss: 0.000003
Training epoch 89 [29440/67107 (43.89%)]	 Loss: 0.000014
Training epoch 89 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 89 [30720/67107 (45.80%)]	 Loss: 0.000033
Training epoch 89 [31360/67107 (46.76%)]	 Loss: 0.000005
Training epoch 89 [32000/67107 (47.71%)]	 Loss: 0.000019
Training epoch 89 [32640/67107 (48.66%)]	 Loss: 0.000008
Training epoch 89 [33280/67107 (49.62%)]	 Loss: 0.000025
Training epoch 89 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 89 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 89 [35200/67107 (52.48%)]	 Loss: 0.000002
Training epoch 89 [35840/67107 (53.44%)]	 Loss: 0.000005
Training epoch 89 [36480/67107 (54.39%)]	 Loss: 0.000002
Training epoch 89 [37120/67107 (55.34%)]	 Loss: 0.000001
Training epoch 89 [37760/67107 (56.30%)]	 Loss: 0.000002
Training epoch 89 [38400/67107 (57.25%)]	 Loss: 0.000028
Training epoch 89 [39040/67107 (58.21%)]	 Loss: 0.000006
Training epoch 89 [39680/67107 (59.16%)]	 Loss: 0.000011
Training epoch 89 [40320/67107 (60.11%)]	 Loss: 0.000009
Training epoch 89 [40960/67107 (61.07%)]	 Loss: 0.000006
Training epoch 89 [41600/67107 (62.02%)]	 Loss: 0.000020
Training epoch 89 [42240/67107 (62.98%)]	 Loss: 0.000008
Training epoch 89 [42880/67107 (63.93%)]	 Loss: 0.000009
Training epoch 89 [43520/67107 (64.89%)]	 Loss: 0.000018
Training epoch 89 [44160/67107 (65.84%)]	 Loss: 0.000010
Training epoch 89 [44800/67107 (66.79%)]	 Loss: 0.000013
Training epoch 89 [45440/67107 (67.75%)]	 Loss: 0.000009
Training epoch 89 [46080/67107 (68.70%)]	 Loss: 0.000010
Training epoch 89 [46720/67107 (69.66%)]	 Loss: 0.000005
Training epoch 89 [47360/67107 (70.61%)]	 Loss: 0.000004
Training epoch 89 [48000/67107 (71.56%)]	 Loss: 0.000008
Training epoch 89 [48640/67107 (72.52%)]	 Loss: 0.000026
Training epoch 89 [49280/67107 (73.47%)]	 Loss: 0.000004
Training epoch 89 [49920/67107 (74.43%)]	 Loss: 0.000002
Training epoch 89 [50560/67107 (75.38%)]	 Loss: 0.000001
Training epoch 89 [51200/67107 (76.34%)]	 Loss: 0.000005
Training epoch 89 [51840/67107 (77.29%)]	 Loss: 0.000001
Training epoch 89 [52480/67107 (78.24%)]	 Loss: 0.000003
Training epoch 89 [53120/67107 (79.20%)]	 Loss: 0.000010
Training epoch 89 [53760/67107 (80.15%)]	 Loss: 0.000001
Training epoch 89 [54400/67107 (81.11%)]	 Loss: 0.000023
Training epoch 89 [55040/67107 (82.06%)]	 Loss: 0.000006
Training epoch 89 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 89 [56320/67107 (83.97%)]	 Loss: 0.000005
Training epoch 89 [56960/67107 (84.92%)]	 Loss: 0.000002
Training epoch 89 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 89 [58240/67107 (86.83%)]	 Loss: 0.000032
Training epoch 89 [58880/67107 (87.79%)]	 Loss: 0.000009
Training epoch 89 [59520/67107 (88.74%)]	 Loss: 0.000016
Training epoch 89 [60160/67107 (89.69%)]	 Loss: 0.000013
Training epoch 89 [60800/67107 (90.65%)]	 Loss: 0.000035
Training epoch 89 [61440/67107 (91.60%)]	 Loss: 0.000005
Training epoch 89 [62080/67107 (92.56%)]	 Loss: 0.000006
Training epoch 89 [62720/67107 (93.51%)]	 Loss: 0.000003
Training epoch 89 [63360/67107 (94.47%)]	 Loss: 0.000003
Training epoch 89 [64000/67107 (95.42%)]	 Loss: 0.000002
Training epoch 89 [64640/67107 (96.37%)]	 Loss: 0.000002
Training epoch 89 [65280/67107 (97.33%)]	 Loss: 0.000003
Training epoch 89 [65920/67107 (98.28%)]	 Loss: 0.000023
Training epoch 89 [66560/67107 (99.24%)]	 Loss: 0.000008
Test set: Average Loss: 0.000838
Training epoch 90 [0/67107 (0.00%)]	 Loss: 0.000017
Training epoch 90 [640/67107 (0.95%)]	 Loss: 0.000002
Training epoch 90 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 90 [1920/67107 (2.86%)]	 Loss: 0.000004
Training epoch 90 [2560/67107 (3.82%)]	 Loss: 0.000007
Training epoch 90 [3200/67107 (4.77%)]	 Loss: 0.000008
Training epoch 90 [3840/67107 (5.73%)]	 Loss: 0.000004
Training epoch 90 [4480/67107 (6.68%)]	 Loss: 0.000008
Training epoch 90 [5120/67107 (7.63%)]	 Loss: 0.000028
Training epoch 90 [5760/67107 (8.59%)]	 Loss: 0.000025
Training epoch 90 [6400/67107 (9.54%)]	 Loss: 0.000021
Training epoch 90 [7040/67107 (10.50%)]	 Loss: 0.000037
Training epoch 90 [7680/67107 (11.45%)]	 Loss: 0.000003
Training epoch 90 [8320/67107 (12.40%)]	 Loss: 0.000045
Training epoch 90 [8960/67107 (13.36%)]	 Loss: 0.000001
Training epoch 90 [9600/67107 (14.31%)]	 Loss: 0.000069
Training epoch 90 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 90 [10880/67107 (16.22%)]	 Loss: 0.000011
Training epoch 90 [11520/67107 (17.18%)]	 Loss: 0.000028
Training epoch 90 [12160/67107 (18.13%)]	 Loss: 0.000019
Training epoch 90 [12800/67107 (19.08%)]	 Loss: 0.000019
Training epoch 90 [13440/67107 (20.04%)]	 Loss: 0.000018
Training epoch 90 [14080/67107 (20.99%)]	 Loss: 0.000005
Training epoch 90 [14720/67107 (21.95%)]	 Loss: 0.000123
Training epoch 90 [15360/67107 (22.90%)]	 Loss: 0.000024
Training epoch 90 [16000/67107 (23.85%)]	 Loss: 0.000011
Training epoch 90 [16640/67107 (24.81%)]	 Loss: 0.000002
Training epoch 90 [17280/67107 (25.76%)]	 Loss: 0.000003
Training epoch 90 [17920/67107 (26.72%)]	 Loss: 0.000037
Training epoch 90 [18560/67107 (27.67%)]	 Loss: 0.000014
Training epoch 90 [19200/67107 (28.63%)]	 Loss: 0.000054
Training epoch 90 [19840/67107 (29.58%)]	 Loss: 0.000015
Training epoch 90 [20480/67107 (30.53%)]	 Loss: 0.000011
Training epoch 90 [21120/67107 (31.49%)]	 Loss: 0.000029
Training epoch 90 [21760/67107 (32.44%)]	 Loss: 0.000003
Training epoch 90 [22400/67107 (33.40%)]	 Loss: 0.000090
Training epoch 90 [23040/67107 (34.35%)]	 Loss: 0.000013
Training epoch 90 [23680/67107 (35.31%)]	 Loss: 0.000006
Training epoch 90 [24320/67107 (36.26%)]	 Loss: 0.000007
Training epoch 90 [24960/67107 (37.21%)]	 Loss: 0.000003
Training epoch 90 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 90 [26240/67107 (39.12%)]	 Loss: 0.000003
Training epoch 90 [26880/67107 (40.08%)]	 Loss: 0.000001
Training epoch 90 [27520/67107 (41.03%)]	 Loss: 0.000015
Training epoch 90 [28160/67107 (41.98%)]	 Loss: 0.000005
Training epoch 90 [28800/67107 (42.94%)]	 Loss: 0.000019
Training epoch 90 [29440/67107 (43.89%)]	 Loss: 0.000020
Training epoch 90 [30080/67107 (44.85%)]	 Loss: 0.000062
Training epoch 90 [30720/67107 (45.80%)]	 Loss: 0.000005
Training epoch 90 [31360/67107 (46.76%)]	 Loss: 0.000026
Training epoch 90 [32000/67107 (47.71%)]	 Loss: 0.000003
Training epoch 90 [32640/67107 (48.66%)]	 Loss: 0.000001
Training epoch 90 [33280/67107 (49.62%)]	 Loss: 0.000006
Training epoch 90 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 90 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 90 [35200/67107 (52.48%)]	 Loss: 0.000021
Training epoch 90 [35840/67107 (53.44%)]	 Loss: 0.000005
Training epoch 90 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 90 [37120/67107 (55.34%)]	 Loss: 0.000006
Training epoch 90 [37760/67107 (56.30%)]	 Loss: 0.000013
Training epoch 90 [38400/67107 (57.25%)]	 Loss: 0.000003
Training epoch 90 [39040/67107 (58.21%)]	 Loss: 0.000007
Training epoch 90 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 90 [40320/67107 (60.11%)]	 Loss: 0.000006
Training epoch 90 [40960/67107 (61.07%)]	 Loss: 0.000023
Training epoch 90 [41600/67107 (62.02%)]	 Loss: 0.000038
Training epoch 90 [42240/67107 (62.98%)]	 Loss: 0.000016
Training epoch 90 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 90 [43520/67107 (64.89%)]	 Loss: 0.000005
Training epoch 90 [44160/67107 (65.84%)]	 Loss: 0.000002
Training epoch 90 [44800/67107 (66.79%)]	 Loss: 0.000008
Training epoch 90 [45440/67107 (67.75%)]	 Loss: 0.000006
Training epoch 90 [46080/67107 (68.70%)]	 Loss: 0.000002
Training epoch 90 [46720/67107 (69.66%)]	 Loss: 0.000010
Training epoch 90 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 90 [48000/67107 (71.56%)]	 Loss: 0.000011
Training epoch 90 [48640/67107 (72.52%)]	 Loss: 0.000009
Training epoch 90 [49280/67107 (73.47%)]	 Loss: 0.000052
Training epoch 90 [49920/67107 (74.43%)]	 Loss: 0.000006
Training epoch 90 [50560/67107 (75.38%)]	 Loss: 0.000001
Training epoch 90 [51200/67107 (76.34%)]	 Loss: 0.000001
Training epoch 90 [51840/67107 (77.29%)]	 Loss: 0.000001
Training epoch 90 [52480/67107 (78.24%)]	 Loss: 0.000001
Training epoch 90 [53120/67107 (79.20%)]	 Loss: 0.000012
Training epoch 90 [53760/67107 (80.15%)]	 Loss: 0.000008
Training epoch 90 [54400/67107 (81.11%)]	 Loss: 0.000002
Training epoch 90 [55040/67107 (82.06%)]	 Loss: 0.000004
Training epoch 90 [55680/67107 (83.02%)]	 Loss: 0.000011
Training epoch 90 [56320/67107 (83.97%)]	 Loss: 0.000005
Training epoch 90 [56960/67107 (84.92%)]	 Loss: 0.000005
Training epoch 90 [57600/67107 (85.88%)]	 Loss: 0.000003
Training epoch 90 [58240/67107 (86.83%)]	 Loss: 0.000007
Training epoch 90 [58880/67107 (87.79%)]	 Loss: 0.000001
Training epoch 90 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 90 [60160/67107 (89.69%)]	 Loss: 0.000002
Training epoch 90 [60800/67107 (90.65%)]	 Loss: 0.000027
Training epoch 90 [61440/67107 (91.60%)]	 Loss: 0.000016
Training epoch 90 [62080/67107 (92.56%)]	 Loss: 0.000007
Training epoch 90 [62720/67107 (93.51%)]	 Loss: 0.000011
Training epoch 90 [63360/67107 (94.47%)]	 Loss: 0.000025
Training epoch 90 [64000/67107 (95.42%)]	 Loss: 0.000011
Training epoch 90 [64640/67107 (96.37%)]	 Loss: 0.000010
Training epoch 90 [65280/67107 (97.33%)]	 Loss: 0.000002
Training epoch 90 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 90 [66560/67107 (99.24%)]	 Loss: 0.000003
Test set: Average Loss: 0.000165
Training epoch 91 [0/67107 (0.00%)]	 Loss: 0.000001
Training epoch 91 [640/67107 (0.95%)]	 Loss: 0.000007
Training epoch 91 [1280/67107 (1.91%)]	 Loss: 0.000018
Training epoch 91 [1920/67107 (2.86%)]	 Loss: 0.000005
Training epoch 91 [2560/67107 (3.82%)]	 Loss: 0.000010
Training epoch 91 [3200/67107 (4.77%)]	 Loss: 0.000027
Training epoch 91 [3840/67107 (5.73%)]	 Loss: 0.000009
Training epoch 91 [4480/67107 (6.68%)]	 Loss: 0.000026
Training epoch 91 [5120/67107 (7.63%)]	 Loss: 0.000005
Training epoch 91 [5760/67107 (8.59%)]	 Loss: 0.000003
Training epoch 91 [6400/67107 (9.54%)]	 Loss: 0.000002
Training epoch 91 [7040/67107 (10.50%)]	 Loss: 0.000001
Training epoch 91 [7680/67107 (11.45%)]	 Loss: 0.000002
Training epoch 91 [8320/67107 (12.40%)]	 Loss: 0.000008
Training epoch 91 [8960/67107 (13.36%)]	 Loss: 0.000017
Training epoch 91 [9600/67107 (14.31%)]	 Loss: 0.000008
Training epoch 91 [10240/67107 (15.27%)]	 Loss: 0.000008
Training epoch 91 [10880/67107 (16.22%)]	 Loss: 0.000024
Training epoch 91 [11520/67107 (17.18%)]	 Loss: 0.000010
Training epoch 91 [12160/67107 (18.13%)]	 Loss: 0.000021
Training epoch 91 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 91 [13440/67107 (20.04%)]	 Loss: 0.000008
Training epoch 91 [14080/67107 (20.99%)]	 Loss: 0.000001
Training epoch 91 [14720/67107 (21.95%)]	 Loss: 0.000001
Training epoch 91 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 91 [16000/67107 (23.85%)]	 Loss: 0.000015
Training epoch 91 [16640/67107 (24.81%)]	 Loss: 0.000010
Training epoch 91 [17280/67107 (25.76%)]	 Loss: 0.000021
Training epoch 91 [17920/67107 (26.72%)]	 Loss: 0.000003
Training epoch 91 [18560/67107 (27.67%)]	 Loss: 0.000006
Training epoch 91 [19200/67107 (28.63%)]	 Loss: 0.000006
Training epoch 91 [19840/67107 (29.58%)]	 Loss: 0.000019
Training epoch 91 [20480/67107 (30.53%)]	 Loss: 0.000006
Training epoch 91 [21120/67107 (31.49%)]	 Loss: 0.000009
Training epoch 91 [21760/67107 (32.44%)]	 Loss: 0.000010
Training epoch 91 [22400/67107 (33.40%)]	 Loss: 0.000006
Training epoch 91 [23040/67107 (34.35%)]	 Loss: 0.000042
Training epoch 91 [23680/67107 (35.31%)]	 Loss: 0.000014
Training epoch 91 [24320/67107 (36.26%)]	 Loss: 0.000022
Training epoch 91 [24960/67107 (37.21%)]	 Loss: 0.000012
Training epoch 91 [25600/67107 (38.17%)]	 Loss: 0.000003
Training epoch 91 [26240/67107 (39.12%)]	 Loss: 0.000002
Training epoch 91 [26880/67107 (40.08%)]	 Loss: 0.000001
Training epoch 91 [27520/67107 (41.03%)]	 Loss: 0.000003
Training epoch 91 [28160/67107 (41.98%)]	 Loss: 0.000002
Training epoch 91 [28800/67107 (42.94%)]	 Loss: 0.000005
Training epoch 91 [29440/67107 (43.89%)]	 Loss: 0.000006
Training epoch 91 [30080/67107 (44.85%)]	 Loss: 0.000003
Training epoch 91 [30720/67107 (45.80%)]	 Loss: 0.000021
Training epoch 91 [31360/67107 (46.76%)]	 Loss: 0.000010
Training epoch 91 [32000/67107 (47.71%)]	 Loss: 0.000006
Training epoch 91 [32640/67107 (48.66%)]	 Loss: 0.000006
Training epoch 91 [33280/67107 (49.62%)]	 Loss: 0.000030
Training epoch 91 [33920/67107 (50.57%)]	 Loss: 0.000006
Training epoch 91 [34560/67107 (51.53%)]	 Loss: 0.000002
Training epoch 91 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 91 [35840/67107 (53.44%)]	 Loss: 0.000003
Training epoch 91 [36480/67107 (54.39%)]	 Loss: 0.000001
Training epoch 91 [37120/67107 (55.34%)]	 Loss: 0.000011
Training epoch 91 [37760/67107 (56.30%)]	 Loss: 0.000003
Training epoch 91 [38400/67107 (57.25%)]	 Loss: 0.000003
Training epoch 91 [39040/67107 (58.21%)]	 Loss: 0.000010
Training epoch 91 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 91 [40320/67107 (60.11%)]	 Loss: 0.000009
Training epoch 91 [40960/67107 (61.07%)]	 Loss: 0.000015
Training epoch 91 [41600/67107 (62.02%)]	 Loss: 0.000019
Training epoch 91 [42240/67107 (62.98%)]	 Loss: 0.000026
Training epoch 91 [42880/67107 (63.93%)]	 Loss: 0.000023
Training epoch 91 [43520/67107 (64.89%)]	 Loss: 0.000008
Training epoch 91 [44160/67107 (65.84%)]	 Loss: 0.000026
Training epoch 91 [44800/67107 (66.79%)]	 Loss: 0.000008
Training epoch 91 [45440/67107 (67.75%)]	 Loss: 0.000002
Training epoch 91 [46080/67107 (68.70%)]	 Loss: 0.000004
Training epoch 91 [46720/67107 (69.66%)]	 Loss: 0.000017
Training epoch 91 [47360/67107 (70.61%)]	 Loss: 0.000003
Training epoch 91 [48000/67107 (71.56%)]	 Loss: 0.000001
Training epoch 91 [48640/67107 (72.52%)]	 Loss: 0.000003
Training epoch 91 [49280/67107 (73.47%)]	 Loss: 0.000001
Training epoch 91 [49920/67107 (74.43%)]	 Loss: 0.000004
Training epoch 91 [50560/67107 (75.38%)]	 Loss: 0.000008
Training epoch 91 [51200/67107 (76.34%)]	 Loss: 0.000006
Training epoch 91 [51840/67107 (77.29%)]	 Loss: 0.000001
Training epoch 91 [52480/67107 (78.24%)]	 Loss: 0.000010
Training epoch 91 [53120/67107 (79.20%)]	 Loss: 0.000011
Training epoch 91 [53760/67107 (80.15%)]	 Loss: 0.000002
Training epoch 91 [54400/67107 (81.11%)]	 Loss: 0.000016
Training epoch 91 [55040/67107 (82.06%)]	 Loss: 0.000009
Training epoch 91 [55680/67107 (83.02%)]	 Loss: 0.000002
Training epoch 91 [56320/67107 (83.97%)]	 Loss: 0.000003
Training epoch 91 [56960/67107 (84.92%)]	 Loss: 0.000003
Training epoch 91 [57600/67107 (85.88%)]	 Loss: 0.000007
Training epoch 91 [58240/67107 (86.83%)]	 Loss: 0.000033
Training epoch 91 [58880/67107 (87.79%)]	 Loss: 0.000012
Training epoch 91 [59520/67107 (88.74%)]	 Loss: 0.000005
Training epoch 91 [60160/67107 (89.69%)]	 Loss: 0.000008
Training epoch 91 [60800/67107 (90.65%)]	 Loss: 0.000005
Training epoch 91 [61440/67107 (91.60%)]	 Loss: 0.000006
Training epoch 91 [62080/67107 (92.56%)]	 Loss: 0.000002
Training epoch 91 [62720/67107 (93.51%)]	 Loss: 0.000003
Training epoch 91 [63360/67107 (94.47%)]	 Loss: 0.000014
Training epoch 91 [64000/67107 (95.42%)]	 Loss: 0.000009
Training epoch 91 [64640/67107 (96.37%)]	 Loss: 0.000033
Training epoch 91 [65280/67107 (97.33%)]	 Loss: 0.000003
Training epoch 91 [65920/67107 (98.28%)]	 Loss: 0.000019
Training epoch 91 [66560/67107 (99.24%)]	 Loss: 0.000023
Test set: Average Loss: 0.000152
Training epoch 92 [0/67107 (0.00%)]	 Loss: 0.000008
Training epoch 92 [640/67107 (0.95%)]	 Loss: 0.000016
Training epoch 92 [1280/67107 (1.91%)]	 Loss: 0.000051
Training epoch 92 [1920/67107 (2.86%)]	 Loss: 0.000004
Training epoch 92 [2560/67107 (3.82%)]	 Loss: 0.000015
Training epoch 92 [3200/67107 (4.77%)]	 Loss: 0.000005
Training epoch 92 [3840/67107 (5.73%)]	 Loss: 0.000004
Training epoch 92 [4480/67107 (6.68%)]	 Loss: 0.000069
Training epoch 92 [5120/67107 (7.63%)]	 Loss: 0.000022
Training epoch 92 [5760/67107 (8.59%)]	 Loss: 0.000027
Training epoch 92 [6400/67107 (9.54%)]	 Loss: 0.000020
Training epoch 92 [7040/67107 (10.50%)]	 Loss: 0.000068
Training epoch 92 [7680/67107 (11.45%)]	 Loss: 0.000020
Training epoch 92 [8320/67107 (12.40%)]	 Loss: 0.000006
Training epoch 92 [8960/67107 (13.36%)]	 Loss: 0.000005
Training epoch 92 [9600/67107 (14.31%)]	 Loss: 0.000008
Training epoch 92 [10240/67107 (15.27%)]	 Loss: 0.000039
Training epoch 92 [10880/67107 (16.22%)]	 Loss: 0.000012
Training epoch 92 [11520/67107 (17.18%)]	 Loss: 0.000011
Training epoch 92 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 92 [12800/67107 (19.08%)]	 Loss: 0.000029
Training epoch 92 [13440/67107 (20.04%)]	 Loss: 0.000034
Training epoch 92 [14080/67107 (20.99%)]	 Loss: 0.000016
Training epoch 92 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 92 [15360/67107 (22.90%)]	 Loss: 0.000003
Training epoch 92 [16000/67107 (23.85%)]	 Loss: 0.000002
Training epoch 92 [16640/67107 (24.81%)]	 Loss: 0.000004
Training epoch 92 [17280/67107 (25.76%)]	 Loss: 0.000002
Training epoch 92 [17920/67107 (26.72%)]	 Loss: 0.000004
Training epoch 92 [18560/67107 (27.67%)]	 Loss: 0.000003
Training epoch 92 [19200/67107 (28.63%)]	 Loss: 0.000011
Training epoch 92 [19840/67107 (29.58%)]	 Loss: 0.000013
Training epoch 92 [20480/67107 (30.53%)]	 Loss: 0.000017
Training epoch 92 [21120/67107 (31.49%)]	 Loss: 0.000007
Training epoch 92 [21760/67107 (32.44%)]	 Loss: 0.000005
Training epoch 92 [22400/67107 (33.40%)]	 Loss: 0.000018
Training epoch 92 [23040/67107 (34.35%)]	 Loss: 0.000034
Training epoch 92 [23680/67107 (35.31%)]	 Loss: 0.000004
Training epoch 92 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 92 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 92 [25600/67107 (38.17%)]	 Loss: 0.000009
Training epoch 92 [26240/67107 (39.12%)]	 Loss: 0.000002
Training epoch 92 [26880/67107 (40.08%)]	 Loss: 0.000009
Training epoch 92 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 92 [28160/67107 (41.98%)]	 Loss: 0.000013
Training epoch 92 [28800/67107 (42.94%)]	 Loss: 0.000003
Training epoch 92 [29440/67107 (43.89%)]	 Loss: 0.000004
Training epoch 92 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 92 [30720/67107 (45.80%)]	 Loss: 0.000017
Training epoch 92 [31360/67107 (46.76%)]	 Loss: 0.000010
Training epoch 92 [32000/67107 (47.71%)]	 Loss: 0.000026
Training epoch 92 [32640/67107 (48.66%)]	 Loss: 0.000020
Training epoch 92 [33280/67107 (49.62%)]	 Loss: 0.000008
Training epoch 92 [33920/67107 (50.57%)]	 Loss: 0.000034
Training epoch 92 [34560/67107 (51.53%)]	 Loss: 0.000006
Training epoch 92 [35200/67107 (52.48%)]	 Loss: 0.000003
Training epoch 92 [35840/67107 (53.44%)]	 Loss: 0.000003
Training epoch 92 [36480/67107 (54.39%)]	 Loss: 0.000002
Training epoch 92 [37120/67107 (55.34%)]	 Loss: 0.000037
Training epoch 92 [37760/67107 (56.30%)]	 Loss: 0.000035
Training epoch 92 [38400/67107 (57.25%)]	 Loss: 0.000010
Training epoch 92 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 92 [39680/67107 (59.16%)]	 Loss: 0.000045
Training epoch 92 [40320/67107 (60.11%)]	 Loss: 0.000006
Training epoch 92 [40960/67107 (61.07%)]	 Loss: 0.000025
Training epoch 92 [41600/67107 (62.02%)]	 Loss: 0.000031
Training epoch 92 [42240/67107 (62.98%)]	 Loss: 0.000009
Training epoch 92 [42880/67107 (63.93%)]	 Loss: 0.000004
Training epoch 92 [43520/67107 (64.89%)]	 Loss: 0.000006
Training epoch 92 [44160/67107 (65.84%)]	 Loss: 0.000011
Training epoch 92 [44800/67107 (66.79%)]	 Loss: 0.000003
Training epoch 92 [45440/67107 (67.75%)]	 Loss: 0.000002
Training epoch 92 [46080/67107 (68.70%)]	 Loss: 0.000003
Training epoch 92 [46720/67107 (69.66%)]	 Loss: 0.000000
Training epoch 92 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 92 [48000/67107 (71.56%)]	 Loss: 0.000008
Training epoch 92 [48640/67107 (72.52%)]	 Loss: 0.000005
Training epoch 92 [49280/67107 (73.47%)]	 Loss: 0.000008
Training epoch 92 [49920/67107 (74.43%)]	 Loss: 0.000002
Training epoch 92 [50560/67107 (75.38%)]	 Loss: 0.000003
Training epoch 92 [51200/67107 (76.34%)]	 Loss: 0.000009
Training epoch 92 [51840/67107 (77.29%)]	 Loss: 0.000008
Training epoch 92 [52480/67107 (78.24%)]	 Loss: 0.000020
Training epoch 92 [53120/67107 (79.20%)]	 Loss: 0.000014
Training epoch 92 [53760/67107 (80.15%)]	 Loss: 0.000009
Training epoch 92 [54400/67107 (81.11%)]	 Loss: 0.000008
Training epoch 92 [55040/67107 (82.06%)]	 Loss: 0.000004
Training epoch 92 [55680/67107 (83.02%)]	 Loss: 0.000023
Training epoch 92 [56320/67107 (83.97%)]	 Loss: 0.000058
Training epoch 92 [56960/67107 (84.92%)]	 Loss: 0.000010
Training epoch 92 [57600/67107 (85.88%)]	 Loss: 0.000001
Training epoch 92 [58240/67107 (86.83%)]	 Loss: 0.000002
Training epoch 92 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 92 [59520/67107 (88.74%)]	 Loss: 0.000014
Training epoch 92 [60160/67107 (89.69%)]	 Loss: 0.000001
Training epoch 92 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 92 [61440/67107 (91.60%)]	 Loss: 0.000004
Training epoch 92 [62080/67107 (92.56%)]	 Loss: 0.000009
Training epoch 92 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 92 [63360/67107 (94.47%)]	 Loss: 0.000006
Training epoch 92 [64000/67107 (95.42%)]	 Loss: 0.000005
Training epoch 92 [64640/67107 (96.37%)]	 Loss: 0.000003
Training epoch 92 [65280/67107 (97.33%)]	 Loss: 0.000005
Training epoch 92 [65920/67107 (98.28%)]	 Loss: 0.000004
Training epoch 92 [66560/67107 (99.24%)]	 Loss: 0.000008
Test set: Average Loss: 0.001531
Training epoch 93 [0/67107 (0.00%)]	 Loss: 0.000003
Training epoch 93 [640/67107 (0.95%)]	 Loss: 0.000001
Training epoch 93 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 93 [1920/67107 (2.86%)]	 Loss: 0.000004
Training epoch 93 [2560/67107 (3.82%)]	 Loss: 0.000010
Training epoch 93 [3200/67107 (4.77%)]	 Loss: 0.000012
Training epoch 93 [3840/67107 (5.73%)]	 Loss: 0.000007
Training epoch 93 [4480/67107 (6.68%)]	 Loss: 0.000014
Training epoch 93 [5120/67107 (7.63%)]	 Loss: 0.000011
Training epoch 93 [5760/67107 (8.59%)]	 Loss: 0.000008
Training epoch 93 [6400/67107 (9.54%)]	 Loss: 0.000009
Training epoch 93 [7040/67107 (10.50%)]	 Loss: 0.000006
Training epoch 93 [7680/67107 (11.45%)]	 Loss: 0.000016
Training epoch 93 [8320/67107 (12.40%)]	 Loss: 0.000002
Training epoch 93 [8960/67107 (13.36%)]	 Loss: 0.000007
Training epoch 93 [9600/67107 (14.31%)]	 Loss: 0.000001
Training epoch 93 [10240/67107 (15.27%)]	 Loss: 0.000007
Training epoch 93 [10880/67107 (16.22%)]	 Loss: 0.000001
Training epoch 93 [11520/67107 (17.18%)]	 Loss: 0.000004
Training epoch 93 [12160/67107 (18.13%)]	 Loss: 0.000010
Training epoch 93 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 93 [13440/67107 (20.04%)]	 Loss: 0.000003
Training epoch 93 [14080/67107 (20.99%)]	 Loss: 0.000001
Training epoch 93 [14720/67107 (21.95%)]	 Loss: 0.000015
Training epoch 93 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 93 [16000/67107 (23.85%)]	 Loss: 0.000002
Training epoch 93 [16640/67107 (24.81%)]	 Loss: 0.000015
Training epoch 93 [17280/67107 (25.76%)]	 Loss: 0.000003
Training epoch 93 [17920/67107 (26.72%)]	 Loss: 0.000010
Training epoch 93 [18560/67107 (27.67%)]	 Loss: 0.000028
Training epoch 93 [19200/67107 (28.63%)]	 Loss: 0.000003
Training epoch 93 [19840/67107 (29.58%)]	 Loss: 0.000006
Training epoch 93 [20480/67107 (30.53%)]	 Loss: 0.000008
Training epoch 93 [21120/67107 (31.49%)]	 Loss: 0.000024
Training epoch 93 [21760/67107 (32.44%)]	 Loss: 0.000021
Training epoch 93 [22400/67107 (33.40%)]	 Loss: 0.000020
Training epoch 93 [23040/67107 (34.35%)]	 Loss: 0.000015
Training epoch 93 [23680/67107 (35.31%)]	 Loss: 0.000030
Training epoch 93 [24320/67107 (36.26%)]	 Loss: 0.000007
Training epoch 93 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 93 [25600/67107 (38.17%)]	 Loss: 0.000001
Training epoch 93 [26240/67107 (39.12%)]	 Loss: 0.000008
Training epoch 93 [26880/67107 (40.08%)]	 Loss: 0.000011
Training epoch 93 [27520/67107 (41.03%)]	 Loss: 0.000021
Training epoch 93 [28160/67107 (41.98%)]	 Loss: 0.000006
Training epoch 93 [28800/67107 (42.94%)]	 Loss: 0.000005
Training epoch 93 [29440/67107 (43.89%)]	 Loss: 0.000002
Training epoch 93 [30080/67107 (44.85%)]	 Loss: 0.000004
Training epoch 93 [30720/67107 (45.80%)]	 Loss: 0.000002
Training epoch 93 [31360/67107 (46.76%)]	 Loss: 0.000003
Training epoch 93 [32000/67107 (47.71%)]	 Loss: 0.000010
Training epoch 93 [32640/67107 (48.66%)]	 Loss: 0.000013
Training epoch 93 [33280/67107 (49.62%)]	 Loss: 0.000006
Training epoch 93 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 93 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 93 [35200/67107 (52.48%)]	 Loss: 0.000003
Training epoch 93 [35840/67107 (53.44%)]	 Loss: 0.000002
Training epoch 93 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 93 [37120/67107 (55.34%)]	 Loss: 0.000030
Training epoch 93 [37760/67107 (56.30%)]	 Loss: 0.000005
Training epoch 93 [38400/67107 (57.25%)]	 Loss: 0.000002
Training epoch 93 [39040/67107 (58.21%)]	 Loss: 0.000002
Training epoch 93 [39680/67107 (59.16%)]	 Loss: 0.000028
Training epoch 93 [40320/67107 (60.11%)]	 Loss: 0.000006
Training epoch 93 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 93 [41600/67107 (62.02%)]	 Loss: 0.000004
Training epoch 93 [42240/67107 (62.98%)]	 Loss: 0.000004
Training epoch 93 [42880/67107 (63.93%)]	 Loss: 0.000007
Training epoch 93 [43520/67107 (64.89%)]	 Loss: 0.000017
Training epoch 93 [44160/67107 (65.84%)]	 Loss: 0.000007
Training epoch 93 [44800/67107 (66.79%)]	 Loss: 0.000022
Training epoch 93 [45440/67107 (67.75%)]	 Loss: 0.000037
Training epoch 93 [46080/67107 (68.70%)]	 Loss: 0.000027
Training epoch 93 [46720/67107 (69.66%)]	 Loss: 0.000060
Training epoch 93 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 93 [48000/67107 (71.56%)]	 Loss: 0.000007
Training epoch 93 [48640/67107 (72.52%)]	 Loss: 0.000016
Training epoch 93 [49280/67107 (73.47%)]	 Loss: 0.000006
Training epoch 93 [49920/67107 (74.43%)]	 Loss: 0.000004
Training epoch 93 [50560/67107 (75.38%)]	 Loss: 0.000002
Training epoch 93 [51200/67107 (76.34%)]	 Loss: 0.000003
Training epoch 93 [51840/67107 (77.29%)]	 Loss: 0.000007
Training epoch 93 [52480/67107 (78.24%)]	 Loss: 0.000001
Training epoch 93 [53120/67107 (79.20%)]	 Loss: 0.000003
Training epoch 93 [53760/67107 (80.15%)]	 Loss: 0.000018
Training epoch 93 [54400/67107 (81.11%)]	 Loss: 0.000005
Training epoch 93 [55040/67107 (82.06%)]	 Loss: 0.000008
Training epoch 93 [55680/67107 (83.02%)]	 Loss: 0.000009
Training epoch 93 [56320/67107 (83.97%)]	 Loss: 0.000003
Training epoch 93 [56960/67107 (84.92%)]	 Loss: 0.000012
Training epoch 93 [57600/67107 (85.88%)]	 Loss: 0.000017
Training epoch 93 [58240/67107 (86.83%)]	 Loss: 0.000007
Training epoch 93 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 93 [59520/67107 (88.74%)]	 Loss: 0.000003
Training epoch 93 [60160/67107 (89.69%)]	 Loss: 0.000004
Training epoch 93 [60800/67107 (90.65%)]	 Loss: 0.000014
Training epoch 93 [61440/67107 (91.60%)]	 Loss: 0.000008
Training epoch 93 [62080/67107 (92.56%)]	 Loss: 0.000007
Training epoch 93 [62720/67107 (93.51%)]	 Loss: 0.000021
Training epoch 93 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 93 [64000/67107 (95.42%)]	 Loss: 0.000007
Training epoch 93 [64640/67107 (96.37%)]	 Loss: 0.000006
Training epoch 93 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 93 [65920/67107 (98.28%)]	 Loss: 0.000004
Training epoch 93 [66560/67107 (99.24%)]	 Loss: 0.000006
Test set: Average Loss: 0.000157
Training epoch 94 [0/67107 (0.00%)]	 Loss: 0.000039
Training epoch 94 [640/67107 (0.95%)]	 Loss: 0.000008
Training epoch 94 [1280/67107 (1.91%)]	 Loss: 0.000002
Training epoch 94 [1920/67107 (2.86%)]	 Loss: 0.000009
Training epoch 94 [2560/67107 (3.82%)]	 Loss: 0.000008
Training epoch 94 [3200/67107 (4.77%)]	 Loss: 0.000019
Training epoch 94 [3840/67107 (5.73%)]	 Loss: 0.000006
Training epoch 94 [4480/67107 (6.68%)]	 Loss: 0.000002
Training epoch 94 [5120/67107 (7.63%)]	 Loss: 0.000006
Training epoch 94 [5760/67107 (8.59%)]	 Loss: 0.000034
Training epoch 94 [6400/67107 (9.54%)]	 Loss: 0.000002
Training epoch 94 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 94 [7680/67107 (11.45%)]	 Loss: 0.000019
Training epoch 94 [8320/67107 (12.40%)]	 Loss: 0.000005
Training epoch 94 [8960/67107 (13.36%)]	 Loss: 0.000006
Training epoch 94 [9600/67107 (14.31%)]	 Loss: 0.000001
Training epoch 94 [10240/67107 (15.27%)]	 Loss: 0.000006
Training epoch 94 [10880/67107 (16.22%)]	 Loss: 0.000008
Training epoch 94 [11520/67107 (17.18%)]	 Loss: 0.000006
Training epoch 94 [12160/67107 (18.13%)]	 Loss: 0.000003
Training epoch 94 [12800/67107 (19.08%)]	 Loss: 0.000055
Training epoch 94 [13440/67107 (20.04%)]	 Loss: 0.000008
Training epoch 94 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 94 [14720/67107 (21.95%)]	 Loss: 0.000050
Training epoch 94 [15360/67107 (22.90%)]	 Loss: 0.000034
Training epoch 94 [16000/67107 (23.85%)]	 Loss: 0.000039
Training epoch 94 [16640/67107 (24.81%)]	 Loss: 0.000019
Training epoch 94 [17280/67107 (25.76%)]	 Loss: 0.000005
Training epoch 94 [17920/67107 (26.72%)]	 Loss: 0.000003
Training epoch 94 [18560/67107 (27.67%)]	 Loss: 0.000003
Training epoch 94 [19200/67107 (28.63%)]	 Loss: 0.000004
Training epoch 94 [19840/67107 (29.58%)]	 Loss: 0.000024
Training epoch 94 [20480/67107 (30.53%)]	 Loss: 0.000012
Training epoch 94 [21120/67107 (31.49%)]	 Loss: 0.000002
Training epoch 94 [21760/67107 (32.44%)]	 Loss: 0.000017
Training epoch 94 [22400/67107 (33.40%)]	 Loss: 0.000010
Training epoch 94 [23040/67107 (34.35%)]	 Loss: 0.000043
Training epoch 94 [23680/67107 (35.31%)]	 Loss: 0.000008
Training epoch 94 [24320/67107 (36.26%)]	 Loss: 0.000001
Training epoch 94 [24960/67107 (37.21%)]	 Loss: 0.000005
Training epoch 94 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 94 [26240/67107 (39.12%)]	 Loss: 0.000002
Training epoch 94 [26880/67107 (40.08%)]	 Loss: 0.000008
Training epoch 94 [27520/67107 (41.03%)]	 Loss: 0.000009
Training epoch 94 [28160/67107 (41.98%)]	 Loss: 0.000018
Training epoch 94 [28800/67107 (42.94%)]	 Loss: 0.000007
Training epoch 94 [29440/67107 (43.89%)]	 Loss: 0.000012
Training epoch 94 [30080/67107 (44.85%)]	 Loss: 0.000022
Training epoch 94 [30720/67107 (45.80%)]	 Loss: 0.000004
Training epoch 94 [31360/67107 (46.76%)]	 Loss: 0.000015
Training epoch 94 [32000/67107 (47.71%)]	 Loss: 0.000002
Training epoch 94 [32640/67107 (48.66%)]	 Loss: 0.000002
Training epoch 94 [33280/67107 (49.62%)]	 Loss: 0.000001
Training epoch 94 [33920/67107 (50.57%)]	 Loss: 0.000009
Training epoch 94 [34560/67107 (51.53%)]	 Loss: 0.000030
Training epoch 94 [35200/67107 (52.48%)]	 Loss: 0.000015
Training epoch 94 [35840/67107 (53.44%)]	 Loss: 0.000007
Training epoch 94 [36480/67107 (54.39%)]	 Loss: 0.000025
Training epoch 94 [37120/67107 (55.34%)]	 Loss: 0.000003
Training epoch 94 [37760/67107 (56.30%)]	 Loss: 0.000010
Training epoch 94 [38400/67107 (57.25%)]	 Loss: 0.000009
Training epoch 94 [39040/67107 (58.21%)]	 Loss: 0.000029
Training epoch 94 [39680/67107 (59.16%)]	 Loss: 0.000004
Training epoch 94 [40320/67107 (60.11%)]	 Loss: 0.000012
Training epoch 94 [40960/67107 (61.07%)]	 Loss: 0.000014
Training epoch 94 [41600/67107 (62.02%)]	 Loss: 0.000006
Training epoch 94 [42240/67107 (62.98%)]	 Loss: 0.000013
Training epoch 94 [42880/67107 (63.93%)]	 Loss: 0.000002
Training epoch 94 [43520/67107 (64.89%)]	 Loss: 0.000007
Training epoch 94 [44160/67107 (65.84%)]	 Loss: 0.000003
Training epoch 94 [44800/67107 (66.79%)]	 Loss: 0.000010
Training epoch 94 [45440/67107 (67.75%)]	 Loss: 0.000004
Training epoch 94 [46080/67107 (68.70%)]	 Loss: 0.000003
Training epoch 94 [46720/67107 (69.66%)]	 Loss: 0.000001
Training epoch 94 [47360/67107 (70.61%)]	 Loss: 0.000004
Training epoch 94 [48000/67107 (71.56%)]	 Loss: 0.000002
Training epoch 94 [48640/67107 (72.52%)]	 Loss: 0.000002
Training epoch 94 [49280/67107 (73.47%)]	 Loss: 0.000001
Training epoch 94 [49920/67107 (74.43%)]	 Loss: 0.000016
Training epoch 94 [50560/67107 (75.38%)]	 Loss: 0.000001
Training epoch 94 [51200/67107 (76.34%)]	 Loss: 0.000002
Training epoch 94 [51840/67107 (77.29%)]	 Loss: 0.000005
Training epoch 94 [52480/67107 (78.24%)]	 Loss: 0.000008
Training epoch 94 [53120/67107 (79.20%)]	 Loss: 0.000003
Training epoch 94 [53760/67107 (80.15%)]	 Loss: 0.000039
Training epoch 94 [54400/67107 (81.11%)]	 Loss: 0.000003
Training epoch 94 [55040/67107 (82.06%)]	 Loss: 0.000010
Training epoch 94 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 94 [56320/67107 (83.97%)]	 Loss: 0.000022
Training epoch 94 [56960/67107 (84.92%)]	 Loss: 0.000011
Training epoch 94 [57600/67107 (85.88%)]	 Loss: 0.000002
Training epoch 94 [58240/67107 (86.83%)]	 Loss: 0.000008
Training epoch 94 [58880/67107 (87.79%)]	 Loss: 0.000002
Training epoch 94 [59520/67107 (88.74%)]	 Loss: 0.000003
Training epoch 94 [60160/67107 (89.69%)]	 Loss: 0.000001
Training epoch 94 [60800/67107 (90.65%)]	 Loss: 0.000001
Training epoch 94 [61440/67107 (91.60%)]	 Loss: 0.000014
Training epoch 94 [62080/67107 (92.56%)]	 Loss: 0.000002
Training epoch 94 [62720/67107 (93.51%)]	 Loss: 0.000007
Training epoch 94 [63360/67107 (94.47%)]	 Loss: 0.000004
Training epoch 94 [64000/67107 (95.42%)]	 Loss: 0.000017
Training epoch 94 [64640/67107 (96.37%)]	 Loss: 0.000011
Training epoch 94 [65280/67107 (97.33%)]	 Loss: 0.000019
Training epoch 94 [65920/67107 (98.28%)]	 Loss: 0.000003
Training epoch 94 [66560/67107 (99.24%)]	 Loss: 0.000005
Test set: Average Loss: 0.001042
Training epoch 95 [0/67107 (0.00%)]	 Loss: 0.000001
Training epoch 95 [640/67107 (0.95%)]	 Loss: 0.000003
Training epoch 95 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 95 [1920/67107 (2.86%)]	 Loss: 0.000011
Training epoch 95 [2560/67107 (3.82%)]	 Loss: 0.000012
Training epoch 95 [3200/67107 (4.77%)]	 Loss: 0.000015
Training epoch 95 [3840/67107 (5.73%)]	 Loss: 0.000038
Training epoch 95 [4480/67107 (6.68%)]	 Loss: 0.000013
Training epoch 95 [5120/67107 (7.63%)]	 Loss: 0.000011
Training epoch 95 [5760/67107 (8.59%)]	 Loss: 0.000001
Training epoch 95 [6400/67107 (9.54%)]	 Loss: 0.000004
Training epoch 95 [7040/67107 (10.50%)]	 Loss: 0.000002
Training epoch 95 [7680/67107 (11.45%)]	 Loss: 0.000002
Training epoch 95 [8320/67107 (12.40%)]	 Loss: 0.000001
Training epoch 95 [8960/67107 (13.36%)]	 Loss: 0.000008
Training epoch 95 [9600/67107 (14.31%)]	 Loss: 0.000002
Training epoch 95 [10240/67107 (15.27%)]	 Loss: 0.000009
Training epoch 95 [10880/67107 (16.22%)]	 Loss: 0.000011
Training epoch 95 [11520/67107 (17.18%)]	 Loss: 0.000002
Training epoch 95 [12160/67107 (18.13%)]	 Loss: 0.000036
Training epoch 95 [12800/67107 (19.08%)]	 Loss: 0.000015
Training epoch 95 [13440/67107 (20.04%)]	 Loss: 0.000009
Training epoch 95 [14080/67107 (20.99%)]	 Loss: 0.000006
Training epoch 95 [14720/67107 (21.95%)]	 Loss: 0.000003
Training epoch 95 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 95 [16000/67107 (23.85%)]	 Loss: 0.000001
Training epoch 95 [16640/67107 (24.81%)]	 Loss: 0.000010
Training epoch 95 [17280/67107 (25.76%)]	 Loss: 0.000010
Training epoch 95 [17920/67107 (26.72%)]	 Loss: 0.000004
Training epoch 95 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 95 [19200/67107 (28.63%)]	 Loss: 0.000003
Training epoch 95 [19840/67107 (29.58%)]	 Loss: 0.000013
Training epoch 95 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 95 [21120/67107 (31.49%)]	 Loss: 0.000002
Training epoch 95 [21760/67107 (32.44%)]	 Loss: 0.000001
Training epoch 95 [22400/67107 (33.40%)]	 Loss: 0.000002
Training epoch 95 [23040/67107 (34.35%)]	 Loss: 0.000013
Training epoch 95 [23680/67107 (35.31%)]	 Loss: 0.000003
Training epoch 95 [24320/67107 (36.26%)]	 Loss: 0.000003
Training epoch 95 [24960/67107 (37.21%)]	 Loss: 0.000008
Training epoch 95 [25600/67107 (38.17%)]	 Loss: 0.000009
Training epoch 95 [26240/67107 (39.12%)]	 Loss: 0.000024
Training epoch 95 [26880/67107 (40.08%)]	 Loss: 0.000009
Training epoch 95 [27520/67107 (41.03%)]	 Loss: 0.000010
Training epoch 95 [28160/67107 (41.98%)]	 Loss: 0.000008
Training epoch 95 [28800/67107 (42.94%)]	 Loss: 0.000017
Training epoch 95 [29440/67107 (43.89%)]	 Loss: 0.000012
Training epoch 95 [30080/67107 (44.85%)]	 Loss: 0.000007
Training epoch 95 [30720/67107 (45.80%)]	 Loss: 0.000003
Training epoch 95 [31360/67107 (46.76%)]	 Loss: 0.000001
Training epoch 95 [32000/67107 (47.71%)]	 Loss: 0.000053
Training epoch 95 [32640/67107 (48.66%)]	 Loss: 0.000007
Training epoch 95 [33280/67107 (49.62%)]	 Loss: 0.000003
Training epoch 95 [33920/67107 (50.57%)]	 Loss: 0.000010
Training epoch 95 [34560/67107 (51.53%)]	 Loss: 0.000005
Training epoch 95 [35200/67107 (52.48%)]	 Loss: 0.000001
Training epoch 95 [35840/67107 (53.44%)]	 Loss: 0.000008
Training epoch 95 [36480/67107 (54.39%)]	 Loss: 0.000005
Training epoch 95 [37120/67107 (55.34%)]	 Loss: 0.000024
Training epoch 95 [37760/67107 (56.30%)]	 Loss: 0.000006
Training epoch 95 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 95 [39040/67107 (58.21%)]	 Loss: 0.000007
Training epoch 95 [39680/67107 (59.16%)]	 Loss: 0.000006
Training epoch 95 [40320/67107 (60.11%)]	 Loss: 0.000015
Training epoch 95 [40960/67107 (61.07%)]	 Loss: 0.000018
Training epoch 95 [41600/67107 (62.02%)]	 Loss: 0.000009
Training epoch 95 [42240/67107 (62.98%)]	 Loss: 0.000019
Training epoch 95 [42880/67107 (63.93%)]	 Loss: 0.000002
Training epoch 95 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 95 [44160/67107 (65.84%)]	 Loss: 0.000003
Training epoch 95 [44800/67107 (66.79%)]	 Loss: 0.000003
Training epoch 95 [45440/67107 (67.75%)]	 Loss: 0.000003
Training epoch 95 [46080/67107 (68.70%)]	 Loss: 0.000001
Training epoch 95 [46720/67107 (69.66%)]	 Loss: 0.000006
Training epoch 95 [47360/67107 (70.61%)]	 Loss: 0.000006
Training epoch 95 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 95 [48640/67107 (72.52%)]	 Loss: 0.000010
Training epoch 95 [49280/67107 (73.47%)]	 Loss: 0.000018
Training epoch 95 [49920/67107 (74.43%)]	 Loss: 0.000007
Training epoch 95 [50560/67107 (75.38%)]	 Loss: 0.000002
Training epoch 95 [51200/67107 (76.34%)]	 Loss: 0.000018
Training epoch 95 [51840/67107 (77.29%)]	 Loss: 0.000005
Training epoch 95 [52480/67107 (78.24%)]	 Loss: 0.000004
Training epoch 95 [53120/67107 (79.20%)]	 Loss: 0.000001
Training epoch 95 [53760/67107 (80.15%)]	 Loss: 0.000003
Training epoch 95 [54400/67107 (81.11%)]	 Loss: 0.000016
Training epoch 95 [55040/67107 (82.06%)]	 Loss: 0.000011
Training epoch 95 [55680/67107 (83.02%)]	 Loss: 0.000008
Training epoch 95 [56320/67107 (83.97%)]	 Loss: 0.000007
Training epoch 95 [56960/67107 (84.92%)]	 Loss: 0.000009
Training epoch 95 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 95 [58240/67107 (86.83%)]	 Loss: 0.000007
Training epoch 95 [58880/67107 (87.79%)]	 Loss: 0.000014
Training epoch 95 [59520/67107 (88.74%)]	 Loss: 0.000015
Training epoch 95 [60160/67107 (89.69%)]	 Loss: 0.000014
Training epoch 95 [60800/67107 (90.65%)]	 Loss: 0.000005
Training epoch 95 [61440/67107 (91.60%)]	 Loss: 0.000001
Training epoch 95 [62080/67107 (92.56%)]	 Loss: 0.000008
Training epoch 95 [62720/67107 (93.51%)]	 Loss: 0.000016
Training epoch 95 [63360/67107 (94.47%)]	 Loss: 0.000011
Training epoch 95 [64000/67107 (95.42%)]	 Loss: 0.000004
Training epoch 95 [64640/67107 (96.37%)]	 Loss: 0.000017
Training epoch 95 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 95 [65920/67107 (98.28%)]	 Loss: 0.000017
Training epoch 95 [66560/67107 (99.24%)]	 Loss: 0.000001
Test set: Average Loss: 0.000148
Training epoch 96 [0/67107 (0.00%)]	 Loss: 0.000007
Training epoch 96 [640/67107 (0.95%)]	 Loss: 0.000021
Training epoch 96 [1280/67107 (1.91%)]	 Loss: 0.000013
Training epoch 96 [1920/67107 (2.86%)]	 Loss: 0.000007
Training epoch 96 [2560/67107 (3.82%)]	 Loss: 0.000004
Training epoch 96 [3200/67107 (4.77%)]	 Loss: 0.000005
Training epoch 96 [3840/67107 (5.73%)]	 Loss: 0.000015
Training epoch 96 [4480/67107 (6.68%)]	 Loss: 0.000020
Training epoch 96 [5120/67107 (7.63%)]	 Loss: 0.000018
Training epoch 96 [5760/67107 (8.59%)]	 Loss: 0.000003
Training epoch 96 [6400/67107 (9.54%)]	 Loss: 0.000011
Training epoch 96 [7040/67107 (10.50%)]	 Loss: 0.000004
Training epoch 96 [7680/67107 (11.45%)]	 Loss: 0.000035
Training epoch 96 [8320/67107 (12.40%)]	 Loss: 0.000012
Training epoch 96 [8960/67107 (13.36%)]	 Loss: 0.000002
Training epoch 96 [9600/67107 (14.31%)]	 Loss: 0.000004
Training epoch 96 [10240/67107 (15.27%)]	 Loss: 0.000011
Training epoch 96 [10880/67107 (16.22%)]	 Loss: 0.000008
Training epoch 96 [11520/67107 (17.18%)]	 Loss: 0.000005
Training epoch 96 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 96 [12800/67107 (19.08%)]	 Loss: 0.000007
Training epoch 96 [13440/67107 (20.04%)]	 Loss: 0.000008
Training epoch 96 [14080/67107 (20.99%)]	 Loss: 0.000005
Training epoch 96 [14720/67107 (21.95%)]	 Loss: 0.000004
Training epoch 96 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 96 [16000/67107 (23.85%)]	 Loss: 0.000004
Training epoch 96 [16640/67107 (24.81%)]	 Loss: 0.000007
Training epoch 96 [17280/67107 (25.76%)]	 Loss: 0.000007
Training epoch 96 [17920/67107 (26.72%)]	 Loss: 0.000004
Training epoch 96 [18560/67107 (27.67%)]	 Loss: 0.000004
Training epoch 96 [19200/67107 (28.63%)]	 Loss: 0.000011
Training epoch 96 [19840/67107 (29.58%)]	 Loss: 0.000003
Training epoch 96 [20480/67107 (30.53%)]	 Loss: 0.000005
Training epoch 96 [21120/67107 (31.49%)]	 Loss: 0.000005
Training epoch 96 [21760/67107 (32.44%)]	 Loss: 0.000021
Training epoch 96 [22400/67107 (33.40%)]	 Loss: 0.000001
Training epoch 96 [23040/67107 (34.35%)]	 Loss: 0.000008
Training epoch 96 [23680/67107 (35.31%)]	 Loss: 0.000014
Training epoch 96 [24320/67107 (36.26%)]	 Loss: 0.000060
Training epoch 96 [24960/67107 (37.21%)]	 Loss: 0.000038
Training epoch 96 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 96 [26240/67107 (39.12%)]	 Loss: 0.000018
Training epoch 96 [26880/67107 (40.08%)]	 Loss: 0.000010
Training epoch 96 [27520/67107 (41.03%)]	 Loss: 0.000005
Training epoch 96 [28160/67107 (41.98%)]	 Loss: 0.000009
Training epoch 96 [28800/67107 (42.94%)]	 Loss: 0.000029
Training epoch 96 [29440/67107 (43.89%)]	 Loss: 0.000003
Training epoch 96 [30080/67107 (44.85%)]	 Loss: 0.000001
Training epoch 96 [30720/67107 (45.80%)]	 Loss: 0.000005
Training epoch 96 [31360/67107 (46.76%)]	 Loss: 0.000003
Training epoch 96 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 96 [32640/67107 (48.66%)]	 Loss: 0.000005
Training epoch 96 [33280/67107 (49.62%)]	 Loss: 0.000017
Training epoch 96 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 96 [34560/67107 (51.53%)]	 Loss: 0.000005
Training epoch 96 [35200/67107 (52.48%)]	 Loss: 0.000008
Training epoch 96 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 96 [36480/67107 (54.39%)]	 Loss: 0.000020
Training epoch 96 [37120/67107 (55.34%)]	 Loss: 0.000006
Training epoch 96 [37760/67107 (56.30%)]	 Loss: 0.000010
Training epoch 96 [38400/67107 (57.25%)]	 Loss: 0.000005
Training epoch 96 [39040/67107 (58.21%)]	 Loss: 0.000014
Training epoch 96 [39680/67107 (59.16%)]	 Loss: 0.000014
Training epoch 96 [40320/67107 (60.11%)]	 Loss: 0.000015
Training epoch 96 [40960/67107 (61.07%)]	 Loss: 0.000008
Training epoch 96 [41600/67107 (62.02%)]	 Loss: 0.000004
Training epoch 96 [42240/67107 (62.98%)]	 Loss: 0.000010
Training epoch 96 [42880/67107 (63.93%)]	 Loss: 0.000010
Training epoch 96 [43520/67107 (64.89%)]	 Loss: 0.000010
Training epoch 96 [44160/67107 (65.84%)]	 Loss: 0.000006
Training epoch 96 [44800/67107 (66.79%)]	 Loss: 0.000015
Training epoch 96 [45440/67107 (67.75%)]	 Loss: 0.000005
Training epoch 96 [46080/67107 (68.70%)]	 Loss: 0.000008
Training epoch 96 [46720/67107 (69.66%)]	 Loss: 0.000006
Training epoch 96 [47360/67107 (70.61%)]	 Loss: 0.000007
Training epoch 96 [48000/67107 (71.56%)]	 Loss: 0.000008
Training epoch 96 [48640/67107 (72.52%)]	 Loss: 0.000003
Training epoch 96 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 96 [49920/67107 (74.43%)]	 Loss: 0.000002
Training epoch 96 [50560/67107 (75.38%)]	 Loss: 0.000003
Training epoch 96 [51200/67107 (76.34%)]	 Loss: 0.000003
Training epoch 96 [51840/67107 (77.29%)]	 Loss: 0.000001
Training epoch 96 [52480/67107 (78.24%)]	 Loss: 0.000003
Training epoch 96 [53120/67107 (79.20%)]	 Loss: 0.000006
Training epoch 96 [53760/67107 (80.15%)]	 Loss: 0.000009
Training epoch 96 [54400/67107 (81.11%)]	 Loss: 0.000008
Training epoch 96 [55040/67107 (82.06%)]	 Loss: 0.000010
Training epoch 96 [55680/67107 (83.02%)]	 Loss: 0.000002
Training epoch 96 [56320/67107 (83.97%)]	 Loss: 0.000003
Training epoch 96 [56960/67107 (84.92%)]	 Loss: 0.000005
Training epoch 96 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 96 [58240/67107 (86.83%)]	 Loss: 0.000004
Training epoch 96 [58880/67107 (87.79%)]	 Loss: 0.000005
Training epoch 96 [59520/67107 (88.74%)]	 Loss: 0.000006
Training epoch 96 [60160/67107 (89.69%)]	 Loss: 0.000008
Training epoch 96 [60800/67107 (90.65%)]	 Loss: 0.000001
Training epoch 96 [61440/67107 (91.60%)]	 Loss: 0.000009
Training epoch 96 [62080/67107 (92.56%)]	 Loss: 0.000006
Training epoch 96 [62720/67107 (93.51%)]	 Loss: 0.000004
Training epoch 96 [63360/67107 (94.47%)]	 Loss: 0.000007
Training epoch 96 [64000/67107 (95.42%)]	 Loss: 0.000018
Training epoch 96 [64640/67107 (96.37%)]	 Loss: 0.000001
Training epoch 96 [65280/67107 (97.33%)]	 Loss: 0.000004
Training epoch 96 [65920/67107 (98.28%)]	 Loss: 0.000014
Training epoch 96 [66560/67107 (99.24%)]	 Loss: 0.000004
Test set: Average Loss: 0.000123
Training epoch 97 [0/67107 (0.00%)]	 Loss: 0.000003
Training epoch 97 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 97 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 97 [1920/67107 (2.86%)]	 Loss: 0.000002
Training epoch 97 [2560/67107 (3.82%)]	 Loss: 0.000008
Training epoch 97 [3200/67107 (4.77%)]	 Loss: 0.000006
Training epoch 97 [3840/67107 (5.73%)]	 Loss: 0.000009
Training epoch 97 [4480/67107 (6.68%)]	 Loss: 0.000001
Training epoch 97 [5120/67107 (7.63%)]	 Loss: 0.000007
Training epoch 97 [5760/67107 (8.59%)]	 Loss: 0.000007
Training epoch 97 [6400/67107 (9.54%)]	 Loss: 0.000004
Training epoch 97 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 97 [7680/67107 (11.45%)]	 Loss: 0.000004
Training epoch 97 [8320/67107 (12.40%)]	 Loss: 0.000002
Training epoch 97 [8960/67107 (13.36%)]	 Loss: 0.000021
Training epoch 97 [9600/67107 (14.31%)]	 Loss: 0.000006
Training epoch 97 [10240/67107 (15.27%)]	 Loss: 0.000012
Training epoch 97 [10880/67107 (16.22%)]	 Loss: 0.000007
Training epoch 97 [11520/67107 (17.18%)]	 Loss: 0.000008
Training epoch 97 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 97 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 97 [13440/67107 (20.04%)]	 Loss: 0.000004
Training epoch 97 [14080/67107 (20.99%)]	 Loss: 0.000002
Training epoch 97 [14720/67107 (21.95%)]	 Loss: 0.000005
Training epoch 97 [15360/67107 (22.90%)]	 Loss: 0.000002
Training epoch 97 [16000/67107 (23.85%)]	 Loss: 0.000002
Training epoch 97 [16640/67107 (24.81%)]	 Loss: 0.000004
Training epoch 97 [17280/67107 (25.76%)]	 Loss: 0.000008
Training epoch 97 [17920/67107 (26.72%)]	 Loss: 0.000004
Training epoch 97 [18560/67107 (27.67%)]	 Loss: 0.000010
Training epoch 97 [19200/67107 (28.63%)]	 Loss: 0.000005
Training epoch 97 [19840/67107 (29.58%)]	 Loss: 0.000018
Training epoch 97 [20480/67107 (30.53%)]	 Loss: 0.000036
Training epoch 97 [21120/67107 (31.49%)]	 Loss: 0.000000
Training epoch 97 [21760/67107 (32.44%)]	 Loss: 0.000002
Training epoch 97 [22400/67107 (33.40%)]	 Loss: 0.000001
Training epoch 97 [23040/67107 (34.35%)]	 Loss: 0.000002
Training epoch 97 [23680/67107 (35.31%)]	 Loss: 0.000024
Training epoch 97 [24320/67107 (36.26%)]	 Loss: 0.000028
Training epoch 97 [24960/67107 (37.21%)]	 Loss: 0.000015
Training epoch 97 [25600/67107 (38.17%)]	 Loss: 0.000008
Training epoch 97 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 97 [26880/67107 (40.08%)]	 Loss: 0.000006
Training epoch 97 [27520/67107 (41.03%)]	 Loss: 0.000007
Training epoch 97 [28160/67107 (41.98%)]	 Loss: 0.000003
Training epoch 97 [28800/67107 (42.94%)]	 Loss: 0.000009
Training epoch 97 [29440/67107 (43.89%)]	 Loss: 0.000010
Training epoch 97 [30080/67107 (44.85%)]	 Loss: 0.000009
Training epoch 97 [30720/67107 (45.80%)]	 Loss: 0.000012
Training epoch 97 [31360/67107 (46.76%)]	 Loss: 0.000011
Training epoch 97 [32000/67107 (47.71%)]	 Loss: 0.000021
Training epoch 97 [32640/67107 (48.66%)]	 Loss: 0.000005
Training epoch 97 [33280/67107 (49.62%)]	 Loss: 0.000015
Training epoch 97 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 97 [34560/67107 (51.53%)]	 Loss: 0.000002
Training epoch 97 [35200/67107 (52.48%)]	 Loss: 0.000011
Training epoch 97 [35840/67107 (53.44%)]	 Loss: 0.000005
Training epoch 97 [36480/67107 (54.39%)]	 Loss: 0.000003
Training epoch 97 [37120/67107 (55.34%)]	 Loss: 0.000003
Training epoch 97 [37760/67107 (56.30%)]	 Loss: 0.000006
Training epoch 97 [38400/67107 (57.25%)]	 Loss: 0.000007
Training epoch 97 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 97 [39680/67107 (59.16%)]	 Loss: 0.000001
Training epoch 97 [40320/67107 (60.11%)]	 Loss: 0.000002
Training epoch 97 [40960/67107 (61.07%)]	 Loss: 0.000002
Training epoch 97 [41600/67107 (62.02%)]	 Loss: 0.000004
Training epoch 97 [42240/67107 (62.98%)]	 Loss: 0.000002
Training epoch 97 [42880/67107 (63.93%)]	 Loss: 0.000004
Training epoch 97 [43520/67107 (64.89%)]	 Loss: 0.000003
Training epoch 97 [44160/67107 (65.84%)]	 Loss: 0.000003
Training epoch 97 [44800/67107 (66.79%)]	 Loss: 0.000004
Training epoch 97 [45440/67107 (67.75%)]	 Loss: 0.000007
Training epoch 97 [46080/67107 (68.70%)]	 Loss: 0.000021
Training epoch 97 [46720/67107 (69.66%)]	 Loss: 0.000003
Training epoch 97 [47360/67107 (70.61%)]	 Loss: 0.000005
Training epoch 97 [48000/67107 (71.56%)]	 Loss: 0.000002
Training epoch 97 [48640/67107 (72.52%)]	 Loss: 0.000017
Training epoch 97 [49280/67107 (73.47%)]	 Loss: 0.000017
Training epoch 97 [49920/67107 (74.43%)]	 Loss: 0.000005
Training epoch 97 [50560/67107 (75.38%)]	 Loss: 0.000009
Training epoch 97 [51200/67107 (76.34%)]	 Loss: 0.000001
Training epoch 97 [51840/67107 (77.29%)]	 Loss: 0.000026
Training epoch 97 [52480/67107 (78.24%)]	 Loss: 0.000003
Training epoch 97 [53120/67107 (79.20%)]	 Loss: 0.000016
Training epoch 97 [53760/67107 (80.15%)]	 Loss: 0.000006
Training epoch 97 [54400/67107 (81.11%)]	 Loss: 0.000011
Training epoch 97 [55040/67107 (82.06%)]	 Loss: 0.000017
Training epoch 97 [55680/67107 (83.02%)]	 Loss: 0.000004
Training epoch 97 [56320/67107 (83.97%)]	 Loss: 0.000004
Training epoch 97 [56960/67107 (84.92%)]	 Loss: 0.000006
Training epoch 97 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 97 [58240/67107 (86.83%)]	 Loss: 0.000024
Training epoch 97 [58880/67107 (87.79%)]	 Loss: 0.000005
Training epoch 97 [59520/67107 (88.74%)]	 Loss: 0.000002
Training epoch 97 [60160/67107 (89.69%)]	 Loss: 0.000005
Training epoch 97 [60800/67107 (90.65%)]	 Loss: 0.000008
Training epoch 97 [61440/67107 (91.60%)]	 Loss: 0.000018
Training epoch 97 [62080/67107 (92.56%)]	 Loss: 0.000026
Training epoch 97 [62720/67107 (93.51%)]	 Loss: 0.000006
Training epoch 97 [63360/67107 (94.47%)]	 Loss: 0.000002
Training epoch 97 [64000/67107 (95.42%)]	 Loss: 0.000004
Training epoch 97 [64640/67107 (96.37%)]	 Loss: 0.000002
Training epoch 97 [65280/67107 (97.33%)]	 Loss: 0.000003
Training epoch 97 [65920/67107 (98.28%)]	 Loss: 0.000002
Training epoch 97 [66560/67107 (99.24%)]	 Loss: 0.000002
Test set: Average Loss: 0.000159
Training epoch 98 [0/67107 (0.00%)]	 Loss: 0.000007
Training epoch 98 [640/67107 (0.95%)]	 Loss: 0.000001
Training epoch 98 [1280/67107 (1.91%)]	 Loss: 0.000014
Training epoch 98 [1920/67107 (2.86%)]	 Loss: 0.000014
Training epoch 98 [2560/67107 (3.82%)]	 Loss: 0.000006
Training epoch 98 [3200/67107 (4.77%)]	 Loss: 0.000003
Training epoch 98 [3840/67107 (5.73%)]	 Loss: 0.000020
Training epoch 98 [4480/67107 (6.68%)]	 Loss: 0.000002
Training epoch 98 [5120/67107 (7.63%)]	 Loss: 0.000025
Training epoch 98 [5760/67107 (8.59%)]	 Loss: 0.000011
Training epoch 98 [6400/67107 (9.54%)]	 Loss: 0.000006
Training epoch 98 [7040/67107 (10.50%)]	 Loss: 0.000011
Training epoch 98 [7680/67107 (11.45%)]	 Loss: 0.000004
Training epoch 98 [8320/67107 (12.40%)]	 Loss: 0.000009
Training epoch 98 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 98 [9600/67107 (14.31%)]	 Loss: 0.000005
Training epoch 98 [10240/67107 (15.27%)]	 Loss: 0.000009
Training epoch 98 [10880/67107 (16.22%)]	 Loss: 0.000045
Training epoch 98 [11520/67107 (17.18%)]	 Loss: 0.000003
Training epoch 98 [12160/67107 (18.13%)]	 Loss: 0.000006
Training epoch 98 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 98 [13440/67107 (20.04%)]	 Loss: 0.000002
Training epoch 98 [14080/67107 (20.99%)]	 Loss: 0.000027
Training epoch 98 [14720/67107 (21.95%)]	 Loss: 0.000007
Training epoch 98 [15360/67107 (22.90%)]	 Loss: 0.000005
Training epoch 98 [16000/67107 (23.85%)]	 Loss: 0.000004
Training epoch 98 [16640/67107 (24.81%)]	 Loss: 0.000009
Training epoch 98 [17280/67107 (25.76%)]	 Loss: 0.000007
Training epoch 98 [17920/67107 (26.72%)]	 Loss: 0.000001
Training epoch 98 [18560/67107 (27.67%)]	 Loss: 0.000002
Training epoch 98 [19200/67107 (28.63%)]	 Loss: 0.000023
Training epoch 98 [19840/67107 (29.58%)]	 Loss: 0.000007
Training epoch 98 [20480/67107 (30.53%)]	 Loss: 0.000003
Training epoch 98 [21120/67107 (31.49%)]	 Loss: 0.000021
Training epoch 98 [21760/67107 (32.44%)]	 Loss: 0.000001
Training epoch 98 [22400/67107 (33.40%)]	 Loss: 0.000008
Training epoch 98 [23040/67107 (34.35%)]	 Loss: 0.000007
Training epoch 98 [23680/67107 (35.31%)]	 Loss: 0.000022
Training epoch 98 [24320/67107 (36.26%)]	 Loss: 0.000002
Training epoch 98 [24960/67107 (37.21%)]	 Loss: 0.000005
Training epoch 98 [25600/67107 (38.17%)]	 Loss: 0.000004
Training epoch 98 [26240/67107 (39.12%)]	 Loss: 0.000004
Training epoch 98 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 98 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 98 [28160/67107 (41.98%)]	 Loss: 0.000008
Training epoch 98 [28800/67107 (42.94%)]	 Loss: 0.000011
Training epoch 98 [29440/67107 (43.89%)]	 Loss: 0.000021
Training epoch 98 [30080/67107 (44.85%)]	 Loss: 0.000003
Training epoch 98 [30720/67107 (45.80%)]	 Loss: 0.000001
Training epoch 98 [31360/67107 (46.76%)]	 Loss: 0.000002
Training epoch 98 [32000/67107 (47.71%)]	 Loss: 0.000005
Training epoch 98 [32640/67107 (48.66%)]	 Loss: 0.000001
Training epoch 98 [33280/67107 (49.62%)]	 Loss: 0.000003
Training epoch 98 [33920/67107 (50.57%)]	 Loss: 0.000007
Training epoch 98 [34560/67107 (51.53%)]	 Loss: 0.000013
Training epoch 98 [35200/67107 (52.48%)]	 Loss: 0.000004
Training epoch 98 [35840/67107 (53.44%)]	 Loss: 0.000008
Training epoch 98 [36480/67107 (54.39%)]	 Loss: 0.000009
Training epoch 98 [37120/67107 (55.34%)]	 Loss: 0.000003
Training epoch 98 [37760/67107 (56.30%)]	 Loss: 0.000001
Training epoch 98 [38400/67107 (57.25%)]	 Loss: 0.000001
Training epoch 98 [39040/67107 (58.21%)]	 Loss: 0.000003
Training epoch 98 [39680/67107 (59.16%)]	 Loss: 0.000002
Training epoch 98 [40320/67107 (60.11%)]	 Loss: 0.000001
Training epoch 98 [40960/67107 (61.07%)]	 Loss: 0.000002
Training epoch 98 [41600/67107 (62.02%)]	 Loss: 0.000038
Training epoch 98 [42240/67107 (62.98%)]	 Loss: 0.000041
Training epoch 98 [42880/67107 (63.93%)]	 Loss: 0.000003
Training epoch 98 [43520/67107 (64.89%)]	 Loss: 0.000022
Training epoch 98 [44160/67107 (65.84%)]	 Loss: 0.000019
Training epoch 98 [44800/67107 (66.79%)]	 Loss: 0.000006
Training epoch 98 [45440/67107 (67.75%)]	 Loss: 0.000017
Training epoch 98 [46080/67107 (68.70%)]	 Loss: 0.000017
Training epoch 98 [46720/67107 (69.66%)]	 Loss: 0.000018
Training epoch 98 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 98 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 98 [48640/67107 (72.52%)]	 Loss: 0.000003
Training epoch 98 [49280/67107 (73.47%)]	 Loss: 0.000003
Training epoch 98 [49920/67107 (74.43%)]	 Loss: 0.000012
Training epoch 98 [50560/67107 (75.38%)]	 Loss: 0.000005
Training epoch 98 [51200/67107 (76.34%)]	 Loss: 0.000010
Training epoch 98 [51840/67107 (77.29%)]	 Loss: 0.000003
Training epoch 98 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 98 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 98 [53760/67107 (80.15%)]	 Loss: 0.000007
Training epoch 98 [54400/67107 (81.11%)]	 Loss: 0.000018
Training epoch 98 [55040/67107 (82.06%)]	 Loss: 0.000011
Training epoch 98 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 98 [56320/67107 (83.97%)]	 Loss: 0.000009
Training epoch 98 [56960/67107 (84.92%)]	 Loss: 0.000015
Training epoch 98 [57600/67107 (85.88%)]	 Loss: 0.000004
Training epoch 98 [58240/67107 (86.83%)]	 Loss: 0.000016
Training epoch 98 [58880/67107 (87.79%)]	 Loss: 0.000004
Training epoch 98 [59520/67107 (88.74%)]	 Loss: 0.000051
Training epoch 98 [60160/67107 (89.69%)]	 Loss: 0.000004
Training epoch 98 [60800/67107 (90.65%)]	 Loss: 0.000002
Training epoch 98 [61440/67107 (91.60%)]	 Loss: 0.000085
Training epoch 98 [62080/67107 (92.56%)]	 Loss: 0.000005
Training epoch 98 [62720/67107 (93.51%)]	 Loss: 0.000007
Training epoch 98 [63360/67107 (94.47%)]	 Loss: 0.000012
Training epoch 98 [64000/67107 (95.42%)]	 Loss: 0.000001
Training epoch 98 [64640/67107 (96.37%)]	 Loss: 0.000012
Training epoch 98 [65280/67107 (97.33%)]	 Loss: 0.000002
Training epoch 98 [65920/67107 (98.28%)]	 Loss: 0.000010
Training epoch 98 [66560/67107 (99.24%)]	 Loss: 0.000002
Test set: Average Loss: 0.000103
Training epoch 99 [0/67107 (0.00%)]	 Loss: 0.000012
Training epoch 99 [640/67107 (0.95%)]	 Loss: 0.000004
Training epoch 99 [1280/67107 (1.91%)]	 Loss: 0.000003
Training epoch 99 [1920/67107 (2.86%)]	 Loss: 0.000002
Training epoch 99 [2560/67107 (3.82%)]	 Loss: 0.000002
Training epoch 99 [3200/67107 (4.77%)]	 Loss: 0.000009
Training epoch 99 [3840/67107 (5.73%)]	 Loss: 0.000020
Training epoch 99 [4480/67107 (6.68%)]	 Loss: 0.000016
Training epoch 99 [5120/67107 (7.63%)]	 Loss: 0.000003
Training epoch 99 [5760/67107 (8.59%)]	 Loss: 0.000009
Training epoch 99 [6400/67107 (9.54%)]	 Loss: 0.000003
Training epoch 99 [7040/67107 (10.50%)]	 Loss: 0.000003
Training epoch 99 [7680/67107 (11.45%)]	 Loss: 0.000004
Training epoch 99 [8320/67107 (12.40%)]	 Loss: 0.000001
Training epoch 99 [8960/67107 (13.36%)]	 Loss: 0.000001
Training epoch 99 [9600/67107 (14.31%)]	 Loss: 0.000003
Training epoch 99 [10240/67107 (15.27%)]	 Loss: 0.000012
Training epoch 99 [10880/67107 (16.22%)]	 Loss: 0.000008
Training epoch 99 [11520/67107 (17.18%)]	 Loss: 0.000001
Training epoch 99 [12160/67107 (18.13%)]	 Loss: 0.000004
Training epoch 99 [12800/67107 (19.08%)]	 Loss: 0.000004
Training epoch 99 [13440/67107 (20.04%)]	 Loss: 0.000010
Training epoch 99 [14080/67107 (20.99%)]	 Loss: 0.000017
Training epoch 99 [14720/67107 (21.95%)]	 Loss: 0.000006
Training epoch 99 [15360/67107 (22.90%)]	 Loss: 0.000011
Training epoch 99 [16000/67107 (23.85%)]	 Loss: 0.000011
Training epoch 99 [16640/67107 (24.81%)]	 Loss: 0.000011
Training epoch 99 [17280/67107 (25.76%)]	 Loss: 0.000008
Training epoch 99 [17920/67107 (26.72%)]	 Loss: 0.000008
Training epoch 99 [18560/67107 (27.67%)]	 Loss: 0.000007
Training epoch 99 [19200/67107 (28.63%)]	 Loss: 0.000017
Training epoch 99 [19840/67107 (29.58%)]	 Loss: 0.000005
Training epoch 99 [20480/67107 (30.53%)]	 Loss: 0.000012
Training epoch 99 [21120/67107 (31.49%)]	 Loss: 0.000006
Training epoch 99 [21760/67107 (32.44%)]	 Loss: 0.000004
Training epoch 99 [22400/67107 (33.40%)]	 Loss: 0.000004
Training epoch 99 [23040/67107 (34.35%)]	 Loss: 0.000004
Training epoch 99 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 99 [24320/67107 (36.26%)]	 Loss: 0.000001
Training epoch 99 [24960/67107 (37.21%)]	 Loss: 0.000007
Training epoch 99 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 99 [26240/67107 (39.12%)]	 Loss: 0.000020
Training epoch 99 [26880/67107 (40.08%)]	 Loss: 0.000008
Training epoch 99 [27520/67107 (41.03%)]	 Loss: 0.000015
Training epoch 99 [28160/67107 (41.98%)]	 Loss: 0.000010
Training epoch 99 [28800/67107 (42.94%)]	 Loss: 0.000007
Training epoch 99 [29440/67107 (43.89%)]	 Loss: 0.000009
Training epoch 99 [30080/67107 (44.85%)]	 Loss: 0.000005
Training epoch 99 [30720/67107 (45.80%)]	 Loss: 0.000009
Training epoch 99 [31360/67107 (46.76%)]	 Loss: 0.000027
Training epoch 99 [32000/67107 (47.71%)]	 Loss: 0.000090
Training epoch 99 [32640/67107 (48.66%)]	 Loss: 0.000012
Training epoch 99 [33280/67107 (49.62%)]	 Loss: 0.000029
Training epoch 99 [33920/67107 (50.57%)]	 Loss: 0.000004
Training epoch 99 [34560/67107 (51.53%)]	 Loss: 0.000004
Training epoch 99 [35200/67107 (52.48%)]	 Loss: 0.000010
Training epoch 99 [35840/67107 (53.44%)]	 Loss: 0.000004
Training epoch 99 [36480/67107 (54.39%)]	 Loss: 0.000007
Training epoch 99 [37120/67107 (55.34%)]	 Loss: 0.000007
Training epoch 99 [37760/67107 (56.30%)]	 Loss: 0.000072
Training epoch 99 [38400/67107 (57.25%)]	 Loss: 0.000031
Training epoch 99 [39040/67107 (58.21%)]	 Loss: 0.000005
Training epoch 99 [39680/67107 (59.16%)]	 Loss: 0.000061
Training epoch 99 [40320/67107 (60.11%)]	 Loss: 0.000024
Training epoch 99 [40960/67107 (61.07%)]	 Loss: 0.000007
Training epoch 99 [41600/67107 (62.02%)]	 Loss: 0.000002
Training epoch 99 [42240/67107 (62.98%)]	 Loss: 0.000007
Training epoch 99 [42880/67107 (63.93%)]	 Loss: 0.000004
Training epoch 99 [43520/67107 (64.89%)]	 Loss: 0.000004
Training epoch 99 [44160/67107 (65.84%)]	 Loss: 0.000003
Training epoch 99 [44800/67107 (66.79%)]	 Loss: 0.000009
Training epoch 99 [45440/67107 (67.75%)]	 Loss: 0.000009
Training epoch 99 [46080/67107 (68.70%)]	 Loss: 0.000008
Training epoch 99 [46720/67107 (69.66%)]	 Loss: 0.000013
Training epoch 99 [47360/67107 (70.61%)]	 Loss: 0.000008
Training epoch 99 [48000/67107 (71.56%)]	 Loss: 0.000005
Training epoch 99 [48640/67107 (72.52%)]	 Loss: 0.000005
Training epoch 99 [49280/67107 (73.47%)]	 Loss: 0.000043
Training epoch 99 [49920/67107 (74.43%)]	 Loss: 0.000004
Training epoch 99 [50560/67107 (75.38%)]	 Loss: 0.000006
Training epoch 99 [51200/67107 (76.34%)]	 Loss: 0.000023
Training epoch 99 [51840/67107 (77.29%)]	 Loss: 0.000010
Training epoch 99 [52480/67107 (78.24%)]	 Loss: 0.000005
Training epoch 99 [53120/67107 (79.20%)]	 Loss: 0.000007
Training epoch 99 [53760/67107 (80.15%)]	 Loss: 0.000018
Training epoch 99 [54400/67107 (81.11%)]	 Loss: 0.000002
Training epoch 99 [55040/67107 (82.06%)]	 Loss: 0.000002
Training epoch 99 [55680/67107 (83.02%)]	 Loss: 0.000003
Training epoch 99 [56320/67107 (83.97%)]	 Loss: 0.000002
Training epoch 99 [56960/67107 (84.92%)]	 Loss: 0.000002
Training epoch 99 [57600/67107 (85.88%)]	 Loss: 0.000006
Training epoch 99 [58240/67107 (86.83%)]	 Loss: 0.000025
Training epoch 99 [58880/67107 (87.79%)]	 Loss: 0.000008
Training epoch 99 [59520/67107 (88.74%)]	 Loss: 0.000020
Training epoch 99 [60160/67107 (89.69%)]	 Loss: 0.000012
Training epoch 99 [60800/67107 (90.65%)]	 Loss: 0.000007
Training epoch 99 [61440/67107 (91.60%)]	 Loss: 0.000002
Training epoch 99 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 99 [62720/67107 (93.51%)]	 Loss: 0.000010
Training epoch 99 [63360/67107 (94.47%)]	 Loss: 0.000006
Training epoch 99 [64000/67107 (95.42%)]	 Loss: 0.000004
Training epoch 99 [64640/67107 (96.37%)]	 Loss: 0.000004
Training epoch 99 [65280/67107 (97.33%)]	 Loss: 0.000009
Training epoch 99 [65920/67107 (98.28%)]	 Loss: 0.000011
Training epoch 99 [66560/67107 (99.24%)]	 Loss: 0.000009
Test set: Average Loss: 0.000168
Training epoch 100 [0/67107 (0.00%)]	 Loss: 0.000005
Training epoch 100 [640/67107 (0.95%)]	 Loss: 0.000015
Training epoch 100 [1280/67107 (1.91%)]	 Loss: 0.000007
Training epoch 100 [1920/67107 (2.86%)]	 Loss: 0.000003
Training epoch 100 [2560/67107 (3.82%)]	 Loss: 0.000002
Training epoch 100 [3200/67107 (4.77%)]	 Loss: 0.000004
Training epoch 100 [3840/67107 (5.73%)]	 Loss: 0.000005
Training epoch 100 [4480/67107 (6.68%)]	 Loss: 0.000006
Training epoch 100 [5120/67107 (7.63%)]	 Loss: 0.000001
Training epoch 100 [5760/67107 (8.59%)]	 Loss: 0.000004
Training epoch 100 [6400/67107 (9.54%)]	 Loss: 0.000003
Training epoch 100 [7040/67107 (10.50%)]	 Loss: 0.000005
Training epoch 100 [7680/67107 (11.45%)]	 Loss: 0.000006
Training epoch 100 [8320/67107 (12.40%)]	 Loss: 0.000003
Training epoch 100 [8960/67107 (13.36%)]	 Loss: 0.000003
Training epoch 100 [9600/67107 (14.31%)]	 Loss: 0.000001
Training epoch 100 [10240/67107 (15.27%)]	 Loss: 0.000004
Training epoch 100 [10880/67107 (16.22%)]	 Loss: 0.000046
Training epoch 100 [11520/67107 (17.18%)]	 Loss: 0.000014
Training epoch 100 [12160/67107 (18.13%)]	 Loss: 0.000023
Training epoch 100 [12800/67107 (19.08%)]	 Loss: 0.000062
Training epoch 100 [13440/67107 (20.04%)]	 Loss: 0.000004
Training epoch 100 [14080/67107 (20.99%)]	 Loss: 0.000004
Training epoch 100 [14720/67107 (21.95%)]	 Loss: 0.000009
Training epoch 100 [15360/67107 (22.90%)]	 Loss: 0.000009
Training epoch 100 [16000/67107 (23.85%)]	 Loss: 0.000006
Training epoch 100 [16640/67107 (24.81%)]	 Loss: 0.000006
Training epoch 100 [17280/67107 (25.76%)]	 Loss: 0.000004
Training epoch 100 [17920/67107 (26.72%)]	 Loss: 0.000005
Training epoch 100 [18560/67107 (27.67%)]	 Loss: 0.000005
Training epoch 100 [19200/67107 (28.63%)]	 Loss: 0.000012
Training epoch 100 [19840/67107 (29.58%)]	 Loss: 0.000027
Training epoch 100 [20480/67107 (30.53%)]	 Loss: 0.000003
Training epoch 100 [21120/67107 (31.49%)]	 Loss: 0.000002
Training epoch 100 [21760/67107 (32.44%)]	 Loss: 0.000004
Training epoch 100 [22400/67107 (33.40%)]	 Loss: 0.000008
Training epoch 100 [23040/67107 (34.35%)]	 Loss: 0.000021
Training epoch 100 [23680/67107 (35.31%)]	 Loss: 0.000002
Training epoch 100 [24320/67107 (36.26%)]	 Loss: 0.000004
Training epoch 100 [24960/67107 (37.21%)]	 Loss: 0.000006
Training epoch 100 [25600/67107 (38.17%)]	 Loss: 0.000006
Training epoch 100 [26240/67107 (39.12%)]	 Loss: 0.000005
Training epoch 100 [26880/67107 (40.08%)]	 Loss: 0.000003
Training epoch 100 [27520/67107 (41.03%)]	 Loss: 0.000002
Training epoch 100 [28160/67107 (41.98%)]	 Loss: 0.000013
Training epoch 100 [28800/67107 (42.94%)]	 Loss: 0.000004
Training epoch 100 [29440/67107 (43.89%)]	 Loss: 0.000005
Training epoch 100 [30080/67107 (44.85%)]	 Loss: 0.000006
Training epoch 100 [30720/67107 (45.80%)]	 Loss: 0.000006
Training epoch 100 [31360/67107 (46.76%)]	 Loss: 0.000011
Training epoch 100 [32000/67107 (47.71%)]	 Loss: 0.000003
Training epoch 100 [32640/67107 (48.66%)]	 Loss: 0.000007
Training epoch 100 [33280/67107 (49.62%)]	 Loss: 0.000001
Training epoch 100 [33920/67107 (50.57%)]	 Loss: 0.000005
Training epoch 100 [34560/67107 (51.53%)]	 Loss: 0.000002
Training epoch 100 [35200/67107 (52.48%)]	 Loss: 0.000005
Training epoch 100 [35840/67107 (53.44%)]	 Loss: 0.000011
Training epoch 100 [36480/67107 (54.39%)]	 Loss: 0.000008
Training epoch 100 [37120/67107 (55.34%)]	 Loss: 0.000001
Training epoch 100 [37760/67107 (56.30%)]	 Loss: 0.000010
Training epoch 100 [38400/67107 (57.25%)]	 Loss: 0.000015
Training epoch 100 [39040/67107 (58.21%)]	 Loss: 0.000003
Training epoch 100 [39680/67107 (59.16%)]	 Loss: 0.000012
Training epoch 100 [40320/67107 (60.11%)]	 Loss: 0.000002
Training epoch 100 [40960/67107 (61.07%)]	 Loss: 0.000003
Training epoch 100 [41600/67107 (62.02%)]	 Loss: 0.000007
Training epoch 100 [42240/67107 (62.98%)]	 Loss: 0.000017
Training epoch 100 [42880/67107 (63.93%)]	 Loss: 0.000014
Training epoch 100 [43520/67107 (64.89%)]	 Loss: 0.000008
Training epoch 100 [44160/67107 (65.84%)]	 Loss: 0.000007
Training epoch 100 [44800/67107 (66.79%)]	 Loss: 0.000023
Training epoch 100 [45440/67107 (67.75%)]	 Loss: 0.000010
Training epoch 100 [46080/67107 (68.70%)]	 Loss: 0.000009
Training epoch 100 [46720/67107 (69.66%)]	 Loss: 0.000007
Training epoch 100 [47360/67107 (70.61%)]	 Loss: 0.000004
Training epoch 100 [48000/67107 (71.56%)]	 Loss: 0.000003
Training epoch 100 [48640/67107 (72.52%)]	 Loss: 0.000018
Training epoch 100 [49280/67107 (73.47%)]	 Loss: 0.000018
Training epoch 100 [49920/67107 (74.43%)]	 Loss: 0.000011
Training epoch 100 [50560/67107 (75.38%)]	 Loss: 0.000004
Training epoch 100 [51200/67107 (76.34%)]	 Loss: 0.000006
Training epoch 100 [51840/67107 (77.29%)]	 Loss: 0.000007
Training epoch 100 [52480/67107 (78.24%)]	 Loss: 0.000006
Training epoch 100 [53120/67107 (79.20%)]	 Loss: 0.000008
Training epoch 100 [53760/67107 (80.15%)]	 Loss: 0.000003
Training epoch 100 [54400/67107 (81.11%)]	 Loss: 0.000006
Training epoch 100 [55040/67107 (82.06%)]	 Loss: 0.000003
Training epoch 100 [55680/67107 (83.02%)]	 Loss: 0.000007
Training epoch 100 [56320/67107 (83.97%)]	 Loss: 0.000015
Training epoch 100 [56960/67107 (84.92%)]	 Loss: 0.000003
Training epoch 100 [57600/67107 (85.88%)]	 Loss: 0.000005
Training epoch 100 [58240/67107 (86.83%)]	 Loss: 0.000006
Training epoch 100 [58880/67107 (87.79%)]	 Loss: 0.000011
Training epoch 100 [59520/67107 (88.74%)]	 Loss: 0.000009
Training epoch 100 [60160/67107 (89.69%)]	 Loss: 0.000015
Training epoch 100 [60800/67107 (90.65%)]	 Loss: 0.000003
Training epoch 100 [61440/67107 (91.60%)]	 Loss: 0.000003
Training epoch 100 [62080/67107 (92.56%)]	 Loss: 0.000004
Training epoch 100 [62720/67107 (93.51%)]	 Loss: 0.000014
Training epoch 100 [63360/67107 (94.47%)]	 Loss: 0.000005
Training epoch 100 [64000/67107 (95.42%)]	 Loss: 0.000004
Training epoch 100 [64640/67107 (96.37%)]	 Loss: 0.000010
Training epoch 100 [65280/67107 (97.33%)]	 Loss: 0.000003
Training epoch 100 [65920/67107 (98.28%)]	 Loss: 0.000004
Training epoch 100 [66560/67107 (99.24%)]	 Loss: 0.000001
Test set: Average Loss: 0.000169
