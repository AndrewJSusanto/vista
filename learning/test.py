import os
import argparse
import pickle5 as pickle
import numpy as np
import ray
from ray.rllib.evaluation.worker_set import WorkerSet

import misc
from policies import PolicyManager
from envs import wrappers
from trainers import get_trainer_class


def parse_args():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description='Roll out a reinforcement learning agent given a checkpoint.')
    parser.add_argument(
        'checkpoint',
        type=str,
        help='Checkpoint from which to roll out.')
    parser.add_argument( # TODO: this should be inside config but it's not stored
        '--run',
        type=str,
        default='PPO',
        help='The algorithm or model to train')
    parser.add_argument(
        '--monitor',
        default=False,
        action='store_true',
        help='Use gym Monitor-like wrapper to record video.')
    parser.add_argument(
        '--episodes',
        default=1,
        type=int,
        help='Number of complete episodes to roll out.')
    parser.add_argument(
        '--save-info',
        default=False,
        action='store_true',
        help='Save the info field generated by the step() method.')
    parser.add_argument(
        '--num-workers',
        default=0,
        type=int,
        help='Number of workers.')
    parser.add_argument(
        '--num-gpus',
        default=1,
        type=int,
        help='Number of GPUs.')
    parser.add_argument(
        '--temp-dir',
        default='~/tmp',
        type=str,
        help='Directory for temporary files generated by ray.')
    parser.add_argument(
        '--local-mode',
        action='store_true',
        help='Whether to run ray with `local_mode=True`. ')
    parser.add_argument(
        '--task-mode',
        default=None,
        type=str,
        help='VISTA task mode (for obstacle avoidance and takeover).')
    parser.add_argument(
        '--deterministic',
        action='store_true',
        help='Use deterministic action.')

    args = parser.parse_args()

    return args


def main():
    # Load config
    args = parse_args()
    config_dir = os.path.dirname(args.checkpoint)
    config_path = os.path.join(config_dir, 'params.pkl')
    if not os.path.exists(config_path):
        config_path = os.path.join(config_dir, '../params.pkl')
    
    with open(config_path, 'rb') as f:
        config = pickle.load(f)

    # Overwrite some config with arguments
    config['num_workers'] = args.num_workers
    config['num_gpus'] = args.num_gpus
    if args.task_mode is not None:
        config['env_config']['task_mode'] = args.task_mode

    # Register custom model
    misc.register_custom_env(config['env'])
    misc.register_custom_model(config['model'])

    # Start ray
    args.temp_dir = os.path.abspath(os.path.expanduser(args.temp_dir))
    ray.init(
        local_mode=args.local_mode,
        _temp_dir=args.temp_dir,
        include_dashboard=False)

    # Get agent
    cls = get_trainer_class(args.run)
    agent = cls(env=config['env'], config=config)
    if args.checkpoint:
        agent.restore(args.checkpoint)

    # Evaluation
    save_dir = os.path.expanduser(config_dir)
    test(args, agent, args.episodes, save_dir)

    # End ray
    ray.shutdown()


def test(args, agent, num_episodes, save_dir):
    assert hasattr(agent, 'workers') and isinstance(agent.workers, WorkerSet)

    env = agent.workers.local_worker().env
    if args.monitor:
        video_dir = os.path.join(save_dir, 'monitor')
        video_dir = '/home/gridsan/tsunw/tmp/monitor' # TODO: debug
        env = wrappers.MultiAgentMonitor(env, video_dir, video_callable=lambda x: True, force=True)
    policy_mapping_fn = agent.config["multiagent"]["policy_mapping_fn"]
    policy_map = agent.workers.local_worker().policy_map

    for ep in range(num_episodes):
        obs = env.reset()
        done = False
        episode_reward = dict()
        for agent_id in obs.keys():
            episode_reward[agent_id] = 0.
        state = {p: m.get_initial_state() for p, m in policy_map.items()}
        has_state = {p: len(s) > 0 for p, s in state.items()}
        while not done:
            act = dict()
            for agent_id, a_obs in obs.items():
                policy_id = policy_mapping_fn(agent_id)
                if has_state[policy_id]:
                    a_state = state[policy_id]
                    a_act, a_state, _ = agent.compute_action(a_obs, a_state, policy_id=policy_id, explore=args.deterministic)
                    state[agent_id] = a_state
                else:
                    a_act = agent.compute_action(a_obs, policy_id=policy_id, explore=args.deterministic)
                act[agent_id] = a_act
            next_obs, rew, done, info = env.step(act)
        
            # TODO: save data

            for agent_id, a_rew in rew.items():
                episode_reward[agent_id] += a_rew

            done = np.any(list(done.values()))
            obs = next_obs
    
        print("Episode #{}: reward: {}".format(ep, episode_reward))


if __name__ == '__main__':
    main()