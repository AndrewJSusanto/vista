ma-overtaking-state-obs:
  local_dir: ~/tmp
  env: MultiAgentStateObs
  run: PPO
  num_samples: 1
  checkpoint_freq: 10
  checkpoint_at_end: true
  stop:
    episode_reward_mean: 99999
    timesteps_total: 3e6
  ray_resources:
    num_cpus: 40
    num_gpus: 0
  config:
    num_workers: 32
    framework: torch
    # environment
    env_config:
      subsample_road: 10
      max_horizon: 200
      rigid_body_collision: true
      rigid_body_collision_coef: 0.001
      rigid_body_collision_repulsive_coef: 0.9
      task: MultiAgentOvertaking
      trace_paths: [~/data/20190205-102931_blue_prius_devens_rightside]
      mesh_dir: ~/data/old_vista_mesh_lib_calib/
      init_agent_range: [6, 12]
      collision_overlap_threshold: 0.05
      task_mode: infinite_horizon
      velocity_range: [5., 15.]
      min_velocity_diff: 4
      passed_dist: 10.
      svo_theta: 0
      wrappers: [RandomPermuteAgent]
      wrappers_config:
        RandomPermuteAgent:
          permute_prob: 0.0
    observation_filter: MeanStdFilter
    callbacks: BasicCallbacks
    # policies
    multiagent:
      policies: [default_policy_0, default_policy_1]
      policies_to_train: [default_policy_0, default_policy_1]
      policy_mapping_fn: one_to_one
      callbacks_config:
        n_recent_ckpt: 0
        ckpt_freq: 2
        main_policy_id: default_policy_0
        ckpt_policy_id: default_policy_1
    # MDP-related
    gamma: 0.99
    rollout_fragment_length: 200
    # sgd-related
    lr: 0.0002
    lr_schedule: null
    num_sgd_iter: 8
    train_batch_size: 32000
    sgd_minibatch_size: 512
    # value function
    vf_loss_coeff: 1.0
    vf_clip_param: 1000.0
    # PPO-specific
    entropy_coeff: 0.01
    entropy_coeff_schedule: null
    lambda: 0.95
    kl_coeff: 0.2
    kl_target: 0.01
    clip_param: 0.3
    # model
    model:
      vf_share_layers: true
      fcnet_hiddens: [32, 32]
      fcnet_activation: relu
      custom_action_dist: TorchBeta # prevent inf logprob
      custom_model_config:
        custom_action_dist_config:
          low: [-0.3]
          high: [0.3]