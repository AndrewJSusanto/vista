overtaking-lstm:
  local_dir: $TMPDIR/
  env: Overtaking
  run: PPO
  num_samples: 1
  checkpoint_freq: 10
  checkpoint_at_end: true
  stop:
    episode_reward_mean: 99999
    timesteps_total: 1e7 #NOTE 3e6
  ray_resources:
    num_cpus: 40
    num_gpus: 1
  config:
    num_workers: 12
    num_gpus: 0.28
    num_gpus_per_worker: 0.06
    framework: torch
    # environment
    env_config:
      trace_paths: [$DATA_DIR/traces/20210527-131252_lexus_devens_center_outerloop,
                    $DATA_DIR/traces/20210527-131709_lexus_devens_center_outerloop_reverse,
                    $DATA_DIR/traces/20210609-122400_lexus_devens_outerloop_reverse,
                    $DATA_DIR/traces/20210609-123703_lexus_devens_outerloop,
                    $DATA_DIR/traces/20210609-133320_lexus_devens_outerloop,
                    $DATA_DIR/traces/20210609-154525_lexus_devens_sideroad,
                    $DATA_DIR/traces/20210609-154745_lexus_devens_outerloop_reverse,
                    $DATA_DIR/traces/20210609-155238_lexus_devens_outerloop,
                    $DATA_DIR/traces/20210609-155752_lexus_devens_subroad,
                    $DATA_DIR/traces/20210609-175037_lexus_devens_outerloop_reverse,
                    $DATA_DIR/traces/20210609-175503_lexus_devens_outerloop]
      mesh_dir: $DATA_DIR/carpack01/
      rendering_config:
        use_lighting: false
      init_agent_range: [8, 15]
      init_lat_shift_range: [1.5, 2.0]
      collision_overlap_threshold: 0.05
      soft_collision_ub: 0.05
      soft_collision: 0.01
      task_mode: episodic
      motion_model: constant_speed # motion model for other non-controllable agents
      constant_speed_range: [0., 4.]
      ego_constant_speed_range: [4., 10.]
      curv_reset_mode: default
      dilate_ref_agent: [1.0, 0.4]
      with_velocity: false
      free_width_mul: 0.5 # effective road width = 4
      wrappers: [PreprocessObservation, BasicManeuverReward]
      wrappers_config:
        PreprocessObservation:
          fx: 1
          fy: 1
          random_gamma: [0.5, 1.5]
          color_jitter: [0.5, 0.7, 0.5, 0.0] # brightness / contrast / saturation / hue
          standardize: true
          randomize_at_episode: true
        BasicManeuverReward:
          center_coeff: 0.001
          jitter_coeff: 0.001
          inherit_reward: true
    observation_filter: NoFilter
    callbacks: BasicCallbacks
    # policies
    multiagent:
      policies: [default_policy]
      policies_to_train: [default_policy]
      policy_mapping_fn: all_to_default
    # MDP-related
    gamma: 0.99
    rollout_fragment_length: 200
    # sgd-related
    lr: 0.0002
    lr_schedule: null
    num_sgd_iter: 8
    train_batch_size: 32000
    sgd_minibatch_size: 512
    # value function
    vf_loss_coeff: 1.0
    vf_clip_param: 1000.0
    # PPO-specific
    entropy_coeff: 0.01
    entropy_coeff_schedule: null
    lambda: 0.95
    kl_coeff: 0.2
    kl_target: 0.01
    clip_param: 0.3
    # model
    model:
      vf_share_layers: true
      lstm_cell_size: 64
      max_seq_len: 20
      custom_model: ConvNet
      conv_filters: [
        [3, 24, 5, 2, 2],
        [24, 36, 5, 2, 2], 
        [36, 48, 3, 2, 1], 
        [48, 64, 3, 1, 1], 
        [64, 64, 3, 1, 1]
      ]
      conv_activation: relu
      custom_action_dist: TorchBeta # prevent inf logprob
      custom_model_config:
        avgpool_after_extractor: [1, 20]
        use_recurrent: true
        rnn_num_layers: 1
        with_bn: true
        policy_hiddens: [64, 2]
        policy_activation: relu
        policy_dropout: 0.0
        value_fcnet_hiddens: [64, 1]
        value_fcnet_activation: relu
        value_fcnet_dropout: 0.0 
        custom_action_dist_config:
          low: -0.3
          high: 0.3