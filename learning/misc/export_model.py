import os
import argparse
import pickle5 as pickle
import numpy as np
import torch
import ray
from ray.rllib.evaluation.worker_set import WorkerSet
from ray.tune.registry import get_trainable_cls

from . import utils
import models


def parse_args():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description='Roll out a reinforcement learning agent given a checkpoint.')
    parser.add_argument(
        'checkpoint',
        type=str,
        help='Checkpoint from which to roll out.')
    parser.add_argument( # TODO: this should be inside config but it's not stored
        '--run',
        type=str,
        default='PPO',
        help='The algorithm or model to train')
    parser.add_argument(
        '--num-workers',
        default=0,
        type=int,
        help='Number of workers.')
    parser.add_argument(
        '--num-gpus',
        default=1,
        type=int,
        help='Number of GPUs.')
    parser.add_argument(
        '--temp-dir',
        default='~/tmp',
        type=str,
        help='Directory for temporary files generated by ray.')
    parser.add_argument(
        '--export',
        action='store_true',
        help='Whether to export the model or test on the exported model.')
    parser.add_argument(
        '--to-deepknight',
        action='store_true',
        help='Whether to export to deepknight format.')

    args = parser.parse_args()

    return args


def main():
    # Load config
    args = parse_args()
    config_dir = os.path.dirname(args.checkpoint)
    config_path = os.path.join(config_dir, 'params.pkl')
    if not os.path.exists(config_path):
        config_path = os.path.join(config_dir, '../params.pkl')
    
    with open(config_path, 'rb') as f:
        config = pickle.load(f)

    # Overwrite some config with arguments
    config['num_workers'] = args.num_workers
    config['num_gpus'] = args.num_gpus

    # Register custom model
    utils.register_custom_env(config['env'])
    utils.register_custom_model(config['model'])

    # Start ray
    args.temp_dir = os.path.abspath(os.path.expanduser(args.temp_dir))
    ray.init(
        local_mode=True,
        _temp_dir=args.temp_dir,
        include_dashboard=False)

    # Get agent
    cls = get_trainable_cls(args.run)
    agent = cls(env=config['env'], config=config)
    if args.checkpoint:
        agent.restore(args.checkpoint)

    # Run
    export_path = os.path.join(config_dir, 'model.pkl')
    if args.export:
        state_dict = agent.get_policy().model.state_dict()

        # make compatible with e2ed
        if args.to_deepknight:
            export_path = os.path.join(config_dir, 'model_deepknight.pkl')

            new_state_dict = dict()
            for key, value in state_dict.items():
                if key.startswith('extractor.'):
                    key = key.replace('extractor.', 'module.extractors.fcamera.features.')
                    new_state_dict[key] = value
                elif key.startswith('policy.'):
                    key = key.replace('policy.', 'module.transform.')
                    new_state_dict[key] = value
                # drop value function
            saved_data = {'model': new_state_dict}
        else:
            new_state_dict = dict()
            for key, value in state_dict.items():
                if key.startswith('extractor.'):
                    key = key.replace('extractor.', 'extractor.')
                    new_state_dict[key] = value
                elif key.startswith('policy.'):
                    key = key.replace('policy.', 'policy.')
                    new_state_dict[key] = value
            saved_data = new_state_dict
        
        torch.save(saved_data, export_path)
    else:
        test_exported_model(agent, config, export_path)

    # End ray
    ray.shutdown()


def test_exported_model(agent, config, export_path):
    # get environment
    env = agent.workers.local_worker().env
    obs_space = env.observation_space
    act_space = env.action_space

    # prepare for rllib agent
    policy_mapping_fn = agent.config["multiagent"]["policy_mapping_fn"]

    # get and load exported model (and action distribution)
    model_cls = getattr(models, config['model']['custom_model'])
    model_name = agent.get_policy().model.name
    model_num_outputs = agent.get_policy().model.num_outputs
    kwargs = config['model']['custom_model_config'] if 'custom_model_config' in \
        config['model'].keys() else {}
    exported_model = model_cls(obs_space, act_space, model_num_outputs, 
        config['model'], model_name, **kwargs)
    exported_model.load_state_dict(torch.load(export_path), strict=False)
    exported_model.cuda()
    
    act_dist_cls = getattr(models, config['model']['custom_action_dist'])
    try: # TODO: hacky fix for checkpoints that doesn't save this attribute because of pop
        act_dist_config = config['model']['custom_action_dist_config']
    except:
        act_dist_config = {'low': -0.3, 'high': 0.3}

    # run
    obs = env.reset()
    done = False
    episode_reward = dict()
    for agent_id in obs.keys():
        episode_reward[agent_id] = 0.
    n_steps = 0
    while not done:
        # get action from rllib agent object
        act = dict()
        act_info = dict()
        for agent_id, a_obs in obs.items():
            policy = agent.get_policy(policy_id=policy_mapping_fn(agent_id))
            a_act, _, a_act_info = policy.compute_single_action(a_obs)
            act[agent_id] = a_act
            act_info[agent_id] = a_act_info

        # get action from exported model
        act_exported = dict()
        act_info_exported = dict()
        for agent_id, a_obs in obs.items():
            input_dict = {'obs': torch.Tensor(a_obs[None,...]).cuda()}
            with torch.no_grad():
                act_dist_inputs, _ = exported_model(input_dict, None, None)
            act_dist = act_dist_cls(act_dist_inputs, exported_model, **act_dist_config)
            a_act = act_dist.sample()
            act_exported[agent_id] = a_act
            act_info_exported[agent_id] = {
                'action_dist_inputs': act_dist_inputs.cpu().numpy(),
                'action_logp': act_dist.logp(a_act).cpu().numpy(),
                'rllib_action_logp': act_dist.logp(torch.Tensor(act[agent_id]).cuda()).cpu().numpy(),
            }

        # check action logprob
        for agent_id in obs.keys():
            a_act_info = act_info[agent_id]
            a_act_info_exported = act_info_exported[agent_id]
            if not np.allclose(a_act_info['action_logp'], a_act_info_exported['rllib_action_logp']):
                err = np.abs(a_act_info['action_logp'] - a_act_info_exported['rllib_action_logp'])
                print('Not allclose. Error = {}'.format(err))

        # step environment
        next_obs, rew, done, info = env.step(act)
        
        # update data
        for agent_id, a_rew in rew.items():
            episode_reward[agent_id] += a_rew
        done = np.any(list(done.values()))
        obs = next_obs
        n_steps += 1

    print("Reward: {}, #Steps: {}".format(episode_reward, n_steps))


if __name__ == '__main__':
    main()
